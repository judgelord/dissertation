---
# fontsize: 12pt
bibliography:  mendeley.bib
biblio-style: apa
citecolor: black
output: #word_document
  bookdown::pdf_document2:
    latex_engine: pdflatex
    keep_tex: true
    template: article-template.tex
    toc: false
    fig_caption: true
    citation_package: natbib
header-includes:
#- \usepackage{ragged2e}
#- \input{preamble.tex}
#- \let\footnote=\endnote
#- \renewcommand{\footnotesize}{\normal}
#- \renewcommand{\normalsize}{\large}
spacing: doublespacing
#endnotes: true
title: "Do Public Pressure Campaigns Influence Bureaucratic Policymaking?"
author: 
- name: Devin Judge-Lord
  affiliation: University of Wisconsin-Madison

abstract: "I develop several measures of lobbying success and corresponding tests of whether public pressure campaigns increase lobbying success in agency rulemaking. To measure lobbying success, I first hand-code a random sample of proposed rules with and without a mass-comment campaign for whether each lobbying coalition got the policy outcome they sought. This hand-coded set validates computational methods that I use to identify coalitions and estimate lobbying success for all rules posted for comment on regulations.gov between 2015 and 2018. I then assess potential mechanisms by which mass public engagement may affect policy. Each mechanism involves a distinct type of information revealed to decisionmakers. Of primary interest is the extent to which pressure campaigns affect agency decisionmakers directly or indirectly through their effects on elected officials' oversight behaviors. I test this by assessing congressional oversight as a causal mediator using a subset of rules where I collect and code letters from Member of Congress to agencies about proposed agency rules."
---
```{r setup, echo = FALSE, include = FALSE}
source("setup.R")
```

# Data
I combine several sources of data on U.S. federal agency rulemaking. The core data are the texts of draft and final rules and public comments on these rules. This includes 16 thousand proposed rules from 144 agencies (as defined by regulations.gov) that were open for comment on regulations.gov between 2005 and 2018 with a final agency action between 2005 and 2019 and 50 million comments on these rules. I scrape draft and final rule texts from federalregister.gov and comments submitted as attachments or by mail from regulations.gov. I retrieve comments submitted directly on regulations.gov and metadata on rules and comments (such as when the proposed rule was open for comment and whether the agency identified the organization submitting the comment) from the regulations.gov API. I collect additional metadata on rules (such as whether the rule was considered "significant") from the Unified Agenda published by the Office of Information and Regulatory Affairs (reginfo.gov). 

Finally, to better capture comments from Members of Congress on proposed rules, I supplement congressional comments posted on regulations.gov with all communication from Members of Congress to an agency on proposed rules from 2007 to 2019 obtained through Freedom of Information Act Requests to each agency. 

The combined dataset has 50 million observations of a comment on a proposed rule. I attempt to identify the organization(s) that submitted or mobilized each comment by extracting all organization names from the comment text. For comments that do not reference an organization, I am often able to identify organizations with internet searches using the comment's text. I then identify lobbying coalitions by clustering comments that use similar phrases or word frequencies. This includes cases where organizations co-sign comments.^[For more on how I identify organizations and coalitions, see [Chapter 2, "Why Do Agencies (Sometimes) Get So Much Mail?"](https://judgelord.github.io/dissertation/whyMail.pdf).]

For analysis, I collapse these data to either one observation per proposed rule per organization or coalition. The number of comments is an attribute of the mobilizing organization or coalition. The number and type of organizations is an attribute of each coalition. If an organization mobilizes more than 1000 comments or 100 identical comments on a proposed rule, I code that organization, its coalition, and the proposed rule as having a mass comment campaign. For hand-coding, I use coalition-level observations for a random sample of 100 proposed rules with a mass-comment campaign and 100 proposed rules without a mass comment campaign. Except for comment texts and attachments, these data are available separately and combined on github.com/judgelord/rulemaking.  

# Methods
The simplest way to assess the hypothesis that mass engagement increases lobbying success is to assess the magnitude of the relationship between the number of comments that a coalition mobilizes and its lobbying success. However, public pressure campaigns may only be effective under certain conditions. Thus, I first assess the main relationship and then assess evidence for or against different pathways of influence.

## Measuring Lobbying Success
The dependent variable is the extent to which a lobbying coalition got the policy outcome they sought. I use three measures of lobbying success. First, on a sample of rules, I hand-coded lobbying success for each lobbying coalition, comparing the change between the draft and final rule to each organization's comment's demands on a five-point scale from "mostly as requested" to "significantly different/opposite than requested." To do this, I first identify organizational comments. For each organization, I identify the main overall demand and the top three specific demands and the corresponding parts of the draft and final rule texts.^[This does not capture rule changes on which an organization did not comment. The codebook can be accessed [here](https://docs.google.com/document/d/1o1hi0z9O-G9xsgkspOFG2VWzh0wQKjiezzoVpItaCxU/edit?usp=sharing). Examples of coded cases are described [here](https://judgelord.github.io/dissertation/influence_coded_cases.pdf). I group organizations with common demands into coalitions, which I code as business-led, public interest group-led, or other. I then code overall lobbying success and lobbying success on each specific demand for each organization and coalition. The overall score and average score across specific demands both fall on the interval from -2 ("significantly different") to 2 ("mostly as requested"). 

Second, I use methods similar to automated plagiarism detection algorithms to identify changes between a draft and final rule that were suggested in a comment. Specifically, I count the number of words in phrases of at least ten words that appear in the comment and final rule, but not the draft rule. To do this, I first identify new text in the final rule by removing any 10-word or longer phrases that were retained from the draft rule. I then search each comment for any 10-word or longer phrases shared with the new rule text and count the total number of words from these shared phrases. Finally, I normalize this count of "copied" words across shorter and longer comments by dividing it by the total number of words in the comment. This measure falls between 0 (zero percent of words from the comment added to the final rule) and 1 (100 percent of words from the comment added to the final rule).

Third, I model the similarity in word frequency distributions between comments and changes to the rule. Here, I also include the proposed and final rule preambles and the agency's responses to comments. Agencies write lengthy justifications of their decisions in response to some comments but not others. By including preambles and responses to comments, text-similarity captures this measure of attention to a comment's demands. I measure similarity by averaging the differences in topic proportions between the comment and new rule text across 45 LDA models estimated with 5 through 50 topics, $\sum_{k_1=5}^{k_n=50}(\sum(\theta_{rule\ change,k}-\theta_comment,k)/k)/(k_n-k_1)$.  This measure falls between 0 (completely different) and 1 (exactly the same).^[As robustness checks, I also use other word-frequency based methods of document similarity, including Wordfish and cosign similarity scores. For more on methods of measuring textual similarity, see [Measuring Change and Influence in Budget Texts](https://judgelord.github.io/research/budgets/JudgeLord%20APSA%202017.pdf).]  This measure falls between 0 (completely different) and 1 (identical).

To assess the plausibility of the two automated methods (tex-reuse and word-frequency similarity), I calculate the correlation between these scores and my hand-coded 5-point scale for rules in the hand-coded sample where a final rule was published. 

## Explanatory variables
The core predictors of lobbying success in the models below are the number of supportive comments generated by a public pressure campaign (the main variable of interest), the size of the lobbying coalition, and whether the coalition is business-led. As the marginal impact of additional comments likely diminishes, I take the log of one plus the total number of supporting comments such that *log mass comments* is 0 when there is no mass comment campaign. The size of the lobbying coalition is the number of distinct commenting organizations in the coalition (including those that co-sign a comment). I code a coalition as a business-led coalition if the majority of commenting organizations are for-profit businesses or if, upon investigation, I find it to be primarily led or sponsored by for-profit businesses. 

## Limitations
The two main limitations of this design both bias estimates of public pressure campaign influence toward zero. 

First, lobbying success may take forms other than changes in policy texts. Agencies may speed up or delay finalizing a rule, extend the comment period, or delay the date at which the rule goes into effect. Indeed, commentors often request speedy or delayed rule finalization, comment period extensions, or delayed effective dates. I include these potential outcomes in my hand-coding but not in the two automated methods, which apply only to observations with a final rule text.^[Time allowing, I will hand-code a larger sample of proposed rules where no final rule was published.]

Second, bureaucrats may anticipate public pressure campaigns when writing draft rules, muting the observed relationship between public pressure and rule change at the final rule stage of the policy process. 

# Modeling the direct relationship
For all three measures of lobbying success, I assess the relationship between lobbying success and mass comments by modeling lobbying success, $y_i$, as a combination of coalition size, a business coalition, and the logged number of mass comments. For the hand-coded lobbying success on a five-point scale, I estimate OLS^[See OLS model estimates with simulated data [here](https://judgelord.github.io/dissertation/preanalysis.pdf). For robustness, I also estimate this as ordered logit, which is more appropriate by less interpretable.] regression: 

$$
y_i = \beta_0 + \beta_1*log(mass\ comments_i) + \beta_2*coalition\ size_i + \beta_3*business_i + \epsilon_i
$$

For the automated measures of lobbying success, I estimate beta regression models with the same variables.

# Modeling mediated relationships



## Examples of hand-coded lobbying success
To illustrate how I create these variables, consider two such cases:

**2015 Waters of the United States Rule:**
In response to litigation over which waters were protected by the Clean Water Act, the Environmental Protection Agency and Army Corp of Engineers proposed a rule based on a legal theory articulated by Justice Kennedy, which was more expansive than Justice Scalia's. 
The Natural Resources Defense Council submitted a 69-page highly technical comment "on behalf of the Natural Resources Defense Council..., the Sierra Club, the Conservation Law Foundation, the League of Conservation Voters, Clean Water Action, and Environment America" supporting the proposed rule:

> "we strongly support EPA’s and the Corps’ efforts to clarify which waters are protected by the Clean Water Act. We urge the agencies to strengthen the proposal and move quickly to finalize it..." 

I coded this as support for the rule change, specifically not going far enough. I also coded it as requesting speedy publication. NRDC makes four substantive requests: one about retaining language in the proposed rule ("proposed protections for tributaries and adjacent waters...must be included in the final rule") and three proposed changes ("we describe three key aspects of the rule that must be strengthened").^[These three aspects are: (1) "The Rule Should Categorically Protect Certain “Other Waters” including Vernal Pools, Pocosins, Sinkhole Wetlands, Rainwater Basin Wetlands, Sand Hills Wetlands, Playa Lakes, Interdunal Wetlands, Carolina and Delmarva Bays, and Other Coastal Plain Depressional Wetlands, and Prairie Potholes. Furthermore, "Other 'Isolated' Waters Substantially Affect Interstate Commerce and Should be Categorically Protected Under the Agencies’ Commerce Clause Authority." (2) "The Rule Should Not Exempt Ditches Without a Scientific Basis" (3) "The Rule Should Limit the Current Exemption for Waste Treatment Systems
"] These demands provide specific keywords and phrases for which to search in the draft and final rule text. 

A coalition of 15 environmental organizations mobilized over 944,000 comments, over half (518,963) were mobilized by the four above organizations: 2421,641 by Environment America, 108,076 by NRDC, 101,496 by clean water action, and 67,750 by the Sierra Club. Other coalition partners included EarthJustice (99,973 comments) and Organizing for Action (formerly president Obama's campaign organization, 69,369 comments). This is the upper tail end of the distribution. This coalition made sophisticated recommendations and mobilized a million people.

The final rule moved in the direction requested by NRDC's coalition, but to a lesser extent than requested--what I code as "some desired changs."" As NRDC et al. requested, the final rule retained the language protecting tributaries and adjacent waters and added some protections for "other waters" like prairie potholes and vernal pools, but EPA did not alter the exemptions for ditches and waste treatment systems. 

Comparing the draft and final with text reuse allows us to count the number words that belong to 10-word phrases that appear in both the draft and final, those that appear only in the draft, and those that appear only in the final. For the 2015 Waters Of The U.S. rule, 15 thousand words were deleted, 37 thousand words were added, and 22 thousand words were kept the same. This means that more words "changed" than remained the same, specifically 69% of words appearing in the draft or final were part were either deleted or added.

For this coalition, the dependent variable, *coalitions success* is 1, *coalition size* is 15, *business coalition* is 0, *comment length* is 69/88, `r round(69/88,2)`, and *log mass comments* is log(943,931), `r round(log(943931), 2)`.

**2009 Fine Particle National Ambient Air Quality Standards:** In 2008, the EPA proposed a rule expanding air quality protections. Because measuring small particles of air pollution was once difficult, measurements of large particulates were allowed as a surrogate measure for fine particles under EPA's 1977 PM10 Surrogate Policy. EPA proposed eliminating this policy, thus requiring regulated entities and state regulators to measure and enforce limits on much finer particles of air pollution. 

EPA received 163 comments on the rule, 129 from businesses, business associations such as the American Petroleum Institute and The Chamber of Commerce, and state regulators that opposed the rule. Most of these were short and cited their support for the 63-page comment from the PM Group, "an ad hoc group of industry trade associations" that opposed the regulation of fine particulate matter. Six state regulators, including Oregon's, only requested delayed implication of the rule until they next revised their State Implementation Plans (SIPs) for Prevention of Significant Deterioration (PSD). EarthJustice supported the rule but opposed the idea that the cost of measuring fine particles should be a consideration. On behalf of the Sierra Club, the Clean Air Task Force, EarthJustice commented: "We support EPA’s proposal to get rid of the policy but reject the line of questioning as to the benefits and costs associated with ending a policy that is illegal." The EarthJustice-led coalition also opposed delaying implementation: "EPA must immediately end any use of the Surrogate Policy – either by "grandfathered" sources or sources in states with SIP‐approved PSD programs – and may not consider whether some flexibility or transition is warranted by policy considerations."

The final rule did eliminate the Surrate Policy but allowed states to delay implementation and enforcement until the next scheduled revision of their Implementation Plans. I code this as the EarthJustice coalition getting most of what they requested, but not a complete loss for the regulated coalition.

For the PM Group coalition, the dependent variable, *coalitions success* is -1, *coalition size* is 129, *business coalition* is 1, *comment length* is 63/85, `r round(63/85, 2)`, and *log mass comments* is 0.

For the State of Oregon's coalition, the dependent variable, *coalitions success* is 2, *coalition size* is 6, *business coalition* is 0, *comment length* is 5/85, `r round(5/85, 2)`, and *log mass comments* is 0.

For the EarthJustice coalition, the dependent variable, *coalitions success* is 1, *coalition size* is 3, *business coalition* is 0, *comment length* is 7/85, `r round(7/85, 2)`, and *log mass comments* is 0.



# Pre-analysis

To illustrate my planned analysis, I simulate data for each variable described above. 

**Dependent variables:** *Coalition success* is drawn from a descrete distribution {-2, -1, 0, 1, 2}. 

**Explanatory variables:** *Coalition size* (a count) is drawn from a Poisson distribution. *Business colation* is binomial. In reality, business coalitions are more common than non-business coalitions, but here I estimate a balanced sample. I set rule pages constant at 85 and draw *comment lengths* from a Poisson distribution. While in reality, less than one percent of coalitions lobbying in rulemaking opt for a mass-comment campaign, I aim to gather a balanced sample, so half of the simulated data are assumed to have no mass comment campaign (*mass comments* = 0) and the other half have a number of *mass comments* drawn from a Zero-Truncated Poisson distribution, which is then transformed to a log scale. 

```{r,  fig.width=2, fig.height=2}
success <- sample(x = c(-2,1,0,1,2), 1000, replace = T, prob = c(0.1, 0.3, .1, 0.4, 0.1)) 

size <- rtnorm(1000, mean = 5, sd= 100, lower = 1) %>% round()

business <- sample(x = c(0,1), 1000, replace = T, prob = c(0.5, .5)) 

comment_length <- rpois(1000, 10)
length <- comment_length/85 *100

comments <-c(rtnorm(500, mean = 10000, sd = 100000, lower = 100)  %>% log(), rep(0, 500)) %>% sample() %>% round()

d = tibble(success, size, business, length, comments)

d1 <- d %>% mutate(rule = sample(1:1000), coalitions  = 1, unopposed = 1)
d2 <- d %>% mutate(rule = sample(1001:2000), coalitions  = 2, unopposed = 0)
d3 <- d %>% mutate(rule = sample(1001:2000), coalitions  = 2, unopposed = 0)

# full_join(d1,d2) %>% full_join(d3) %>% arrange(-rule)

# ggplot(d, aes( x = success)) + geom_histogram()
# ggplot(d, aes( x = business)) + geom_histogram()
ggplot(d, aes( x = size)) + geom_histogram()+ labs(x = "Coalition size")
ggplot(d, aes( x= length)) + geom_histogram()+ labs(x = "Comment length/proposed rule length")
ggplot(d, aes( x= comments)) + geom_histogram() + labs(x = "Log(mass comments)")
```

Unsurprisingly this model yields no significant results. With lobbying success as the dependent variable, the coefficient on the main variable of interest would be interpreted as a one-unit increase in the logged number of comments corresponds to a $\beta_{logmasscomments}$ increase in the five-point influence scale.

```{r model_success}
m <- lm(success ~ size + business + length + comments, data = d) 

# knitr::kable(m %>% tidy(), digits = 3)

m %>%
  tidy(conf.int = TRUE) %>% 
  filter(term != "(Intercept)") %>% 
  ggplot() + 
  geom_hline(yintercept = 0, color = "grey") + 
  aes(x = term, y = estimate, ymin = conf.low, ymax =conf.high) + 
  geom_pointrange( )  + 
  coord_flip() +
  labs(title = "",
       y = "Lobbying Success", 
       x = "") + 
  scale_color_discrete()
```

