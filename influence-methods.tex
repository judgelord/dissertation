% MEASUREMENT 3 % FIX THIS 
% it is difficult
The main dependent variable here is changes in rule text. However assessing policy change is difficult. Thus, I also use other measures of agency responses to lobbying efforts. 
%Different inputs may yield different results: 
Agencies may or may not change draft policies or may speed up or delay finalizing them. They write lengthy justifications of their decisions in response to some demands but not others. They may or may not extend the comment period. Measuring actual changes in policy text is more difficult. I aim to use automated methods to systemically identify changes between draft and final rules, parse these textual differences to identify meaningful policy changes, and compare them to demands raised in comments to measure which coalition got their way.
Observing policy influence, especially in the final stages of policymaking is difficult. Given the momentum of political agendas and the fact that much is determined before draft rules are made public, changes are often on the margins. But such marginal victories are also the aim of business and other interest groups. 
Additionally, my theory suggests that influence is likely only in cases where mass mobilization is (1) aimed at influencing policy and (2) not accurately anticipated by policymakers. Measuring these will also be difficult.

\input{causal-influence-test.tex}

Observational studies of policy decisions are almost always frustrated by the fact that decisionmakers rationally anticipate the actions of those who would influence them, rendering this influence difficult to observe. Thus I expect to observe larger effects in cases where mobilization or the level of engagement achieved was not anticipated by agency staff. However, as long as rulewriters do not perfectly anticipate mass engagement, it should have observable, if depressed, effects. 

My method of identifying whether a rule seems to move in the direction requested is similar to leading existing methods---\citet{Yackee2006JOP} measure whether commenters requested for more or less regulation---and superior to self-reported influence \citep{Furlong1997}.

As most rules address long-defined problems. They are next steps advancing a policy agenda \citep{West2013} or the first steps in a new, often reverse, policy direction, it is possible that effects of ``going public'' are cumulative in a policy area over time, starting out small, but gaining agenda-setting power with sustained public attention. This may not be possible to measure with my rule-focused research design. However, if sequential rules can be linked to distinct policy agendas, my strategy could be extended to model dynamics over time following \citet{Brookhart2015}.



% Measuring Policy Change}
Policies may shape and be shaped by many forces, including the collective action of citizens, expert opinions, and businesses interests. Yet the drivers and consequences of policymaking are difficult to disentangle. Business groups may fund scientists or advocacy campaigns to preempt or undo costly regulations. Experts and policymakers may inspire broader civic mobilization, and citizen mobilization may, in turn, shape the priorities of experts and policymakers. Some policy debates divide along lines of citizen and corporate interest or expert and popular opinion, but many entail various clusters of claims regarding the public interest, expertise, and business interests: claims about the public good, scientific truths, and the proper role of government. Inferring policy demands from identity alone and assuming static coalitions may miss much of the story. 

\paragraph{Text as Data}
Existing measures of who gets their way in rulemaking are blunt---hand-coding texts on a few pre-defined and simplified categories such as ``pro- or anti-regulation.'' This is well motivated by theory, but in practice, it is often unclear whether a policy, on its face, increases or decreases government regulation, is liberal or conservative, or maps on to any other such latent dimension. 

Another drawback of the hand-coding approach is that one must often read each comment and compare it to the rule change to identify influential groups. If groups get their way when they lobby in many venues over time or on obscure or uncontested rules, studies focusing on highly visible and contested policy processes may miss much. Conversely, if mass mobilization in high-profile and hotly-contested rules matters, studies of rulemaking that discard the hundreds of thousands of form letters and other evidence of occasional bursts of civic engagement are unable to assess if mass participation matters. Legal scholarship suggests that mass mobilization may be important (Coglianese 2001). This limited and potentially biased empirical focus is largely due to the cost of hand-coding methods.

I use computational text-analysis tools to address these concerns. Political scientists have only begun to leverage text-analysis tools to measure political relationships (see Grimmer 2013 on priorities, Kl\"uver and Mahoney 2015 on framing, Wilkerson et al. 2015 on tracing policy ideas). I expand analysis of rulemaking from thousands of documents to hundreds of thousands and from a few variables to many. Specifically, text-analysis tools can do two things: (1) rapidly code large amounts of text for pre-established theoretically-informed variables and (2) identify new dimensions of variation (Grimmer and King 2011). A key advantage of text-analysis methods is that they can identify new dimensions of variation one had no prior reason to suspect, suggesting new hypotheses. For example, in addition to our prior notions of who ought to be influential, we may expect that the topics, priorities, arguments, or issue frames on which comments cluster may identify new dimensions distinguishing winning and losing coalitions in different contexts.  %The literature suggests three broad rival hypotheses: \textit{The change between the draft and final rule reflects the interests of ($H_1$) underrepresented groups, ($H_2$) regulated businesses, ($H_3$) no particular class of commenters.} 
% regs.gov
% regulation.gov - sometimes they are coming in after the time period

%I refine tools to measure influence in policymaking with two aims: (1) to estimate the relative influence of different actors or texts and (2) to identify new factors that predict influence. These methods will allow new tests to advance core debates about how and why influence occurs and may benefit a wide range of scholarship. Active debates over bureaucratic autonomy and capture (Carpenter 2001, 2010, Carpenter and Moss 2014) and the influence of Congress (Clinton et al. 2014, Farhang and Yaver 2015), the courts (Lauderdale and Clark 2014), the president (Carrigan and Krazdin 2015), and public opinion (Dunleavy 2013) will all benefit from using text-analysis to measure influence. 

%Generally speaking, my method of measuring influence in each rulemaking case has two steps, each targeting a conceptually distinct type of influence. First I identify the major dimensions of variation in comments and the direction in which the policy moved in relation to those dimensions, i.e. which side(s) won. Second, I identify particularly influential texts within the winning coalition(s). 

% More specifically, to address my three descriptive questions, I propose to uses an ensemble of five text-analysis methods to identify participants, track coalitions, and quantify the relationship between various input texts and outcome text. This ensemble combines (1) citations(finding texts that mention the same organization and individual names across comments) to identify participants (2) single member topic modeling to identify coalitions, (3) Smith-Waterman alignment (text reuse) identify text fragments copied from each individual texts, (4) mixed-member topic models to identify the distribution of topics in comments and changes to rule texts, and (5) sentiment analysis to identify positive and negative positions on each topic. These are described in more detail below.

%This ensemble method has several applications in this project. First, it allows me to identify clusters (possible coalitions) of actors commenting on draft regulations and estimate the relative alignment of each of these clusters (and individual documents within them) to the draft rule and final rule. Similarly, it allows me to identify clusters of actors submitting briefs to courts reviewing these rules and their alignment with the court opinion. 

%In the remainder of this section I describe what is measured by each component of the ensemble, and then discuss how these improved measures of who participate, who lobbies together, and whose ideas end up in policy through rulemaking can provide leverage to test mechanisms of policy feedback. 



\paragraph{Measuring Policy Change}
%\subsection{What Policy Texts Do?}

As described below, I measure the extent to which the text of a policy becomes more similar or less similar to the text of each public comment. 

One way to think about this is that this change represents an increase in utility for those lobbying for the change. Purposeful actors got what they asked for and presumably reap the rewards. All dimensions of disagreement collapse to the latent dimension of utility. Actors participate and form coalitions to the extent that the expected benefits exceed the costs of doing so. Each new layer of law affects politics by altering actors' utility functions. 

Taking a broader view of politics offers a less parsimonious, but more direct interpretation. Changes in law may deliver utility, but more precisely they reflect ideas. These may be ideas about ``who gets what, when, and how,'' but also about identities, aspirations, possibilities, whose opinions matter, and who constitutes the political community, which may be difficult to reduce to a single dimension. Focusing on costs and benefits or alone, risks overlooking much of the ideological and interpretive work lawmaking does in constructing political communities, possibilities, and norms. Public comments in rulemaking, like other forms of policymaking, may often be about more than self-interest. 

Indeed ``who succeeds?'' is a constraining question laden with certain ideas of what politics really is or ought to be. A better question may be ``How do certain ideas end up in law and how has this shaped politics and law over time?'' This latter formulation recognizes that ideas may end up in law as accidental to other efforts and that politics is not only who gets what but, more fundamentally, a process of deciding who we are and what we want to do together. 

In the case of rulemaking, each notice-and-comment process creates an implicit political community based on who participates and whose interests are claimed to be represented. Regulations generally prescribe concrete rights, prohibitions, and governmental actions, making clear what was decided to be done. They also establish norms, issue frames, scientific and legal standards, and goals that inspire and constrain politics in other policy processes, especially future rulemakings on the same issue.

Importantly, few observers would see any final rule as the final word. Rules are frequently revisited at regular intervals, challenged in court, rewritten under future administrations, rebuked by a new Congress, and occasionally swept aside by new legislation. Most often, the questions in rulemaking will be revisited by the same agency and many of the same participants, but they are not the same; both have been transformed and engage the questions on terrain reshaped by added layers of law. This means that it is essential to consider the historically contingent evolution of policy and lobbying coalitions over time. Quantitative scholarship on rulemaking rarely considers the date rules are made, much less how the politics of rulemaking may be historically contingent. I aim to address this gap. 

% One way to think about the kind of political influence I aim to study is to ...

%Recognizing this, I try to ....
\paragraph{Measuring change in policy texts}

If available and reliably reflecting what actors want, policy positions expressed as texts have great advantages over those expressed in votes.

Policy disagreements are disagreements about words that give governmental force to ideas. Information is an important currency for those trying to influence policy and that rhetoric and framing can affect perceptions of facts and policies. Policy learning can be seen as an updating of where one stands, an updating of beliefs about the what is true about the world and, most specifically, an updating about which words (and thus ideas) ought to be in law. Importantly, policymakers are constantly learning about new problems, facts, and policy ideas about which they had no prior position. Thus new dimensions of disagreement are created every time claims are advanced about new problem definitions, new facts, and new ideas for what the law ought to say.

Estimating spatial ideal points is a leading way of estimating what actors want, especially when we only observe a series of votes. The quantity of interest is what kind of policy actors ideally want, and because voting only tells us whether one is for or against a policy text, we need multiple observations in which an actor (or others who plausibly share their position) falls on each side to begin to narrow down their ideological distance from any given policy. To get multiple observations, we must assume that a number of proposed policies can be placed as different points on the same underlying dimension of disagreement. When it is persuasively argued that a number of these underlying issue dimensions more or less collapse to an even more general dimension, we further increase our observations and thus the information we have about each actor regarding that more general dimension. 

When, rather than up or down votes, we have the text of what each actor wanted, we have much more information about where they stand in relation to a policy text. Indeed, instead of needing to uncover more general latent dimensions and estimate actors' positions on them, we directly observe the quantity of interest: the substance, direction, and magnitude of the disagreement in each case. We can cluster these disagreements to make more general statements about the broader nature of disagreements and relative policy potions, but, unlike with voting data, this reduction is not necessary to know the extent to which actors are getting what they want as we can directly observe how much outcome texts incorporate various actors' expressed ideal language. Flattening proposed changes in texts to \textit{n} dimensions where \textit{n} is fewer than the number of unique demands made by all actors may help descriptively, but is not required analytically. For example, we may reduce textual policy demands to left and right ideologies or preferences for more or less government, and doing so can be descriptively useful, but it is neither necessary nor helpful for answering the question of whose ideas end up in law.

Importantly, my project does not attempt to answer what actors want in general, \textit{a priori} of any policy proposal. Without some reference point (existing policy, for example) what actors want may be impossible to define. I assess what commenters want \textit{given} a proposed policy. What commenters request may not be a sincere representation of their ideal policy, but it is plausibly what they really want given what they believe is possible. While this may be insufficient for estimating ideal points, it is sufficient for measuring who gets what they ask for. 

If the primary aim is to identify policy actors' ideological proximity to each other, the analysis can be reduced to the similarity and difference in their ideal policy texts, summed across all areas or within broad policy areas. If alternatively, we ask whose ideas end up in policy, we want to know how similar the policy outcome was to the specific suggestions of each actor and the frequency of changes in their proposed direction regardless of whether they are advocating for ideologically consistent, orthogonal, or even opposite positions in different cases. In practice, the unique dimensions of disagreement in each policy process are rarely perfectly parallel or orthogonal. Mapping ideological distance requires reducing to some tractable number of dimensions. Measuring rates of getting one's way do not.

This is one great advantage of textual data compared to votes. Each observation is far richer in content and analysis requires fewer assumptions to say where actors really stand on a proposed policy. A finding that the words one actor suggested be added to a policy were twice as likely to appear in the final policy as average is a powerful and intuitive description of where power to shape policy resides.\footnote{This is, perhaps, even more powerful than saying that the policy tended to shift toward their ideal point on some latent dimension, where the exact content anchoring the ideal point and the dimension are at least slightly ambiguous.}

\paragraph{Measuring the extent commenters got what they asked for}

Participants may ask for two general kinds of things: they ask for specific changes to identified parts of the text or they may ask for a broad shift in emphasis, what \citet{Jones2005} call a policy image. For example, on the same Clean Power Plan rule, some may ask the Environmental Protection Agency to make specific changes to two sentences having to do with the classification of power plants and the division of federal and state enforcement authority. Others may ask for broad reframing to focus less on economic costs and more on environmental equity and the effects of pollution on children. Many commenters do both. 

I thus collect two key pieces of information from the rulemaking record: the text comments and the change from the draft rule to the final. I use each type of information in a different kind of analysis, one targeting specific demands and one targeting broad demands. Both approaches require the same initial steps. To identify how exactly the rule changed from draft to final, I use text reuse methods to identify what is the same and has been added or subtracted. Similarly, I identify text in comments that is not copied from the draft rule.\footnote{I may also need to exclude other kinds of text such as narratives introducing the commenter which commonly precede policy demands.} The result of these two steps is the changes requested by commenters and textual change in the rule. 

To identify the adoption of specific demands, I use the same text reuse methods to identify any matches between textual changes in the rule and the changes requested by commenters. If final rules include the specific phrases suggested in comments, this is evidence that these commenters got some of what they asked for. The significance of this kind of relationship between texts could be measured by how many words were copied, weighted by the forcefulness of these words. For example, I could create a dictionary of legally-significant words such as ``shall,’’ ``must,’’ ``enforcement,’’ and ``standard.’’ and weight textual alignment scores accordingly. Text reuse can be measured for individual commenters and averaged over coalitions. 

To identify the adoption of demands for broader shifts in policy image and emphasis, I propose a relational topic modeling approach. In contrast to the single topic model used to classify commenters into coalitions, this approach assumes that each text is a mixture over a number of topics. Each word token in a document is assigned to exactly one topic. Words and thus documents have distributions over topics. The extent to which distribution of topics that changed from the draft to the final is similar to the distribution in comments may be seen as a measure of whether the commenter got what the kind of change in policy emphasis they asked for. 

Some rulemaking processes also have a commenting period before the draft policy is published. In these cases, commenters respond to an Advanced Notice of Proposed Rulemaking (ANPRM). A similar approach can be used in these processes with the key difference that similarities between comments and the draft rule (now the outcome text), either in specific text fragments or general topic distribution, take on a different meaning. Instead of representing changes to a policy text, it may represent common understandings of what policy already was or had to be on this topic. Changes in the final rule more plausibly represent differences in what policy could be. With respect to the ANPRM and proposed rule, it is more difficult to infer that the same result would not have occurred without their comments. While such counterfactual inference is not my purpose and both measure the same core phenomena of the words actors want becoming policy, interpretation of what this means must attend to this difference. 

\Paragraph{Relating Comments to Rule Changes}

My methodological contribution is to combine topic modeling approaches with text reuse methods, allowing scholars to better understand not just what is discussed but the topic distributions of what is being added, cut, copied from others, or otherwise receiving special attention. I call this a relational topic modeling approach. Of course, all topic models focus on the relationship between text, but by making some of the text units themselves a relationship between texts with text reuse methods, the topic model takes a ``difference in difference'' (e.g. what was added or deleted) or ``difference in similarity'' (e.g. what was copied) form. Much of the rule content is retained from one version to the next, but some content often changes. We want to know how these changes relate to the changes proposed by participants. 

%\subsubsection{LDA: the Distribution of Words over Topics} 

% This section first reviews the \textit{Latent Dirichlet Allocation} (LDA: Blei et al. 2003) model, then the unique text preparation and effect estimation steps necessary to address my questions, and finally additional steps and extensions to improve topic and effect estimation. In the discussion section, I suggest additional questions that may be addressed using the text analysis approach advanced here.

To measure the relationship between comments and policy change, I draw on the \textit{Latent Dirichlet Allocation} (LDA: Blei et al. 2003) model. Unlike the model used to estimate coalition membership, this is a mixed-member model. In LDA, each document can be represented as a vector of topic proportions, i.e. what fraction of the words in that document belong to each topic (Blei et al. 2003). For example, in a model of a rulemaking under the Environmental Protection Agency's Clean Power Plan, ``climate,'' ``adaptation,'' ``carbon,'' and a dozen other words may co-occur and indicate a topic about climate change. The words, ``clean'', ``air,'' and ``health'' may also co-occur and have relatively high frequencies in a topic that seems to be about air quality.  The change in the rule from NPRM to Final Rule and each comment would have a $\pi$ proportion of words belonging to the \textit{climate change} topic (\%$\tau_{Climate}=\pi_{Climate}$). This may be a relatively high portion for Climate Action Coalition comments and a low portion for American Lung Association comments ($\pi_{Climate, CAC} > \pi_{Climate, ALA}$) compared to the air quality topic, which may be the opposite ($\pi_{AirQuality, CAC} < \pi_{AirQuality, ALA}$). If the rule changes to focus more on climate change from draft to final rule ($\pi_{Climate, \Delta EPA} > \pi_{AirQuality, \Delta EPA}$), the Climate Action Coalition may be seen as more successful than the American Lung Association with respect to the broad emphasis of the regulation. This offers a new way to capture what Jones and Baumgartner (2005) call \textit{attention allocation}, the changing weights on policy images and issues: in this case, what the Environmental Protection Agency ought to do.

\begin{figure}[h!]
\label{percent}
\caption{The Latent Dirichlet Model (LDA)}
\begin{tabular}{@{\extracolsep{5pt}}rlccl} 
& & & \\
Text: &\fbox{Comments} &  $\longrightarrow$ & \fbox{Change in Rule}\\
& & & \\
Dist. over $T$ topics (e.g. $\tau_{1:3}$): & Comment$_{j_1}$ $\sim $\{\%$\tau_{1}$, \%$\tau_{2}$, \%$\tau_{3}$\}  & & $\Delta$Rule $\sim $\{\%$\tau_{1}$, \%$\tau_{2}$, \%$\tau_{3}$\}\\
& Comment$_{j_2} \sim $\{\%$\tau_{1}$, \%$\tau_{2}$, \%$\tau_{3}$\} & & \\%$\Delta$Rule$-$ $\sim $\{\%$\tau_{1}$, \%$\tau_{2}$, \%$\tau_{3}$\}\\
& Coalition$_{(j_1+j_2)} \sim $\{\%$\tau_{1}$, \%$\tau_{2}$, \%$\tau_{3}$\} & &
\end{tabular}
\end{figure}


% For comment $j$ and rule change $\Delta r$ on topics $\tau_{1:T}$ with proportions $\pi_{j}$ and $\pi_{\Delta r}$:
% \begin{align*}
% Success_{j, \tau} \ &= 1-(\pi_{j, \tau} - \pi_{\Delta r,\tau})\\
% \end{align*}

I focus on draft-to-final rule change by selecting only the text that was added or subtracted. This can be thought of as a versioning problem where the agency updates the rule. To focus on what changed, I excluded sentences that appear verbatim in the NPRM. I use the Smith-Waterman alignment algorithm (developed for identifying DNA matches and commonly used in plagiarism software) to identify sections of text that are close matches. Wilkerson (2015) successfully employs this approach to identify content copied from various bills in the legislative processes leading to the Affordable Care Act.


%More precisely, this includes the topic distribution of text added to the rule $\pi_{\Delta r+}$ and subtracted $\pi_{\Delta r-}$:
%\begin{align*}
%Success_{j, \tau} \ &= 1-(\pi_{j, \tau} - \pi_{\Delta r+,\tau}) +  1-(\pi_{j, \tau} - \pi_{\Delta r-,\tau})\\
%\end{align*}



The percent of each topic $\tau$ within each document $j$ is estimated as $\pi_{j, \tau}$ where:
\begin{align*}
%z_i &\sim Multinomial(\theta_{D_{i, j} })\\
%\theta_k &\sim Dirichlet(\alpha)\\
%w_i &\sim Multinomial(\phi_i)\\
%\phi &\sim Dirichlet(\beta) \\
%=\\
\tau_{i, j} | W_{i, j} &\sim Multinomial(\pi_{w_{i, j} })\\
\pi_{j} &\sim Dirichlet(\alpha) \\ % should tau be there
W_{i, j} &\sim Multinomial(\rho_{\tau,w})\\ % should be W_{tao}, probability of drawing a word in W at token i is drawn from a multi distribution of the probabilities that each word is in each topic
% should be rho_{w, tau}
\rho_{\tau,w} &\sim Dirichlet(\beta) 
\end{align*}

We observe the total number of unique words ($w_1,...,w_W$) in the vocabulary of all documents and $w_{i, j}$ is the word observed at the $i$th token in document $j$. All texts are ``tokenized'' by giving each word\footnote{For topic estimation, I use single words, but tokenizing may be done by sentence or by any n-gram string of characters or words.} a unique index $i$. If token $i$ belongs to topic $\tau$, then the probability that the token is word $w$ is the topic-specific probability $\pi_{\tau, w}$. At the document level, $\pi_{\tau, j}$ %/$\theta_{i, k}$ 
is the estimated proportion of topic $\tau$ for document $j$. %, a $i$ x $\tau$ matrix of the proportion of words from each topic in each document. 

$T$, $\alpha$, and $\beta$ are defined. 
$T$ is the number of topics $(\tau_1,...\tau_T)$ where $\tau_{i, j}$ is the topic of the $i$th token in document $j$. Each token comes from exactly one topic.
$\alpha$ is the parameter of the prior on the per-document topic distributions, and
$\beta$ is the parameter of the prior on the per-topic word distributions. 
$\rho_{\tau, w}$ % / $\phi_{w, k}] / $ 
is the distribution over $w$ words in each topic $\tau$, i.e. the probability of drawing the $w$th word of the vocabulary for topic $\tau$.
%$\rho_{j,w}$ is the probability that the $i$th token of document $j$ lies in
%$\tau_{i, j}$ %/D
%, $\pi_{w_{i, j}}$.%$\theta_{D_{i, j}}$ / is this right?

%w / $\psi$ is the specific word (the only observable variable) 
% CHOOSE A WORD w[i] GIVEN THE TOPIC z

The result is a quantitative measure of the alignment between suggestions made in comments and text added or subtracted from the draft to final rule. A similar approach can estimate the relationship between ANPRM comments and the draft rule text, omitting the draft-to-final text reuse step. Credible intervals for these comment and topic-specific alignment scores can be calculated from posterior distributions. 

% \subsection{Measuring the Role of Agency Missions and Reputations in Decision making}

% \subsubsection{Measuring agency reputations}

% \subsubsection{Linking comments and reputations