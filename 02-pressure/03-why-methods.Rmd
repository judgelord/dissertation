## Testing the Theory {#why-data-methods}

To assess my theory about which groups should mobilize public participation in bureaucratic policymaking, I use public comments in federal agency rulemaking. However, my theories and methods should also apply to other kinds of political engagement, such as through social media or protests and other political decisions, including state-level rulemaking.
    
### Data {#why-data}

<!--
Bring together information on the mass commenting coding in one table with statistics and examples to summarize for the reader.

TODO: CODE
-->


```{r pressure-data}
load(here::here("data","rules.Rdata"))
load(here::here("data","topdockets.Rdata"))
load(here::here("data","topagencies.Rdata"))

# trim rules
d  <- rules %>% filter(year >2004, year <2021, 
                   docket_type == "Rulemaking",
                   document_type %in% c("Rule", "Proposed Rule"))

mass <- filter(d,campaign)
```

I collected a corpus of over `r rules$number_of_comments_received %>% sum() %>% {./1000000} %>% round()` million public comments via the
regulations.gov API. `r d$number_of_comments_received %>% sum() %>% {./1000000} %>% round()` million of these comments are on rulemaking dockets.
I then linked these comments to other data on the rules from the Unified
Agenda and Office of Information and Regulatory Affairs Reports. Summary statistics for these data are
available in the Appendix.

From 2005 to  2020, agencies posted `r nrow(distinct(d, docket_id))` rulemaking dockets to regulations.gov and solicited public comments on `r nrow(distinct(d %>% filter(open), docket_id))`. Only `r nrow(distinct(mass, docket_id))` of these rulemaking dockets were targeted by one or more public pressure campaigns, but this small share of rules garnered `r percent(sum(mass$number_of_comments_received)/sum(d$number_of_comments_received), accuracy = .01)` (`r sum(mass$number_of_comments_received)`) of all comments. Nearly all of these comments are form letters. The top 10 rulemaking dockets account for `r percent(sum(topdockets$docket_comments)/sum(d$number_of_comments_received), accuracy = .01)` (`r sum(topdockets$docket_comments)`), of all comments in agency rulemaking. Again, nearly all of these are form letters.


Table \@ref(tab:dockets) shows the rules that received the most comments on regulations.gov.  Proposed rules that have attracted the most public attention have
    been published by the Environmental Protection Agency (EPA),
    the Department of Interior (DOI), the Bureau of Ocean Energy
    Management (BOEM), the Consumer Financial Protection Bureau (CFPB),
    and Fish and Wildlife Service (FWS). 
    The most commented-on rule was the 2013 "Clean Power Plan"---the Obama administration's flagship climate policy. 

```{r dockets}
# Include plot of draft and final rules over time, report number of pairs
d %>%  
  filter(open) %>% 
    distinct(docket_title, docket_comments, docket_id) %>%
    ungroup() %>%
    group_by(docket_id) %>% 
    slice(n = 1, with_ties = FALSE) %>% 
    ungroup() %>%
    arrange(-docket_comments) %>% 
    head(10) %>%
  # trim docket title 
    mutate(docket_title = docket_title %>% str_sub(0,150)) %>%
    mutate(docket_title = ifelse(nchar(docket_title) == 150, str_c(docket_title, "..."), docket_title) ) %>%
  mutate(docket_comments = docket_comments %>%  pretty_num() ) %>%
  rename(`Docket ID` = docket_id,
         `Docket Title` = docket_title,
          `Total Comments` = docket_comments) %>%
    kable3(caption = "Rulemaking Dockets by Number of Public Comments, 2005-2020",
         full_width = T,
         font_size = 10)
```




Figure \@ref(fig:rules-by-campaign) shows a massive rise in the number of proposed rules targeted by public pressure campaigns (the right panel), greater than the overall increase in the number of proposed rules posted for comment on regulations.gov (the left panel).  To some extent, the increase from 2005 to 2010 results from agencies using regulations.gov more systematically in the years after its launch in 2003. But the ease of online organizing has also increased the frequency of public pressure campaigns. As mentioned earlier, less than 5 percent of proposed rules each year are targeted by a pressure campaign (note the necessary difference in the y-axes). However, this share is growing.

Colors show the handful of agencies that publish the majority of proposed rules on regulations.gov (out of the `r count(rules, agency) %>% nrow() ` federal agencies that use regulations.gov). For the most part, these are also the agencies most often targeted by public pressure campaigns, but some agencies are relatively more and less likely to be targeted than others. For example, the U.S. Fish and Wildlife Service (dark red) publishes a small share of all rules but a large share of rules targeted by public pressure campaigns (many protecting threatened and endangered species habitat). In contrast, the U.S. Coast Guard (red) and Federal Aviation Administration (light blue) both publish a large number of rules (mostly regulating transportation safety), but pressure campaigns rarely target these agencies.  


```{r rules-by-campaign, fig.cap = "Proposed Rules Open for Comment on Regulations.gov 2005-2020"}
include_graphics(here("figs", "rules-by-campaign-1.png"))
```


Figure \@ref(fig:comments-per-year) shows the total number of public comments received per rule from 2005-2020.
Figure \@ref(fig:comments-per-year) also shows an increase in the number of rules receiving a large number of comments from 2005 to 2020. Note that comments per rule (the y-axis) are on a
logarithmic scale in order to see this variation. While most rules receive few comments, there is a steep rise in the number of rules receiving over a thousand from 2005 to the mid-2010s. We see this same trend for the number of rules receiving over 100 thousand rules, peaking around the time that the Obama EPA's Clean Power Plan (the administration's landmark climate change policy, rulemaking docket EPA-HQ-OAR-2013-0602) received over 4 million comments (highlighted in Figures \@ref(fig:comments-per-year) and Figure \@ref(fig:comments-per-rule)). Each year since then, hundreds of rules received over a thousand comments, and dozens received over 100 thousand comments. 

While the average number of comments per rule is consistently around ten, more people are involved in more policy processes today than a decade ago. 
Occasionally, a large number of people are
engaging in agency policymaking. It is not a coincidence that more people are engaging in a select set of policies as pressure campaigns target more agency rules (though still a small portion).

```{r comments-per-year, fig.cap = "Number of Comments (log scale) per Proposed Rule 2005-2020", out.width = "90%", fig.show = "hold"}
knitr::include_graphics(here::here("Figs", "comments-per-rule-3.png"))
```

Figure \@ref(fig:comments-per-rule) clearly shows the inequality in public participation across rulemakings. Of over 25 thousand proposed rules open for comment on regulations.gov between 2005 and 2020, Figure \@ref(fig:comments-per-rule) shows that over a third of them received no comments. These rules appear as a long line of points at the bottom of the plot. Approximately another third received ten or fewer comments, including the U.S. Department of Agriculture's Agriculture Management Services rule regulation the labeling of honey (AMS-FV-07-0008). As in Figure \@ref(fig:comments-per-rule), I also labeled the Clean Power Plan rule. 
A version of Figure \@ref(fig:comments-per-year) split out by year is available in the appendix (Figure \@ref(fig:comments-per-year-app)).

```{r comments-per-rule, fig.cap = "Number of Comments (log scale) per Proposed Rule 2005-2020", out.width = "90%", fig.show = "hold"}
knitr::include_graphics(here::here("Figs", "comments-per-rule-1.png"))

```


#### Policy Advocacy Organizations: From Grassroots to Astroturf {#astroturf}

Testing my hypotheses requires that I classify campaigns as driven primarily by public or private interest groups. This is a challenge because appeals to the government are almost always couched in the language of
public interest, even when true motivations are private
[@Schattschneider1975]. Public pressure campaigns are no exception, and mobilizing organizations almost always evoke some version of the public interest. Classifying thus involves judgment calls. 
I describe my classification methods in section \@ref(classify-public-private). To provide empirical context, this subsection sketches out the range of public and private campaigns with some concrete examples of "public" pressure campaigns that primarily advance private interests.

There is a spectrum of organizing the unorganized from more "grassroots" to more "astroturf" strategies. On the grassroots end, engagement is driven by a combination of passionate volunteerism and a supportive, attentive segment of the public. In practice, most campaigns on the grassroots end of the spectrum in federal rulemaking are not pure volunteerism but are organized by policy advocacy organizations like MoveOn and Organizing For America on the left and Americans for Prosperity and the National Association For Gun Rights on the right.
These organizations have large mailing lists and media operations, providing the capacity to mobilize large numbers of people for a particular policy fight. Both public and private interest groups pay for mobilizing services and software. Some providers are nonprofits (e.g., Care2); others are for-profit lobbying and campaign consultants (e.g., Nationbuilder, SoftEdge, Mandate Media). Most of these services have strong partisan ties, as is generally the case with lobbying firms [@Furnas2017].
Membership organizations like the Sierra Club often mobilize "members and supporters" beyond their official membership base, thereby taking the form of a policy advocacy organization, as well. 

Like people mobilized through their membership organizations, people mobilized by policy advocacy organizations will often cite the mobilizing organization.
Unlike those mobilized through membership organizations, mobilization by policy advocacy organizations is more likely to be concentrated in a few large organizations with the specific resources for running campaigns that engage passionate or interested but unaffiliated or loosely affiliated segments of the public. 


Toward the astroturf end of the spectrum, well-funded efforts gather signatures from a much less passionate and attentive population. Where grassroots organizing relies on existing underlying interests that merely need to be given an opportunity to engage, people engaged by astroturf campaigns are generally disinterested in the policy and engage merely because of paid ads or petition-gathering, often involving some deception (e.g., they are intentionally misled about the policy or its likely effects) to get people to take action on an issue that they would not take if the issue were presented more clearly. Likewise, the organizations collecting the signatures would have no interest in doing so if they were not paid. The aim is to give an appearance of support.
To the extent they mobilize real people, astroturf campaigns are thus a form of outside lobbying intended to create a deceptive appearance of public support. In the extreme, astroturf campaigns may use the names of fake or non-consenting individuals---inside lobbying is thus disguised as outside lobbying. 

For example, in 2016, the Bureau of Ocean Energy Management received several USB drives with hundreds of thousands of comments on its National Outer Continental Shelf Oil and Gas Leasing Program from Joe Jansen. Jansen did not disclose who he worked for, but the form letters, each identical except for the signature, resembled press releases from the American Petroleum Institute (API), the main industry association for oil and gas companies. According to a LinkedIn profile and [Congressional Directory](https://www.govinfo.gov/content/pkg/CDIR-2011-12-01/pdf/CDIR-2011-12-01-OH-H-1.pdf), Jansen was a former legislative director for a Republican member of Congress who now worked in Government Relations.  Unlike more "grassroots" campaigns, no information was provided about who the signatories were or why they cared about oil and gas leasing. Joe Jansen, however, was also associated with other campaigns targeting the EPA and Department of State, several of which identified themselves as organized by the groups "Energy Citizens" and "Energy Nation." These organizations' websites are paid for by the American Petroleum Institute. The photos they post on social media almost exclusively show employees handing out shirts, hats, and water at fairs, bars, and conferences in exchange for signatures [@EnergyCitizen2014astroturf]. Though Energy Citizens and Energy Nation submitted slightly different comments as separate organizations, most of the individual signatories were the same on both sets of comments, and many signatures were submitted twice by each organization, successfully inflating the reported number of pro-API comments on the rule. Energy Citizens has attracted media attention for bussing in paid actors to protests and town halls to oppose regulations @NYT2009astroturf], paying actors to pose as concerned citizens, and skirting Facebook's policy against deceptive advertising [@ProPublica2018astroturf].

In a more complex example, Axcess Financial and other payday lending companies sponsored several campaigns targeting a regulation proposed by the Consumer Financial Protection Bureau in 2016. First, Axcess Financial had storefront employees solicit comments from customers, which Axcess then uploaded to regulations.gov.  The customer comments suggest that they had not been told much about the rule, which limited interest rates, fees, and the number of times short-term loans could be compounded. Most customers wrote some version of "Do not close this store" or "I have been told that payday loans would not exist in my community if the government's proposed regulations went into effect." A few even complained about exactly the issues that CFPB's regulation aimed to address. One customer wrote, "Although some of the fees are a bit high, it should be my choice whether to get a loan or not" (Access Financial Comment 91130). Another wrote, "I need to keep receiving my Check'n'Go loans so I can have the time to start paying them back in the next 1 1/2 to 2 years" (Axcess Financial Comment 91130), indicating that Check'n'Go (a subsidiary of Axcess Financial) was engaged in serial re-lending (repeatedly issuing short-term high-interest loans to pay interest and fees on previous loans of the same type) that put this customer deeper in debt. In their own comments, Axcess claimed that it did not do this kind of serial re-lending. To the extent that this campaign relied on deception and not the customersâ€™ genuine interests (even as the customers understood them), this would count as astroturf. 

Second, Axcess sponsored an effort to gather signatures at churches. Finally, Axcess and other payday loan companies uploaded supportive notes from community organizations to which they had given money. It is unlikely that members of these organizations would have commented had they not been paid by Axcess.  

As the American Petroleum Institute and Axcess Financial examples demonstrate, spotting astroturf in the wild can be difficult by design and involve complex judgment calls about the level of deception involved. However, the clear observable result is often a large number of comments advocating on behalf of narrow private interests. Large businesses or industry associations are the organizations with the resources and incentives to sponsor astroturf campaigns, and they do [@Lyon2005].

Not all campaigns on behalf of private interests fall decisively on the astroturf side of the spectrum. In a cover letter to a batch of comments opposing the regulation of glyphosate herbicides, Monsanto, a major glyphosate manufacturer, described how they collected the letters:

> "These letters were collected during the 2016 Farm Progress Show from U.S. farmers, agriculture professionals, and general consumers who use glyphosate and value its benefits. We think it is important that these voices be heard as part of EPA's review of glyphosate." ([EPA-HQ-OPP-2009-0361-0891](https://www.regulations.gov/comment/EPA-HQ-OPP-2009-0361-0891))

Monsanto may have, like Energy Citizens, given out hats and shirts in exchange for many of these signers. Still, the context and transparency make it more plausible that the petition signers genuinely opposed regulation on glyphosate. Thus, I do not code this as astroturf. Similarly, Shell Oil sponsored a campaign to open the Arctic Outer Continental Shelf to oil and gas drilling and provided a template letter with a place to insert a company or group: 

> "On behalf of [enter company or constituents], I am writing to demonstrate my strong support of oil and gas development in the Arctic Outer Continental Shelf (OCS)...I support Shell's plan to explore its leases in the Chukchi Sea in 2015. The company has invested significant time and resources in the advancement of safe and prudent Arctic exploration. Shell should be allowed to realize the promise of the leases it purchased, and I encourage the BOEM to expeditiously approve its Exploration Plan."^[Some of Shell's supporters neglected to fill in the blanks in the template letter ([BSEE-2013-0011-0033](https://www.regulations.gov/comment/BSEE-2013-0011-0033)).]

Though Shell stood to profit from the rule, the signers of this form letter were mostly companies and workers in the oil and gas sector. Several elected officials also used Shell's form letter (e.g. [BSEE-2013-0011-0033](https://www.regulations.gov/comment/BSEE-2013-0011-0033) and [BSEE-2013-0011-0094](https://www.regulations.gov/comment/BSEE-2013-0011-0094)). I found no evidence of deception or payments from Shell. These companies, workers, and politicians plausibly had a genuine interest in Shell's access to offshore oil. The form letter's transparency about who stood to benefit further increases the plausibility that signers genuinely supported Shell's lobbying effort. Again, this means that it was not coded as astroturf. 


<!--
CITE STEVES ASTROTURF PAPER AND RELATED LIT IN THEORY SECTION
-->


<!-- ACTUAL METHODS SECTION-->

### Methods: Measuring Public Pressure and Political Information {#why-methods}

<!--
In section \@ref(why-theory), I argued that we should observe different patterns of public
participation depending on whether an organization launches a public pressure
campaign as an outside lobbying tactic ("going public"), as a reaction to such a campaign ("breaking a perceived consensus"), or
for reasons other than influencing policy ("credit claiming" or "going down fighting").-->

In this section, I develop methods to identify public pressure campaigns and measure the kinds of political information they create. These measures
capture similar statistics to questions posed by @Verba1987 [p. 9]: "How
much participation is there, what kind is it, and from what segments of society does it come?"
Specifically, I assess the extent to which public comments are mobilized by pressure campaigns, which organizations are behind these campaigns, which campaigns are more successful in mobilizing, and which campaigns go unopposed. 


#### Identifying Organizations and Coalitions using Text Reuse {#reuse}

The primary unit of analysis is a lobbying coalition---a group of organizations advocating for the same policy changes in their comments on a draft rule. 
Advocacy organizations work together on campaigns. For example, "Save our Environment" submitted both sophisticated comments and collected signatures from hundreds of thousands of people on several rulemaking dockets. Save our Environment is a small nonprofit with a simple WordPress website almost entirely dedicated to mobilizing public comments. It is run by The Partnership Project, a coalition of 20 of the largest environmental advocacy organizations in the United States, including the Sierra Club, Natural Resources Defense Council, Greenpeace, and the World Wildlife Fund, with the aim of "harnessing the power of the internet to increase public awareness and activism on today's most important environmental issues" (SaveOurEnvironment.org). Several Partnership Project members, including the Sierra Club, EarthJustice, and NRDC, also submitted technical comments and mobilized hundreds of thousands of their own supporters to comment separately on the same rules.  These lobbying and mobilizing activities are not independent campaigns. These organizations and the people they mobilize are a coalition.

To mobilize broader support, advocacy organizations often engage smaller organizations, which, in turn, mobilize their own members and supporters, often with logistical support and funding from the larger national organization. For example, for the same campaign where the Gulf Restoration Network mobilized hundreds of restaurants that serve sustainable seafood, one of their larger coalition partners, the Pew Charitable Trusts, mobilized thousands of individuals, including members of the New York Underwater Photography Society. These smaller organizations did not identify themselves as part of Pew's campaign, but their letters used almost identical language.

<!-- step 1 portion from campaigns-->
Identifying which people and organizations belong to which coalition is thus a crucial first task for any study of public pressure campaigns.
To identify whether a pressure campaign mobilizes a given comment, I use several strategies.
 I first use textual similarity to identify
clusters of similar comments, reflecting formal and informal coalitions.
Comments with identical text indicate a coordinated campaign. 

To link individual comments and public pressure campaigns to the more sophisticated lobbying efforts that they support (if any), I identify the lobbying coalition(s) (if any) to which each comment belongs. Some individual commenters and organizations are unaffiliated with a broader lobbying coalition, but, as I show below, most people and organizations lobby in broader coalitions.

Importantly, even
campaigns that achieve very low public response rates appear in these
data. Because campaigns aim to collect many thousands of comments, it is
implausible that even the most unpopular position would achieve no
supportive responses. For example, @Potter2017 found Poultry Producers
averaging only 319 comments per campaign. While this is far from the
Sierra Club's average of 17,325 comments per campaign, it is also far
from zero. (These numbers are from Potter's sample of EPA rules; the Sierra Club's average is even larger in my sample; see Table \@ref(tab:toporgs).)

For each comment on a rulemaking docket, I identify the percent of words it shares with other comments using a 10-word (or
"10-gram") moving window function, looping over each
possible pair of texts to identify matches.^[For more about n-gram window functions and comparisons with related partial matching methods such as the Smith-Waterman algorithm, see @Casas2017 and @Judge-Lord2017.]
When actors sign onto the same comment, it is clear that they are
lobbying together. However, various businesses, advocacy groups, and
citizens often comment separately, even when they are aligned. Text-reuse (using the same ten-word phrases) captures this alignment. When individuals use identical wording, I interpret that to mean they're endorsing the same policy position as part of a lobbying coalition.

Figure \@ref(fig:percent-match) shows the percent of shared text for a sample of 50 comments on the Consumer Financial Protection Bureau's 2016 Rule regulating Payday Loans. Comments are arranged by the document identifier assigned by regulations.gov on both axes. 
The black on the diagonal indicates that each document has a perfect overlap with itself. Black squares off the diagonal indicate additional pairs of identical documents. For example, 100% of the words from Comment 95976 are part of some tengram that also appears in 95977 because the exact same comment was uploaded twice. 
The cluster of grey tiles indicates a coalition of commenters using some identical text.
Comments [91130](https://www.regulations.gov/document?D=CFPB-2016-0025-91130) through [91156](https://www.regulations.gov/document?D=CFPB-2016-0025-91154) are all partial or exact matches. All are part of a mass comment campaign by Access Financial. The percent of the identical text is lower than many mass-comment campaigns because these are hand-written comments, but the n-gram method still picks up overlap in the OCRed text in the header and footer. Tengrams that appear in 100 or more comments indicate a mass comment campaign. Some agencies use similar "de-duping" software [@Rinfret2021] and only provide a representative sample comment. In these cases, my linking method assumes that the example comment is representative, and I link these comments to others based on the text of the sample comment provided.

```{r percent-match, fig.show = "hold", out.width = "100%", fig.cap="Example: Identifying Coalitions by the Percent of Matching Text in a Sample of Public Comments"}

knitr::include_graphics("Figs/comment_percent_match_plot.png")  
```

Where a new presidential administration solicited comments on a proposed rule tied to a docket number that a previous administration also used to solicit comments on a different previous rule, I count these as separate rulemaking dockets. I do so because the second policy is usually reversing or going in the opposite direction as the policy on which the previous administration solicited comments. Many of the same organizations comment but with the opposite positions; support becomes opposition and vice versa.


#### Hand-coded Organizations and Coalitions

<!-- step 2 org type and coalitions-->
Second, I hand-code several samples of comments. One sample contains at least one comment from each cluster (coalition) of 100 or more similar comments. This census of form-letter comments allows me to make valid observations about public pressure campaigns across agencies and over time. A second sample includes nearly all comments on a random sample of rules.  A third sample includes nearly all comments on another random sample of rules, weighted by the number of comments they received. These last two samples allow me to make inferences about lobbying coalitions that do and do not use public pressure campaigns. 

Through an iterative process of hand-coding and computational methods, I then identify the organization that is submitting or is responsible for mobilizing each comment (if any) in all three samples of comments. This process involves using regular expressions to search comment texts and metadata for possible names. With a team of research assistants, I inspect a sample and link it or add it to a growing list of organizations known to comment. This corpus of known organizations is then included in the next text search. 

With this approach, I identify the organizations responsible for over `r 40` million comments, including all organizations responsible for mobilizing 100 or more comments with repeated text--either identical text or partially unique texts that contain shared language.



#### Classifying Public and Private Interests {#classify-public-private}

<!--TODO: NAIL DOWN MY NOMENCLATURE.-->

In addition to classifying all organizations that appear in the hand-coded samples as businesses, industry associations, other nonprofits, governments, or individual elected officials and a range of subtypes within these broader categories, I also classify the coalitions in which they lobby. 

Classifying coalitions as primarily driven by private or public interest provides analytic leverage, but scholars have not converged on an approach to do so. @Potter2017 distinguishes "advocacy groups" from "industry groups." @Berry1999 calls these groups "citizen groups" and emphasizes conflict over cultural issues. Some public interest groups focus on conservative or progressive cultural issues, like religious education, immigration, or endangered species. Others are more focused on the public provision or protection of public goods such as national parks, consumer product safety standards, air quality, drinking water, and public safety. Types of membership organizations that are both broad and focused on material outcomes for their members (such as labor unions) are especially difficult to classify. @Potter2017 puts unions in the "Industry" category. I take a different approach based on the coalition with whom such groups lobby. If a union lobbies alongside businesses, I classify this as a private interest-driven coalition [@Mildenberger2020]. If a union lobbies with public interest groups on public health or safety issues, I classify this as a public interest group coalition.

<!-- step 3 public vs private coalitions-->
I code each coalition as primarily advancing an idea of the public interest or more narrow private interests. Public interest coalitions are almost always entirely nonprofits and governments, and private interest coalitions tend to be companies and industry associations. Still, some nonprofits lobby on behalf of companies, and some companies join forces with public interest groups. These can create "hard" cases. For example, a coalition of environmental groups mobilized recreational fishing businesses and sustainable seafood restaurants to help push for stricter commercial fishing regulations. We know that the environmental groups mobilized the restaurants because they used a form letter from a nonprofit called the Gulf Restoration Network ([NOAA-NMFS-2012-0059-0185](https://www.regulations.gov/comment/NOAA-NMFS-2012-0059-0185)). This was coded as a public interest coalition. If instead, the businesses had led this lobbying effort and enlisted a few nonprofits to help protect their business interests, it would have been coded as a private interest coalition. The vast majority of coalitions were much more straightforward to code as public or private.

<!-- DOES ASTROTURF SECTION BELONG HERE?-->

#### Coding Policy Positions

To assess whether organizations and their broader coalitions lobby unopposed or in opposition to other interests, I code the position of each organization on each proposed policy given the direction of change from the current policy. Specifically, I trained research assistants to place comments on a spatial scale relative to change between the status quo and proposed rule like the one shown in Figure \@ref(fig:spatial-coding). In Figure \@ref(fig:spatial-coding), $x_1$ is the current (status quo) policy and $x_2$ is the new proposed policy on which commenters are commenting. Let $p_i$ be commenter $i$'s ideal policy. In Section \@ref(formal) and Appendix \@ref(formal-app), I formalize intuitions about why a commenter may comment and how it may influence a policymaker. Here, I merely aim to clarify the coding of policy support and opposition, which relies on the spatial coding of each comment (for more details, see the Codebook in Appendix \@ref(codebook)).


```{r spatial-coding, fig.cap= "Coding the Spatial Position of Comments on Proposed Policy Changes", fig.height=4, fig.width=5.5}
include_graphics(here("figs", "spatial-coding-1.png"))
```


In spatial models, whether an organization supports or opposes a proposed policy change generally depends on whether the policy is moving closer or further from its ideal policy. For example, if the ideal point of commenter $1$ is the current policy (i.e., $p_1 = x_1$) or close to it, they will oppose any proposed change. Likewise, if the ideal point of commenter $2$ is the new proposed policy ($p_2 = x_2$) or closer to it, they likely support the proposal. 

While incompatible with an assumption of single-peaked preferences assumed by most models, commenters do occasionally oppose a policy change for moving insufficiently in their preferred direction (e.g., describing the proposal as "too little" or "insufficient" to gain their support). For example, if a commenter prefers a more extreme change and will not accept anything less than a certain level of change ($p_i \ge x_3$), they may oppose $x_2$ as "insufficient." This is likely a result of the repeated game nature of policymaking, where commenters believe that rejecting a small change in their preferred direction ($x_2$) now is likely to result in a more extreme and preferred change ($x_3$) later.

If a commenter made statements like "We need stronger, not weaker regulations" or "These regulations are already bad for our business, we should not make them even more strict," they were coded as opposed to the proposed rule for moving in the wrong direction ($p_i < x_1$). If the commenter expressed a preference for the status quo over the proposed rule ($p_i = x_1$), they were also coded as opposing the proposed rule. 

Conversely, when a comment included statements like "we applaud EPA's efforts to regulate, but would prefer less severe limits," this was coded as supporting the rule but asking for less change. If the commenter expressed unqualified support for the proposed rule ($p_i = x_2$) or requested even more policy change ($p_i > x_2$) they were almost always coded as supporting the rule. 

Opposition to a proposed rule because it was insufficient to gain the organization's support was rare but did occur. For example, one commenter stated that "[w]hile the proposed rule may improve current protections to some degree, it is utterly inadequate...If the agency fails to revise the rule to incorporate such measures, then they should withdraw the proposed rule completely." ([NOAA-NMFS-2020-0031-0668](https://www.regulations.gov/comment/NOAA-NMFS-2020-0031-0668)). Taking the commenter at their word, this was coded as opposition to the proposed rule, even though the commenter's spatial position (closer to the proposed rule than the current policy) would have them supporting the rule in a non-repeated game assuming single-peaked preferences.

Having identified the coalition lobbying on each proposed rule and each organization's position, I assign each coalition's position as the position of the lead organization. For robustness, I also calculate the coalition's average position as the average position of its members. Coalition members usually have nearly identical positions, but occasionally, some take more extreme positions than others. For example, while all coalition members may have the same policy demands, some may ask for additional changes. I consider diverging interests to be one coalition only if the asks are entirely compatible with the position of organizations that did not ask for them. Conflicting policy demands indicate different coalitions.



#### Differences with Prior Studies

<!-- not the same as Balla et al.-->

This approach differs from previous studies of mass comment campaigns in at least two major ways. First, my methods allow me to identify coalitions consisting of multiple organizations. Previous studies measure mass comment campaigns at the organization level. For example, @Balla2020 analyzes "1,049 mass comment campaigns that occurred during 22 EPA rulemakings"---an average of nearly 50 "campaigns" per rule. By "campaign," @Balla2020 mean an organization's campaign rather than a coalition's campaign. Especially on EPA rules, there are rarely more than two or three coalitions engaging in public pressure campaigns---one of the environmental advocacy groups and their allies, another of regulated industry groups and their allies, and, occasionally, a coalition of tribal governments primarily concerned with sovereignty issues. 

This is important because many comments nominally submitted by a small business, nonprofit, or membership organization are part of a campaign sponsored by a larger coalition led by industry associations or public interest groups. It would be inaccurate to credit a small organization with little capacity for organizing a campaign when they merely allowed their name and mailing list to be used by a larger group. For example, campaigns by industry associations are often officially submitted by much smaller nonprofit coalition partners. Using organizations as the unit of analysis means that observations are far from independent. An analysis that counts one coalition's campaign as 40 smaller "campaigns" with the same policy demands would count this one campaign as 40 observations. My methods allow me to measure levels of public pressure per organization *and* per coalition. 

The second major difference between my approach and previous research is that I do not compare sophisticated comments to mass comments. Rather, I *attribute* mass comments to organizations and coalitions that also submit sophisticated technical comments. By measuring comments per coalition, both through hand-coding and text reuse, I capture different levels of public pressure than we would see if we were to look only at comments per organization. 









<!-- TODO (some or all of this needs to go to the influence chapter)

### Measuring the Volume, Intensity, and Potential Contagion of Public Engagement

I measure variation in engagement in three ways that provide estimates of the three types of comments described above.

**Volume.** 
First, I count the total number of comments per organization, coalition, and rule. This captures the number of supporters who invested at least a minimal level of effort.
The process that generates mass comments involves several steps. First, a group must decide to
lobby. Then, a group must decide to mobilize public pressure. Finally, the resulting number of comments depends on the response to the
campaign. There may be many cases where groups may have had
success mobilizing but never reached the choice of whether to mobilize
or not. For example, relevant groups may have been unaware of the draft rule or lacked the resources to launch a campaign. Once the decision
to mobilize has been reached and made, the response to mobilizing is a
count process. I thus expect the count of comments across rules to
follow a zero-inflated negative binomial distribution.

**Effort.** 
I measure effort per comment by the number of words people
write, omitting any text longer than ten words that is not unique to that comment. When text is not unique, it is usually because a mobilizing organization provided it. For example, the Sierra Club mobilized more than 47,710
people to submit the same text on the delay of the methane
pollution rule, but 7,452 people also took the time to write a
personalized comment in addition to the text provided (see Figure \@ref(fig:sierra)). Organizations often encourage "personalized notes" because the number of additional words shows the individual was willing to exert additional effort.
 As @Verba1987 note, signing a form letter requires "some" effort, whereas writing an original letter to a government official requires "a lot." Adding a personal note to a form letter is somewhere in between. The longer the letter, the more effort required. 

While effort, as measured by the number of words people write, may be normally distributed, the low end of the observed distribution is truncated.
This is because we will not observe people who have low levels of passion for the issue; they either do not meet the threshold required to participate or opt
to write nothing more than the form letter. 

**Contagion.** Public pressure campaigns have wildly different results.
Some organizations submit a clean 10,000 copies of (signatures on) the same comment.^[For example, Americans For Prosperity's campaign on the XXXX rule. TODO]
Other campaigns "go viral"---inspiring further engagement where the
original messages are translated through social media posts and news
stories. To identify people who were plausibly mobilized indirectly by a
campaign, I count the number of people who use a similar distribution of
words to that of the form letter but fewer than ten words matching any other comment. This is a regular count process. In the hand-coded sample, these people are coded as "individual" members of the coalition. Though perhaps officially unaffiliated, they were clearly inspired by the campaign indirectly.

> TODO: Align this with my terminology in the theory section.

-->
