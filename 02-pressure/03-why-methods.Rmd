## Testing the Theory {#why-data-methods}



I focus on public comments in federal agency rulemaking, but the theories and methods here may also apply to other kinds of political engagement such as through social media or protests as well as to other political decisions, including state-level rulemaking.^[Social media
    engagement may be especially important if agencies implement the
    recommendations of @ACUS2018 that "Agencies should consider using
    social media before or in connection with direct final rulemaking to
    quickly identify whether there are significant or meaningful
    objections" (p. 34).] 
    
### Data {#why-data}

<!--
Bring together information on the mass commenting coding in one table with statistics and examples to summarize for the reader.
-->

To assess my hypotheses about the extent to which more powerful or less powerful groups mobilize public participation in bureaucratic policymaking, I collected a corpus of approximately 80 million comments via the
regulations.gov API. About 50 million of these comments were on proposed
rules (over 16,000 proposed rules from 144 agencies from 2005 to 2020). Of these, public pressure campaigns targeted approximately 400 rules from 46 agencies.
I then linked these comments to other data on the rules from the Unified
Agenda and Office of Information and Regulatory Affairs Reports on draft
rules sent to them for review. Summary statistics for these data are
available in the Appendix.

Importantly, even
campaigns that achieve very low public response rates appear in these
data. Because campaigns aim to collect thousands of comments, it is
implausible that even the most unpopular position would achieve no
supportive responses. For example, @Potter2017 found Poultry Producers
averaging only 319 comments per campaign. While this is far from the
Sierra Club's average of 17,325 comments per campaign, it is also far
from zero.



Figure \@ref(fig:comments-per-year) shows an exponential increase in the number of comments per rule over time. Note that comments per rule are on a
logarithmic scale on the y-axis. Proposed rules that have attracted the most public attention have
    been published by the Federal Communications Commission (FCC,
    omitted from this plot), the Environmental Protection Agency (EPA),
    the Department of Interior (DOI), the Bureau of Ocean Energy
    Management (BOEM), the Consumer Financial Protection Bureau (CFPB),
    and Fish and Wildlife Service (FWS).] Increasingly, a large number of people are
paying attention to agency policymaking.

```{r comments-per-year, fig.cap = "Comments per Proposed Rule and Total Comments per Year", out.width = NULL, out.height = "25%", fig.show = "hold"}

knitr::include_graphics(here::here("Figs", "rules-comments-per-year-1.png"))

knitr::include_graphics(here::here("Figs", "comments-mass-vs-unique.pdf"))

```

Most public comments are, in fact, of the type suggested by the
solicitations on Regulations.gov---ordinary people voicing opinions on a proposed policy. They do not provide useful technical information or
suggest specific edits to policy texts like the interest group comments
that have thus far captured the attention of political scientists. If
they add information to rulemaking, it is a different, more political
flavor of information. Indeed as Figure
\@ref(fig:comments-per-year) shows, every year since 2008, most people who
comment on draft regulations have done so as a result of a public pressure campaign.^[At least for agencies participating in regulations.gov. See
    sections
    \@ref(why-methods) for my definition and methods for identifying mass comments.] Public engagement in rulemaking is highly
clustered on a few rules made salient by these campaigns. 

### Methods: Measuring Public Pressure and Political Information {#why-methods}

In section \@ref(why-theory), I argued that we should observe different patterns of public
participation depending on whether an organization launches a public pressure
campaign as an outside lobbying tactic ("going public"), as a reaction to such a campaign ("breaking a perceived consensus"), or
for reasons other than influencing policy ("credit claiming" or "going down fighting"). In this section, I develop methods to measure these patterns. These measures
capture similar statistics to questions posed by @Verba1987 [p. 9]: "How
much participation is there, what kind is it, and from what segments of
society does it come?"

To identify the groups mobilizing public pressure campaigns, I develop methods to attribute individual comments to the
campaigns that mobilized them and to measure the intensity of preferences
expressed. To link individual comments to the more sophisticated
lobbying efforts they support, I first use textual similarity to identify
clusters of similar comments, reflecting formal and informal coalitions.
Comments with identical text (if any) indicate a coordinated campaign. Through an iterative process of hand-coding and computational methods, I then identify the organization responsible for mobilizing each comment and group organization into coalitions. 

This approach is significantly different than that employed by previous studies in at least two ways. First, I identify coalitions consisting of multiple organizations. Previous studies measure campaigns at the organization level. For example, @Balla2020 analyze "1,049 mass comment campaigns that occurred during 22 EPA rulemakings"---an average of nearly 50 "campaigns" per rule. By "campaign," @Balla2020 mean an organization's campaign rather than a coalition's campaign. Especially on EPA rules, there are rarely more than two or three coalitions engaging in public pressure campaigns--one of environmental advocacy groups and their allies, another of regulated industry groups and their allies. This is important because many comments that might be attributed to a small business, nonprofit, or membership organization are part of a campaign sponsored by a larger coalition of industry or public interest groups. It would be inappropriate to credit a small organization with little capacity for organizing a campaign when they merely allowed their mailing list to be used by a larger group. Using organizations as the unit of analysis may cause problems. If an analysis counts one coalition's campaign as 40 smaller "campaigns" with the same policy demands and response, this one campaign would be counted as 40 non-independent observations. My methods allow me to measure levels of public pressure per organization and per coalition. 

The second major difference between my approach and previous research is that I do not compare sophisticated comments to mass comments. Rather, I *attribute* mass comments to organizations and coalitions that also submit sophisticated technical comments. This difference is less important to the present analysis of the drivers of public pressure campaigns, but the set of comparisons one makes is critical to any study of responsiveness or policy influence. Researchers may reach different conclusions if they compare different things. Consider a study comparing agency responsiveness to Sierra Club form letters to agency responsiveness to the Sierra Club's sophisticated comments. Now consider a study that compares responsiveness to the Sierra Club's sophisticated comments across rules where they did and did not run a mass comment campaign. Likewise, a study comparing the average influence of form-letter comments to the average influence of sophisticated comments is very different from a study that compares the influence of two sets of sophisticated comments with different *levels* of public pressure behind them. By measuring comments per coalition, both through hand-coding and through text reuse, I capture different levels of public pressure than we would see if one were to look only at comments per organization. 

For each campaign, I measure
the intensity and potential for the movement to grow. To measure
intensity, I examine the ratio of high-effort and low-effort comments.
To measure the potential to grow, I measure the number of comments mobilized
indirectly by the campaign (i.e., those that support a campaign but do
not include text provided by the campaign). The result is several new
measures of participation in bureaucratic policymaking.




<!--
### Spontaneous or mediated?

### If mediated, who is mobilizing
-->

### The observable implications of various forms of public engagement

The different forms of participation asserted or assumed in the literature have empirically observable patterns. 

#### Individuals 

 The solicitation on regulations.gov--"Let your voice be heard"--suggests that individuals are expressing themselves directly. Indeed, anyone can write a letter or type a comment in the text box on regulations.gov, and many people do. Individuals acting on their own submit content ranging from obscenities and memes to detailed personal accounts of how a policy would affect them and even poetry aimed at changing officials' hearts and minds. 


Administrative law scholarship often discusses individual participants in this light as well.
@Cuellar2005 finds that members of the "lay public" raise important new concerns beyond those raised by interest groups. He advocates for institutional reforms that would make it easier for individuals to participate and increase the sophistication of individual comments on proposed policies. 

The remaining forms of participation are all mediated through an organization's pressure campaign--what @Verba1987 would call a "cooperative activity." Comments from people mobilized as part of a campaign differ from those of individuals acting on their own in two observable ways: they often mention the name of the organization that mobilized them, and the text is often similar or identical to other comments in the campaign, reflecting coordination through form or template letters. These features eliminate the novel informational value that  @Cuellar2005 and others seek to locate in individual comments. If comments reference an organization that mobilized them, they likely have little to offer than that the more sophisticated organization has not already provided. If comments are identical, they certainly provide no new information.

#### Membership Organizations

If most commenters are members of membership organizations as @Kerwin2011 suggest, a large campaign of, say one million people, will require a large collection of membership organizations. Very few organizations have a million members, and those that do are unlikely to mobilize all of them, so mobilizing a large number of people through membership organizations will usually require a large coalition of membership organizations. We would expect commenters to identify themselves as members of these many organizations.

For example, if most comments are mobilized by membership groups, we would expect most comments to look like this comment from the NorthEast Hook Fisherman's Association:

> "We represent a small group of Commercial Fishermen with the Limited Access Handgear HA Permits, employing the use rod and reel, handlines or tub trawls to catch Cod, Haddock, and Pollock along with small quantities of other regulated and non-regulated marine fish. Historically and currently, our fishermen account for a small percentage of the groundfish landed in New England. However, the monetary gains obtained by the participants in this fishery are very important to us." ([NOAA-NMFS-2013-0050-0025](https://www.regulations.gov/comment/NOAA-NMFS-2013-0050-0025))

Similar groups, narrowly concerned about shrimp and halibut catch limits, also commented on this rule on behalf of their members. 

#### Policy Change Organizations: From Grassroots to Astroturf

Mobilizing people just for a particular policy fight requires a significant organizing capacity. @McNutt2007 calls these formations "policy change organizations." In contrast to membership organizations, they exist to organize public pressure toward a set of policy goals, not to serve a defined membership.

There is a spectrum of organizing the unorganized. The poles might be labeled "grassroots" and "astroturf." On the grassroots end, engagement is driven by a combination of passionate volunteerism and a supportive, attentive public [@Key1961] or issue public [@Converse1964]. In practice, the campaigns that most resemble the grassroots ideal are not pure volunteerism but are organized by policy change organizations like Credo Action and Organizing For American on the left and Americans for Prosperity on the right.
These organizations have large mailing lists and media operations. Membership organizations like the Sierra Club may often mobilize beyond their membership base and thus take the form of a policy change organization as well. 
Like people mobilized through their membership organizations, people mobilized by policy change organizations will often cite the mobilizing organization.  
Unlike those mobilized through membership organizations, mobilization by policy change organizations is more likely to be concentrated in a few large organizations with the specific resources for running campaigns that engage passionate or interested but unaffiliated segments of the public. 


Toward the astroturf end of the spectrum of policy change organizations, well-funded efforts gather signatures from a much less passionate and attentive population. Where grassroots organizing relies on existing underlying interests that merely need to be given an opportunity to engage, people engaged by astroturf campaigns are generally disinterested in the policy and engage merely because of paid ads or petition gathering, often involving some deception (e.g., intentionally misled about the policy or its likely effects) in order to get people to take action on an issue that they would not take if the issue was presented more clearly. Likewise, the organizations actually collecting the signatures would have no interest in doing so if they were not being paid. The aim is to give an appearance of support.
To the extent they mobilize real people, astroturf campaigns are thus a form of outside lobbying intended to create a deceptive appearance of public support. In the extreme, astroturf campaigns may use the names of fake or non-consenting individuals. 



In a more complex example, Access Financial and other payday lending companies sponsored two campaigns targeting a regulation proposed by Consumer Financial Protection Bureau in 2016. First, Access Financial had their employees solicit comments from customers, which Access then uploaded to regulations.gov. Many customer letters contained some version of "I have been told that payday loans would not exist in my community if the government's proposed regulations went into effect." The customer comments suggest that they had not been told much about the rule, which limited interest rates and the number of times short-term loans could be compounded. Most customers wrote some version of "Do not close this store"--a few even complained about exactly the issues that CFPB's regulation aimed to address. One customer wrote, "Although some of the fees are a bit high, it should be my choice whether to get a loan or not" (Access Financial Comment 91130). Another wrote, "I need to keep receiving my Check'n'Go loans so I can have the time to start paying them back in the next 1 1/2 to 2 years" (Access Financial Comment 91130), indicating that Check'n'Go (a subsidiary of Access Financial) was engaged in serial re-lending that put this customer deeper in debt. To the extent that this campaign relied on deception and not the customer's interests (even as the customers understood them), this would count as astroturf. Second, Access sponsored an effort to gather petition signatures at churches. Finally, Access and other payday loan companies uploaded supportive notes from community organizations to which they had given money. Like the churchgoers, these people had no reason to comment except that they had received money from the potentially regulated companies. 

Spotting astroturf in the wild is difficult by design. However, the clear observable result is often a large number of comments advocating on behalf of large business interests. Large businesses or industry associations are the organizations with the resources and incentives to sponsor astroturf campaigns, and they do [Lyon2005].

Not all public pressure campaigns on behalf of private interests fall on the astroturf side of the spectrum. In a cover letter to a batch of comments opposing the regulation of glyphosate herbicides, Monsanto, major glyphosate manufacture, specified how they collected the letters:

> "These letters were collected during the 2016 Farm Progress Show from US farmers, agriculture professionals, and general consumers who use glyphosate and value its benefits. We think it is important that these voices be heard as part of EPA's review of glyphosate." ([EPA-HQ-OPP-2009-0361-0891](https://www.regulations.gov/comment/EPA-HQ-OPP-2009-0361-0891))


Shell Oil sponsored a campaign to open the Arctic Outer Continental Shelf to oil and gas drilling and provided a template letter with a place to insert a company or constituency: 

> "On behalf of [enter company or constituents], I am writing to demonstrate my strong support of oil and gas development in the Arctic Outer Continental Shelf (OCS)...I support Shell's plan to explore its leases in the Chukchi Sea in 2015. The company has invested significant time and resources in the advancement of safe and prudent Arctic exploration. Shell should be allowed to realize the promise of the leases it purchased, and I encourage the BOEM to expeditiously approve its Exploration Plan."

However, the signatories of this petition were mostly companies and workers in the oil and gas sector. Several elected officials also used Shell's form letter (e.g. [BSEE-2013-0011-0033](https://www.regulations.gov/comment/BSEE-2013-0011-0033) and [BSEE-2013-0011-0094](https://www.regulations.gov/comment/BSEE-2013-0011-0094)). I found no evidence of deception or payments from Shell. These companies and workers presumably had a genuine interest in Shell's access to offshore oil.

A similar rule


<!--
Astroturf: Interest Group Lobbying and Corporate Strategy
Thomas P. Lyon  John W. Maxwell

- in which the firm covertly subsidizes a group with similar views to lobby when it normally would not;
Lloyd Bentsen, a long‐time senator from Texas, describes the artificial grassroots campaigns created by public relations (PR) firms (Stauber and Rampton, 1995, p. 79)
For example, Lupia and McCubbins (1994) and de Figueiredo et al. (1999) study how administrative procedures can be designed to optimize the flow of information to politicians, and Baron (2001) develops a model in which activists attempt to influence corporate strategy via the threat of consumer boycotts. Kollman (1998) studies the motivations and strategy behind lobbying behavior based on detailed interviews with 90 interest group leaders. Grossman and Helpman (2001) provide an excellent introduction to the recent theoretical literature on interest group politics.



Astroturf, Technology and the Future of
Community Mobilization:
Implications for Nonprofit Theory
JOHN McNutt
University of Delaware
School of Urban Affairs & Public Policy
KATHERINE BOLAND
Astroturf, quite simply, is synthetic grassroots organizing created for manipulative political purposes (see Lyon &
Maxwell, 2004; Allen, 1998; Austin, 2002). In this type of activity, an entity that wishes to affect public policy creates an effort
that gives the appearance of grassroots support
-->

<!--




> CITE STEVES ASTROTURF PAPER AND RELATED LIT

-->

### Who lobbies?

Metadata on the authors of comments and their
organizational affiliations are inconsistent and incomplete. As this
information is key to identifying influential actors, improving these
data was a significant data-organization task.

#### Mobilizing organizations

Through an iterative combination of automated search methods and hand-coding, I identify the organization responsible for over 40 million comments, including all organizations responsible for mobilizing 100 or more
comments with repeated text--either identical text or partially unique texts that contain shared language. I then searched comment texts for
mentions of these organizations' names to complete missing information
on the mobilizing organization. 

### Who Lobbies Together?

Having identified who is participating in rulemaking, the next step is
to determine who is lobbying together. Studies of rulemaking stress the importance of coalitions [@Yackee2006JOP; @Dwidar2019]. Scholars have measured coalitions of organized groups but have yet to attribute citizen comments to the coalition that mobilized them.

#### Identifying Coalitions Using Text Reuse 


I identify comments that are not identical but share a 10-word (or
"10-gram") string using a moving window function looping over each
possible pair of texts to identify matches.^[For more about this method and comparisons with related partial matching methods such as the Smith-Waterman algorithm, see @Casas2017 and @Judge-Lord2017.]
When actors sign onto the same comment, it is clear that they are
lobbying together. However, various businesses, advocacy groups, and
citizens often comment separately, even when they are aligned. Text-reuse (using the exact same ten-word phrases) captures this alignment. 

Figure \@ref(fig:percent-match) shows the percent of shared text for a sample of 50 comments on the Consumer Financial Protection Bureau's 2016 Rule regulating Payday Loans. Comments are arranged by the document identifier assigned by regulations.gov on both axes. 
The black on the diagonal indicates that each document has perfect overlap with itself. Black squares off the diagonal indicate additional pairs of identical documents. For example, 100% of the words from Comment 95976 are part of some tengram that also appears in 95977 because the exact same comment was uploaded twice. 
The block of grey squares indicates a coalition of commenters using some identical text.
Comments [91130](https://www.regulations.gov/document?D=CFPB-2016-0025-91130) through [91156](https://www.regulations.gov/document?D=CFPB-2016-0025-91154) are all partial or exact matches. Most are part of a mass comment campaign by Access Financial. The percent of the identical text is lower than many mass-comment campaigns because these are hand-written comments, but the n-gram method still picks up overlap in the OCRed text in the header and footer. 

```{r percent-match, fig.show = "hold", out.width = "60%", fig.cap="Percent of Matching Text in a Sample of Public Comments"}

knitr::include_graphics("Figs/comment_percent_match_plot-1.png")  
```



<!--  Identifying coalitions using clustering methods
I use statistical models of text to classify comments into coalitions. I cluster
documents by the frequency with which they use different words. Being
classified together does not mean that the documents all address exactly
the same distribution of substantive issues, just that they use similar
words relative to the full set of documents. I start by modeling all
comments on each rule (collapsing identical comments to one document)
with two and three clusters, which I then inspect to see how well the
comments of named organizations were classified. If the two cluster
model most sensibly describes the conflict, I label these clusters "pro" and "con" If the three-cluster model more sensibly describes the
conflict, I label these clusters as "pro, con, other." If neither fits
well, I increase the number of clusters as needed.

"`{r kmeans, fig.cap = "K-means clustering fails to capture coalitions when nearly all comments oppose a regulation"}

knitr::include_graphics(here::here("Figs", "kmeans.png"))
```


The asymmetry in expressed support for most rules presents challenges
for unsupervised clustering because much of the variation in comment
texts is within-coalition variation. For example, one of the most common
clustering methods, k-means clustering, often captures within-coalition
variation. Figure \@ref(fig:kmeans) shows k-means clusters based on a normalized
measure of word frequency (term-frequency/inverse-document-frequency)
compared to two principal components of variation. Neither k-means nor
principal components analysis is well suited to identifying the small
number of comments supporting the Park Service's proposed restrictions
on protests in Washington DC.

Two strategies may improve clustering. First, even partial text reuse
generally indicates that comments belong to the same coalition. For
example, as seen at the top of Figure
\@ref(fig:kmeans), models may be restricted to cluster the large number of comments beginning with
"As a citizen who has frequently participated" in the same coalition
even if they go on to add different personal anecdotes about why protest
rights are important to them. Thus, clustering methods could be
restricted to group partially copied texts, as well as entirely copied
texts. Second, a Bayesian mixture model may better recover pro and con
clusters, especially with strong priors comments using positive and
negative sentiment words belong together.
-->

### Measuring the Volume, Intensity, and Potential Contagion of Public Engagement

I measure variation in engagement in three ways, corresponding to the
three types of comments described above.

**Volume.** 
First, I measure the total number of comments on the rule.
Commenting results from multiple processes. First, a group must decide to
lobby. Then, a group must decide to mobilize public pressure. Finally, the resulting number of comments depends on the response to the
campaign. There may be many cases where groups may have had
success mobilizing but never reached the choice of whether to mobilize
or not. For example, relevant groups may have been unaware of the draft rule. Once the decision
to mobilize has been reached and made, the response to mobilizing is a
count process. Thus, I expect the count of comments across rules to
follow a zero-inflated negative binomial distribution.

**Effort.** 
I measure effort per comment by the number of words people
write, omitting any text longer than ten words not unique to that comment,
usually because a mobilizing organization provided it. For example, the Sierra Club mobilized more than 47,710
people to submit exactly the same text on the delay of the methane
pollution rule, but 7,452 people also took the time to write a
personalized comment in addition to the text provided (see Figure \@ref(fig:sierra)). 
Completely unique comments (those that do not copy text from a form letter at all) generally indicated high levels of effort. As @Verba1987 note, signing a form letter requires "some" effort, writing an original letter to a government official requires "a lot" of "initiative" (effort). The longer the letter, the more effort required. 

While effort, as measured by the number of words people write, maybe normally distributed, the low end of the observed distribution is truncated.
This is because we will not observe people who have low levels of passion for the issue; they either do not cross the effort threshold required to comment or opt
to write nothing more than the form letter. 

**Contagion.** Mass-comment campaigns have wildly different results.
Some organizations submit a clean 10,000 copies of (signatures on) the same comment.^[For example, Americans For Prosperity's campaign on the XXXX rule.]
Others "go viral"---inspiring a mess of further engagement where the
original messages are translated through social media posts and news
stories. To identify people who were plausibly mobilized indirectly by a
campaign, I count the number of people who use a similar distribution of
words to that of the form letter but fewer than ten words matching any
other comment. This is a regular count process.
