

% MEASUREMENT 3 % FIX THIS 
% main measure
The main dependent variable here is whether a coalition got their way. I measure this in three ways. First, on a sample of rules, I hand-coded lobbying success for each lobbying coalition, comparing the change between the draft and final rule to each comment's demands on a five-point scale from "mostly as requested" to "significantly different/opposite than requested." Second, I use text-reuse methods underlying plagarism detection algorithms to identify changes between draft and final rules and count the number of new 10-word phrases that appear in the comment and final rule, but not the draft rule. Finally, I model assess the similarity in word distributions between comments and changes made to the rule.
% Alt measures
% However, policy change is a high bar for influence. Thus, I also use other measures of agency responses to lobbying efforts. 
In addition to changing policy texts, agencies may speed up or delay finalizing a rule, extend the comment period, or delay date at which the rule goes into effect. Indeed, commentors often request speedy or delayed rule finalization, comment period extensions, or a delayed effective dates. Additionally, agencies write lengthy justifications of their decisions in response to some comments but not others. This measure of attention to commentor demands will be captured by the text similarity measure. % Thus, additional dependent variables include (1) whether the comment period is extended, (2) days until rule finalization, (3) days until rule effective date, and (4) length of the agency's response to a coalition's demands. 



% \paragraph{Measuring Policy Change}
%\subsection{What Policy Texts Do?}

I measure the extent to which the text of a policy becomes more similar or less similar to the text of each public comment. 

One way to think about this is that this change represents an increase in utility for those lobbying for the change. Purposeful actors got what they asked for and presumably reap the rewards. All dimensions of disagreement collapse to the latent dimension of utility. %Actors participate and form coalitions to the extent that the expected benefits exceed the costs of doing so. Each new layer of law affects politics by altering actors' utility functions. 
Taking a broader view of politics offers a less parsimonious, but more direct interpretation. Changes in law may deliver utility, but more precisely they reflect ideas. These may be ideas about ``who gets what, when, and how,'' but also about identities, aspirations, possibilities, whose opinions matter, and who constitutes the political community, which may be difficult to reduce to a single dimension. Focusing on costs and benefits alone risks overlooking much of the ideological and interpretive work lawmaking does in constructing political communities, possibilities, and norms. Public comments in rulemaking, like other forms of policymaking, may often be about more than self-interest. 

% Indeed ``who succeeds?'' is a constraining question laden with certain ideas of what politics really is or ought to be. A better question may be ``How do certain ideas end up in law and how has this shaped politics and law over time?'' This latter formulation recognizes that ideas may end up in law as accidental to other efforts and that politics is not only who gets what but, more fundamentally, a process of deciding who we are and what we want to do together. 

% In the case of rulemaking, each notice-and-comment process creates an implicit political community based on who participates and whose interests are claimed to be represented. Regulations generally prescribe concrete rights, prohibitions, and governmental actions, making clear what was decided to be done. They also establish norms, issue frames, scientific and legal standards, and goals that inspire and constrain politics in other policy processes, especially future rulemakings on the same issue.

% Importantly, few observers would see any final rule as the final word. Rules are frequently revisited at regular intervals, challenged in court, rewritten under future administrations, rebuked by a new Congress, and occasionally swept aside by new legislation. Most often, the questions in rulemaking will be revisited by the same agency and many of the same participants, but they are not the same; both have been transformed and engage the questions on terrain reshaped by added layers of law. This means that it is essential to consider the historically contingent evolution of policy and lobbying coalitions over time. Quantitative scholarship on rulemaking rarely considers the date rules are made, much less how the politics of rulemaking may be historically contingent. I aim to address this gap. 

% One way to think about the kind of political influence I aim to study is to ...

%Recognizing this, I try to...
% \paragraph{Measuring change in policy texts}

% If available and reliably reflecting what actors want, policy positions expressed as texts have great advantages over those expressed in votes.

% GOOD BRING BACK 
% Policy disagreements are disagreements about words that give governmental force to ideas. Information is an important currency for those trying to influence policy and that rhetoric and framing can affect perceptions of facts and policies. Policy learning can be seen as an updating of where one stands, an updating of beliefs about the what is true about the world and, most specifically, an updating about which words (and thus ideas) ought to be law. Importantly, policymakers are constantly learning about new problems, facts, and policy ideas about which they had no prior position. Thus new dimensions of disagreement are created every time claims are advanced about new problem definitions, new facts, and new ideas for what the law ought to say.

% GOOD BRING BACK 
% Estimating spatial ideal points is a leading way of estimating what actors want, especially when we only observe a series of votes. The quantity of interest is what kind of policy actors ideally want, and because voting only tells us whether one is for or against a policy text, we need multiple observations in which an actor (or others who plausibly share their position) falls on each side to begin to narrow down their ideological distance from any given policy. To get multiple observations, we must assume that a number of proposed policies can be placed at different points on the same underlying dimension of disagreement. When it is persuasively argued that a number of these underlying issue dimensions more or less collapse to an even more general dimension, we further increase our observations and thus the information we have about each actor regarding that more general dimension. 

% GOOD BRING BACK 
% When, rather than up or down votes, we have the text of what each actor wanted, we have much more information about where they stand in relation to a policy text. Indeed, instead of needing to uncover more general latent dimensions and estimate actors' positions on them, we directly observe the quantity of interest: the substance, direction, and magnitude of the disagreement in each case. We can cluster these disagreements to make more general statements about the broader nature of disagreements and relative policy potions, but, unlike with voting data, this reduction is not necessary to know the extent to which actors are getting what they want as we can directly observe how much outcome texts incorporate various actors' expressed ideal language. Flattening proposed changes in texts to \textit{n} dimensions where \textit{n} is fewer than the number of unique demands made by all actors may help descriptively, but is not required analytically. For example, we may reduce textual policy demands to left and right ideologies or preferences for more or less government, and doing so can be descriptively useful, but it is neither necessary nor helpful for answering the question of whose ideas end up in law.

Importantly, my project does not attempt to answer what actors want in general, \textit{a priori} of any policy proposal. Without some reference point (existing policy, for example), what actors want is difficult to define. I assess what commenters want \textit{given} a proposed policy. What commenters request may not be a sincere representation of their ideal policy, but it is plausibly what they really want given what they believe is possible. While this may be insufficient for estimating their ideal policy, it is sufficient for measuring who gets what they ask for. 

%If the primary aim is to identify policy actors' ideological proximity to each other, the analysis can be reduced to the similarity and difference in their ideal policy texts, summed across all areas or within broad policy areas. If alternatively, we ask whose ideas end up in policy, we want to know how similar the policy outcome was to the specific suggestions of each actor and the frequency of changes in their proposed direction. % regardless of whether they are advocating for ideologically consistent, orthogonal, or even opposite positions in different cases. In practice, the unique dimensions of disagreement in each policy process are rarely perfectly parallel or orthogonal. Mapping ideological distance requires reducing to some tractable number of dimensions. Measuring rates of getting one's way do not.
%This is one great advantage of textual data compared to votes. Each observation is far richer in content and analysis requires fewer assumptions to say where actors really stand on a proposed policy.

\paragraph{Limitations.} 

Observing policy influence, especially in the final stages of policymaking is difficult. Given the momentum of political agendas and the fact that much is determined before draft rules are made public, changes are often on the margins. But such marginal victories are also the aim of business and other interest groups that are not part of a mass mobilization campaign. 

Additionally, my theory suggests that influence is likely only in cases where mass mobilization is (1) aimed at influencing policy and (2) not accurately anticipated by policymakers. Identifying these conditions these will also be difficult.
Observational studies of policy decisions are almost always frustrated by the fact that decisionmakers rationally anticipate the actions of those who would influence them, rendering this influence difficult to observe. Thus I expect to observe larger effects in cases where mobilization or the level of engagement achieved was not anticipated by agency staff. However, as long as rulewriters do not perfectly anticipate mass engagement, it should have observable, if depressed, effects. Thus, without accounting for anticipation, any effects I uncover are likely underestimated, making the methods described above conservative tests.






%---indirect-strategic, direct-normative, indirect-normative---
% by which political information may influence agency decisions.

% Most rules address long-defined problems. They are next steps advancing a policy agenda \citep{West2013} or the first steps in a new, often reverse, policy direction, it is possible that effects of ``going public'' are cumulative in a policy area over time, starting out small, but gaining agenda-setting power with sustained public attention. This may not be possible to measure with the rule-focused research design outlined above. However, if sequential rules can be linked to distinct policy agendas, my strategy could be extended to model dynamics over time following \citet{Brookhart2015}.



% Measuring Policy Change}
% Policies may shape and be shaped by many forces, including the collective action of citizens, expert opinions, and businesses interests. Yet the drivers and consequences of policymaking are difficult to disentangle. Business groups may fund scientists or advocacy campaigns to preempt or undo costly regulations. Experts and policymakers may inspire broader civic mobilization, and citizen mobilization may, in turn, shape the priorities of experts and policymakers. Some policy debates divide along lines of citizen and corporate interest or expert and popular opinion, but many entail various clusters of claims regarding the public interest, expertise, and business interests: claims about the public good, scientific truths, and the proper role of government. Inferring policy demands from identity alone and assuming static coalitions may miss much of the story. 

Participants may ask for thee general kinds of things: (1) specific changes to identified parts of the text (2) broad shifts in emphasis, what \citet{Jones2005} call a policy image, and (3) a general direction of policy change. For example, on the same Clean Power Plan rule, some may ask the Environmental Protection Agency to make specific changes to two sentences having to do with the classification of power plants. % and the division of federal and state enforcement authority. 
Others may ask for broad re-framing to focus less on economic costs and more on environmental equity and the effects of pollution on children \citep{Rinfret2011}.
Nearly all commenters will at least say whether they think the policy goes too far, not far enough, or is about right. Many comments do all of the above. 

To capture specific and broad alignment between commenter requsts and rule changes, I leverage two key pieces of information from the rulemaking record: The text comments and the change from the draft rule to the final. %I use each type of information in a different kind of analysis, one targeting specific demands and one targeting broad demands. 
Both approaches require the same initial steps. To identify how exactly the rule changed from draft to final, I use text reuse methods to identify text that remained the same and text that was added or subtracted. 
%Similarly, I identify text in comments that is not copied from the draft rule.\footnote{I may also need to exclude other kinds of text such as narratives introducing the commenter which commonly precede policy demands.} 
% The result of these two steps is the changes requested by commenters and textual change in the rule. 





% Existing measures of who gets their way in rulemaking are blunt---hand-coding texts on a few pre-defined and simplified categories such as ``pro- or anti-regulation.'' This is well motivated by theory, but in practice, it is often unclear whether a policy, on its face, increases or decreases government regulation.%, is liberal or conservative, or maps on to any other such latent dimension. 
% Another drawback of the hand-coding approach is that one must often read each comment and compare it to the rule change to identify influential groups. %If groups get their way when they lobby in many venues over time or on obscure or uncontested rules, studies focusing on highly visible and contested policy processes may miss much. Conversely, if mass mobilization in high-profile and hotly-contested rules matters, studies of rulemaking that discard the hundreds of thousands of form letters and other evidence of occasional bursts of civic engagement are unable to assess if mass participation matters. Legal scholarship suggests that mass mobilization may be important (Coglianese 2001). This limited and potentially biased empirical focus is largely due to the cost of hand-coding methods.

 % Political scientists have only begun to leverage text-analysis tools to measure political relationships (see Grimmer 2013 on priorities, Kl\"uver and Mahoney 2015 on framing, Wilkerson et al. 2015 on tracing policy ideas). I expand analysis of rulemaking from thousands of documents to hundreds of thousands and from a few variables to many. %Specifically, text-analysis tools can do two things: (1) rapidly code large amounts of text for pre-established theoretically-informed variables and (2) identify new dimensions of variation (Grimmer and King 2011). A key advantage of text-analysis methods is that they can identify new dimensions of variation one had no prior reason to suspect, suggesting new hypotheses. For example, in addition to our prior notions of who ought to be influential, we may expect that the topics, priorities, arguments, or issue frames on which comments cluster may identify new dimensions distinguishing winning and losing coalitions in different contexts.  %The literature suggests three broad rival hypotheses: \textit{The change between the draft and final rule reflects the interests of ($H_1$) underrepresented groups, ($H_2$) regulated businesses, ($H_3$) no particular class of commenters.} 
% regs.gov
% regulation.gov - sometimes they are coming in after the time period

%I refine tools to measure influence in policymaking with two aims: (1) to estimate the relative influence of different actors or texts and (2) to identify new factors that predict influence. These methods will allow new tests to advance core debates about how and why influence occurs and may benefit a wide range of scholarship. Active debates over bureaucratic autonomy and capture (Carpenter 2001, 2010, Carpenter and Moss 2014) and the influence of Congress (Clinton et al. 2014, Farhang and Yaver 2015), the courts (Lauderdale and Clark 2014), the president (Carrigan and Krazdin 2015), and public opinion (Dunleavy 2013) will all benefit from using text-analysis to measure influence. 

%Generally speaking, my method of measuring influence in each rulemaking case has two steps, each targeting a conceptually distinct type of influence. First I identify the major dimensions of variation in comments and the direction in which the policy moved in relation to those dimensions, i.e. which side(s) won. Second, I identify particularly influential texts within the winning coalition(s). 

% More specifically, to address my three descriptive questions, I propose to uses an ensemble of five text-analysis methods to identify participants, track coalitions, and quantify the relationship between various input texts and outcome text. This ensemble combines (1) citations(finding texts that mention the same organization and individual names across comments) to identify participants (2) single member topic modeling to identify coalitions, (3) Smith-Waterman alignment (text reuse) identify text fragments copied from each individual texts, (4) mixed-member topic models to identify the distribution of topics in comments and changes to rule texts, and (5) sentiment analysis to identify positive and negative positions on each topic. These are described in more detail below.

%This ensemble method has several applications in this project. First, it allows me to identify clusters (possible coalitions) of actors commenting on draft regulations and estimate the relative alignment of each of these clusters (and individual documents within them) to the draft rule and final rule. Similarly, it allows me to identify clusters of actors submitting briefs to courts reviewing these rules and their alignment with the court opinion. 

%In the remainder of this section I describe what is measured by each component of the ensemble, and then discuss how these improved measures of who participate, who lobbies together, and whose ideas end up in policy through rulemaking can provide leverage to test mechanisms of policy feedback. 

\subsubsection{Measuring lobbying success as changes in rule texts.}

Having identified coalitions by the textual similarity in comments (having removed all sentences quoting the agency's draft rule and call for comments), I identify general and specific policy demands and whether a policy changes in the direction requested by each coalition. The result is several measures of lobbying success. I then model the relationship between my measures of coalition size (i.e. comment volume), intensity, and contagion with lobbying success.


I measure whether organizations lobbying in rulemaking got what they asked for in three ways. Each captures a different dimension of success. 

% hand coding 
\paragraph{Measure 1: General direction of change.}
First, each new rule moves policy in a general direction. For a subset of rules that received mass comment campaigns\footnote{Approximately 2000 of 14,000 draft rules for agencies participating in regulations.gov)} and a matched sample of other rules, I will hand-code each rule and each coalition on a simple three-point scale: did the comment say the rule goes too far, not far enough, or is about right and did the final rule (in general) go further, retrench, or stay the same as the draft. This is similar to the coding scheme used by \citep{Potter2017} and others who code comments as requesting that rules be ``published as is, strengthened, weakened, or withdrawn. This method of identifying whether a rule seems to move in the direction requested is also similar to leading  methods of assessing influence in rulemaking---\citet{Yackee2006JOP} measure whether commenters requested for more or less regulation---and superior to self-reported influence \citep{Furlong1997}.

% text reuse - specific 
\paragraph{Measure 2: Specific changes in policy text.} Second, when specific changes are made between a draft and final rule, I use text reuse methods (like plagiarism detection) to identify comments that suggested changes in language that match observed rule changes. %To identify the adoption of specific demands, I use the same text reuse methods to identify any matches between textual changes in the rule and the changes requested by commenters. 
If final rules add the specific phrases suggested in comments or revise phrases identified in comments, this is evidence that these commenters may have received some of the changes they requested.\footnote{The significance of this kind of relationship between texts could be measured by how many words were copied, weighted by the forcefulness of these words. For example, I could create a dictionary of legally-significant words such as ``shall,’’ ``must,’’ ``enforcement,’’ and ``standard.’’ and weight textual alignment scores accordingly.} Text reuse can be measured for individual commenters and averaged over coalitions.

\paragraph{Measure 3: General change in policy text.} Third, I assess the relative similarity in word use (i.e. frequency) between each coalitions' comments and changes in rule text and preambles. 
To identify the adoption of demands for broader shifts in policy image and emphasis, I propose a relational topic modeling approach. In contrast to the classifiers I use to classify commenters into coalitions, this approach assumes that each text is a mixture over a number of topics. Each word token in a document is assigned to exactly one topic. Words and thus documents have distributions over topics. The extent to which distribution of topics that changed from the draft to the final is similar to the distribution in comments may be seen as a measure of whether the commenter got what the kind of change in policy emphasis they asked for.\footnote{
% anprm footnote 
Some rulemaking processes also have a commenting period before the draft policy is published. In these cases, commenters respond to an Advanced Notice of Proposed Rulemaking (ANPRM). A similar approach can be used in these processes with the key difference that similarities between comments and the draft rule (now the outcome text), either in specific text fragments or general topic distribution, take on a different meaning. Instead of representing changes to a policy text, it may represent common understandings of what policy already was or had to be on this topic. Changes in the final rule more plausibly represent differences in what policy could be. With respect to the ANPRM and proposed rule, it is more difficult to infer that the same result would not have occurred without their comments. While such counterfactual inference is not my purpose and both measure the same core phenomena of the words actors want becoming policy, interpretation of what this means must attend to this difference.}


I combine topic modeling approaches with text reuse methods, allowing better measurement not just of what is discussed but the topic distributions of what is being added, cut, copied, or otherwise receiving special attention. 
%I call this a relational topic modeling approach. 
Of course, all topic models focus on the relationship between text, but by making some of the text units themselves a relationship between texts with text reuse methods, the topic model takes a ``difference in difference'' form (i.e. the relationship between comment texts and text added or deleted from the final rule versus text that remained the same).  % or ``difference in similarity'' (e.g. what was copied). 
Much of the rule content is retained from one version to the next, but some content changes. This method measures how these changes relate to the changes proposed by sophisticated comments. 

I focus on changes from draft to final rule by selecting only the text that was added or subtracted. This can be thought of as a versioning problem where the agency updates the rule. To focus on what changed, I excluded sentences that appear (approximately) verbatim in both the draft and final. %I use the Smith-Waterman alignment algorithm (developed for identifying DNA matches and commonly used in plagiarism software) to identify sections of text that are close matches. 
\citet{Wilkerson2015} successfully employ this approach to identify content copied from various bills in the legislative processes leading to the Affordable Care Act.

The result is a quantitative measure of the alignment between suggestions made in comments and text added or subtracted from the draft to final rule. %A similar approach can estimate the relationship between ANPRM comments and the draft rule text, omitting the draft-to-final text reuse step. Credible intervals for these comment and topic-specific alignment scores can be calculated from posterior distributions. 

A finding that the words or topics that one actor suggested were twice as likely to appear in the final policy compared to the draft is a powerful and intuitive description of where the power to shape policy resides.\footnote{This is, perhaps, even more powerful than saying that the policy tended to shift toward their ideal point on some latent dimension, where the exact content anchoring the ideal point and the dimension is at least slightly ambiguous.}

\subsubsection{Models of lobbying success in rulemaking}

The three measures of lobbying success (general direction, specific language, general language) allow three distinct tests for hypotheses \ref{hyp:ds}-\ref{hyp:is}:
Let $a_i$ be the demands of sophisticated comments of a coalition $i$, $b_i$ be the number of comments supporting a coalition $i$, $c_i$ be the number of comments from all members of Congress supporting coalition $i$, and $d_i$ be the number of comments from members of Congress with formal oversight powers supporting coalition $i$.

The general model for all three dependent variables is approximately\footnote{As these variables are correlated, a more sophisticated modeling strategy will be needed.} 

\begin{align}
Y_i \sim \beta_0  + 
\beta_1a_i + \beta_2a_i*b_i + \beta_3a_i*c_i + \beta_4a_i*d_i
\end{align}
%\textbf{Dependent variables}

Model 1 is an ordered logit, with one observation per coalition per rule. 
The Dependent variable is the absolute distance between general change in rule change direction and the direction requested by the coalition $\in$ \{less/withdrawn, about the same, more\}: 

$Y_i$ =
\textit{$-|$Direction - Direction requested by coalition $ i |$}.

Model 2 uses a negative binomial link function.
The dependent variable is the textual alignment between specific policy suggestions and specific policy changes: 

$Y_i$ = \textit{number of words requested to be changed that were changed.}

Model 3 uses beta regression. The dependent variable is the similarity in overall word use between the comments of each coalition and changes in the rule and/or rule preamble:

$Y_i$ = \textit{$-|$Proportion of topic in rule and/or preamble change - proportion of topic in coalition $i's$ comment(s) $|$}.


% \subsection{Measuring the Role of Agency Missions and Reputations in Decision making}

% \subsubsection{Measuring agency reputations}

% \subsubsection{Linking comments and reputations