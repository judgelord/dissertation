## Testing the Theory


### The Dependent Variable: Lobbying Success

The dependent variable is the extent to which a lobbying coalition got the policy outcome they sought, which I measure in several ways.

First, on a sample of rules, I hand-code lobbying success for each organization or elected official, comparing the change between the draft and final rule to each organization's demands on a five-point scale from "mostly as requested" to "significantly different/opposite than requested."
For each organization, I identify the main overall demand, the top three specific demands, and the corresponding parts of the draft and final rule texts.^[This does not capture rule changes on which an organization did not comment. 
The codebook is available [here](https://docs.google.com/document/d/1o1hi0z9O-G9xsgkspOFG2VWzh0wQKjiezzoVpItaCxU/edit?usp=sharing). See examples of coded cases [here](https://judgelord.github.io/dissertation/influence_coding_examples.pdf).]  
I then code overall lobbying success and lobbying success on each specific demand for each organization and coalition. Both the overall score and average score across specific demands both fall on the interval from -2 ("significantly different") to 2 ("mostly as requested"). 
A team of undergraduate research assistants then applied the codebook to all comments likely to be from organizations or elected officials on a random sample of rules. Several rules were double-coded by the whole team. Intercoder reliability was XX. I also double-coded all comments that were part of mass comment campaigns with more than XX comments.


In the models below, *coalition success* is the mean of hand-coded lobbying success on a five-point scale ({-2, -1, 0, 1, 2}, recoded to {-1, -.5, 0, .5, 1} for more straightforward model interpretation. 

The average hand-coded success per organizational comment is `r mean(d$success, na.rm = T) %>% round()` (N = `r nrow(d)`). The average success for organizational comments with a mass comment campaign is `r comments_coded %>% filter(comments > 99) %>% pull(success) %>% mean(na.rm = T) %>% round(3)` (N = `r nrow(comments_coded %>% filter(comments > 99))`).

Second, I use methods similar to automated plagiarism detection algorithms to identify changes between a draft and final rule.  Specifically, I count the number of words in phrases of at least ten words that appear in the comment and final rule, but not the draft rule. To do this, I first identify new or changed text in the final rule by removing all 10-word or longer phrases retained from the draft rule. I then search each comment for any 10-word or longer phrases shared with the new rule text and count the total number of shared words in these shared phrases. Finally, I normalize this count of "copied" words across shorter and longer comments by dividing it by the total number of words in the comment. This measure falls between 0 (zero percent of words from the comment added to the final rule) and 1 (100 percent of words from the comment added to the final rule). As a robustness check, I also use the non-normalized version of this variable, i.e., the raw number of "copied" words.

<!--
Third, I capture a broader dimension of lobbying success by modeling the similarity in word frequency distributions between comments and changes to the rule. New or changed text is identified as described above, except that I also include the rule's preamble and the agency's responses to comments. Agencies write lengthy justifications of their decisions in response to some comments but not others. By including preambles and responses to comments, this measure captures attention to a comment's demands and the extent to which the agency adopts a comment's discursive framing (i.e., the distribution of words it uses). I use cosign similarity to scale the word frequencies used by each comment relative to those in changes between the draft and final rule.^[For the subset of rules with five or more organizational comments, I create a more sophisticated measure of word frequency similarity by averaging the absolute value of differences in topic proportions $\theta$ between the comment and new rule text across 45 LDA models of all organizational comments estimated with 5 through 50 topics, normalized by the number of topics $k_n$ and the number of models such that $y_i$ falls between 0 (completely different estimated topic proportions) and 1 (the same topic proportions), $y_i = \sum_{5}^{n=50}(\frac{\sum|\theta_{rule\ change_i|k=n}-\theta_{comment_i|k=n}|}{n})*\frac{1}{45}$. For more on these methods of measuring textual similarity, see ["Measuring Change and Influence in Budget Texts"](https://judgelord.github.io/budgets/JudgeLordAPSA2017.pdf).] This measure falls between 0 (no common words) and 1 (the same word distribution).
-->

<!-- TODO -->
To assess the performance of these automated methods (text-reuse and word-frequency similarity), I calculate the correlation between these scores and my hand-coded 5-point scale for rules in the hand-coded sample where a final rule was published. 

### The Main Predictor Variable

The number of supportive comments generated by a public pressure campaign (the main variable of interest) is a tally of all comments mobilized by each organization or coalition that ran a mass-comment campaign on a proposed rule.  Because the marginal impact of additional comments likely diminishes, the number of comments is logged. This includes the main substantive comments submitted by an organization's staff or lawyers. If an organization mobilizes more than 1000 comments or 100 identical comments on a proposed rule, I code that organization, its coalition, and the proposed rule as having a mass comment campaign. Where a broader campaign does not support organizational comments, *log mass comments* takes a value of 0.

```{r coded-coalition-success, fig.cap= "Lobbying Success by Number of Supportive Comments"}
d %>% 
  group_by(coalition_comment, coalition_type, success) %>%
  mutate(agency = str_remove(docket_id, "-.*"),
         comments = sum(number_of_comments_received, na.rm = T)) %>% 
  filter(!is.na(coalition_type), coalition_type != "na") %>% 
  count(comments, agency) %>% 
  mutate(comments = comments + n) %>% 
  ungroup() %>%
  ggplot() +
  aes(x = success, y = log(comments + 1), color = coalition_type) +
  geom_jitter(aes(size = n), alpha = .5) +
  geom_smooth(se = FALSE) + 
  labs(size = "Coalition size\n(number of organizations)") #+ facet_wrap("agency", scales = "free_y")
```

<!-- not the same as Balla et al.-->
This approach is significantly different than that employed in previous studies of mass comment campaigns in at least two ways. First, my methods allow me to identify coalitions consisting of multiple organizations. Previous studies measure mass comment campaigns at the organization level. For example, @Balla2020 analyzes "1,049 mass comment campaigns that occurred during 22 EPA rulemakings"---an average of nearly 50 "campaigns" per rule. By "campaign," @Balla2020 mean an organization's campaign rather than a coalition's campaign. Especially on EPA rules, there are rarely more than two or three coalitions engaging in public pressure campaigns--one of the environmental advocacy groups and their allies, another of regulated industry groups and their allies. Using organizations as the unit of analysis means that observations are far from independent. An analysis that counts one coalition's campaign as 40 smaller "campaigns" with the same policy demands would count this one campaign as 40 observations. My methods allow me to measure levels of public pressure per organization *and* per coalition. 

The second major difference between my approach and previous research is that I do not compare policymakers' responses to sophisticated comments to policymakers' responses to mass comments. Rather, I *attribute* mass comments to organizations and coalitions that also submit sophisticated technical comments. The set of comparisons one makes is critical to any study of responsiveness or policy influence. Researchers may reach different conclusions if they compare different things. Consider a study comparing how agencies respond to Sierra Club form letters to how they respond to the Sierra Club's sophisticated comments. Now consider a study that compares responsiveness to the Sierra Club's sophisticated comments between rules where they did and did not run a mass comment campaign. A study comparing the average influence of form-letter comments to the average influence of sophisticated comments is very different from a study that compares the influence of two sets of sophisticated comments with different *levels* of public pressure behind them. By measuring comments per coalition, both through hand-coding and text reuse, I capture different levels of public pressure than we would see if we were to look only at comments per organization. 

### Explanatory variables

Other predictors of lobbying success in the models below are the length of the (lead) organization's comment, whether the coalition lobbies unopposed, the size of the lobbying coalition, and whether the coalition is business-led. 

*Comment length* is normalized by dividing the number of words in the comment by the number of words in the proposed rule, thus capturing the complexity of the comment relative to the complexity of the proposed rule. 

The number and type(s) of organization(s) is an attribute of each coalition (e.g., a *business-led* coalition with *N* organizational members). *Coalition size* (a count) is the number of organizations lobbying together on the rule, i.e., the number of distinct commenting organizations in each coalition. For organizations lobbying alone, coalition *coalition size* is 1. 

A coalition is *unopposed* when no opposing organizations comment. This is only for the hand-coded sample where we have coded the spatial position of each comment. 

I code a coalition as *business-led* if the majority of commenting organizations are for-profit businesses, or if upon investigation, I find it to be primarily led or sponsored by for-profit businesses.^[For more on how I identify types of organizations and coalitions, see \@ref(whymail-methods)]  *Business coalition* is binomial.

### Examples of hand-coded lobbying success

**A rule with a public pressure campaign: the 2015 Waters of the United States Rule:**
In response to litigation over the scope of the Clean Water Act, the Environmental Protection Agency and Army Corp of Engineers proposed a rule based on a legal theory articulated by Justice Kennedy, which was more expansive than Justice Scalia's. 
The Natural Resources Defense Council (NRDC) submitted a 69-page highly technical comment "on behalf of the Natural Resources Defense Council..., the Sierra Club, the Conservation Law Foundation, the League of Conservation Voters, Clean Water Action, and Environment America" supporting the proposed rule:

> "we strongly support EPA’s and the Corps’ efforts to clarify which waters are protected by the Clean Water Act. We urge the agencies to strengthen the proposal and move quickly to finalize it..." 

I coded this as support for the rule change, specifically not going far enough.  NRDC makes four substantive requests: one about retaining language in the proposed rule ("proposed protections for tributaries and adjacent waters...must be included in the final rule") and three proposed changes ("we describe three key aspects of the rule that must be strengthened").^[NRDC's three policy demands were: (1) "The Rule Should Categorically Protect Certain “Other Waters” including Vernal Pools, Pocosins, Sinkhole Wetlands, Rainwater Basin Wetlands, Sand Hills Wetlands, Playa Lakes, Interdunal Wetlands, Carolina and Delmarva bays, and Other Coastal Plain Depressional Wetlands, and Prairie Potholes. Furthermore, "Other 'Isolated' Waters Substantially Affect Interstate Commerce and Should be Categorically Protected Under the Agencies’ Commerce Clause Authority." (2) "The Rule Should Not Exempt Ditches Without a Scientific Basis" (3) "The Rule Should Limit the Current Exemption for Waste Treatment Systems
"] I also coded it as requesting speedy publication. These demands provide specific keywords and phrases for which to search in the draft and final rule text. By comparing the requested policy outcomes to the text of the final rule, I evaluate the extent to which NRDC got what it asked for.

A coalition of 15 environmental organizations mobilized over 944,000 comments. Over half (518,963) were mobilized by the four organizations mentioned in NRDC's letter: 2421,641 by Environment America, 108,076 by NRDC, 101,496 by clean water action, and 67,750 by the Sierra Club. Other coalition partners included EarthJustice (99,973 comments) and Organizing for Action (formerly president Obama's campaign organization, 69,369 comments). This is the upper tail end of the distribution. This coalition made sophisticated recommendations and mobilized a million people in support of NRDC's sophisticated lobbying.

The final rule moved in the direction requested by NRDC's coalition, but to a lesser extent than requested--what I code as "some desired changes." As NRDC et al. requested, the final rule retained the language protecting tributaries and adjacent waters and added some protections for "other waters" like prairie potholes and vernal pools.  EPA did not alter the exemptions for ditches and waste treatment systems. 

Comparing the draft and final with text reuse allows us to count the number of words that belong to 10-word phrases that appear in both the draft and final, those that appear only in the draft, and those that appear only in the final. For the 2015 Waters Of The U.S. rule, 15 thousand words were deleted, 37 thousand words were added, and 22 thousand words were kept the same. This means that more words "changed" than remained the same. Specifically, 69% of words appearing in the draft or final were either deleted or added.

For this coalition, the dependent variable, *coalitions success* is 1, *coalition size* is 15, *business coalition* is 0, *comment length* is 69/88, `r round(69/88,2)`, and *log mass comments* is log(943,931), `r round(log(943931), 2)`.

**2009 Fine Particle National Ambient Air Quality Standards:** In 2008, the EPA proposed a rule expanding air quality protections. Because measuring small particles of air pollution was once difficult, large particulates were allowed as a surrogate measure for fine particles under the EPA's 1977 PM10 Surrogate Policy. EPA proposed eliminating this policy, requiring regulated entities and state regulators to measure and enforce limits on much finer particles of air pollution. 

EPA received 163 comments on the rule, 129 from businesses, business associations such as the American Petroleum Institute and The Chamber of Commerce, and state regulators that opposed the rule. Most of these were short and cited their support for the 63-page comment from the PM Group, "an ad hoc group of industry trade associations" that opposed the regulation of fine particulate matter. Six state regulators, including Oregon's, only requested delayed implication of the rule until the next revised their State Implementation Plans (SIPs) for Prevention of Significant Deterioration (PSD). EarthJustice supported the rule but opposed the idea that the cost of measuring fine particles should be a consideration. On behalf of the Sierra Club, the Clean Air Task Force, EarthJustice commented: "We support EPA’s proposal to get rid of the policy but reject the line of questioning as to the benefits and costs associated with ending a policy that is illegal." The EarthJustice-led coalition also opposed delaying implementation: "EPA must immediately end any use of the Surrogate Policy – either by "grandfathered" sources or sources in states with SIP‐approved PSD programs – and may not consider whether some flexibility or transition is warranted by policy considerations."

The final rule did eliminate the Surrate Policy but allowed states to delay implementation and enforcement until the next scheduled revision of their Implementation Plans. I code this as the EarthJustice coalition getting most of what they requested, but not a complete loss for the regulated coalition.

For the PM Group coalition, the dependent variable, *coalitions success* is -1, *coalition size* is 129, *business coalition* is 1, *comment length* is 63/85, `r round(63/85, 2)`, and *log mass comments* is 0.

For the State of Oregon's coalition, the dependent variable, *coalitions success* is 2, *coalition size* is 6, *business coalition* is 0, *comment length* is 5/85, `r round(5/85, 2)`, and *log mass comments* is 0.

For the EarthJustice coalition, the dependent variable, *coalitions success* is 1, *coalition size* is 3, *business coalition* is 0, *comment length* is 7/85, `r round(7/85, 2)`, and *log mass comments* is 0.

> TODO: MORE EXAMPLES



### Summary Statistics for Hand-coded Data

These hand-coded data include `r nrow(comments_coded)` unique comments, some of which have identical copies for a total of `r sum(comments_coded$number_of_comments_received %>% as.numeric(),na.rm = T )` comments. These comments represent `r nrow(coalitions_coded)` distinct lobbying coalitions ranging in size from 2 to `r max(coalitions_coded$coalition_size)` organizations. Figure \@ref(fig:hist-coalitions) shows that coalitions are fairly balanced between those that succeed and fail to get the changes they seek in the final rule. `r percent( sum(coalitions_coded$coalition_business, na.rm = T)/nrow(coalitions_coded) )` are business-led coalitions. 

Table \@ref(tab:data-coded) shows a sample of coded data, summarized at the coalition level.

```{r data-coded, cache = FALSE}
d <- coalitions_coded %>% 
  drop_na(coalition_business) %>%
  filter(coalition_comment != "FALSE") %>% 
  distinct() %>% 
  dplyr::select(-coalitions)

d %>% group_by(docket_id) %>% 
  slice_max(comments, n = 2) %>%
  dplyr::select(docket_id, coalition_id, everything()) %>% 
  distinct() %>% 
  kable3(caption = "A Sample of Hand-Coded Data Summarized by Coaltion") 
```

```{r hist-coalitions, fig.width=2, fig.height=2,  out.width = "30%", fig.cap="Hand-coded Data by Coalition", cache=FALSE}
ggplot(d, aes(x = coalition_success)) + 
  geom_histogram() + 
  labs(x = "Coalition Success")

ggplot(d) + 
  aes(x = as.numeric(coalition_business)) + 
  geom_histogram() + 
  labs(x = "Business Coalition")

ggplot(d, aes(x = coalition_size)) + 
  geom_histogram() + 
  labs(x = "Coalition size")

```

```{r hist-comments,  fig.width=3, fig.height=2, out.width = "49%", fig.cap="Number of Comments Linked to Hand-Coded Coalitions", cache=FALSE}
#TODO
#ggplot(d, aes( x= comment_length)) + geom_histogram()+ labs(x = "% (Comment length/proposed rule length)*100")

ggplot(d, aes( x= log(comments))) + 
  geom_histogram() + 
  labs(x = "Log(comments)")

```

### Summary Statistics for Machine-coded Data

> IN PROGRESS

**Dependent variable:** *The percent change in policy text*...

**Explanatory variables:** The *total number of comments*...




### Limitations

The two main limitations of this design both bias estimates of public pressure campaign influence toward zero.

First, lobbying success may take forms other than changes in policy texts. Agencies may speed up or delay finalizing a rule, extend the comment period, or delay the date at which the rule goes into effect. Indeed, commenters often request speedy or delayed rule finalization, comment period extensions, or delayed effective dates. I capture these potential outcomes in my hand-coding but not in the two automated methods, which apply only to observations with a final rule text. Likewise, when there is no change between draft and final rule, both automated methods necessarily record lobbying success as 0, even if a comment asks an agency to publish a rule without change. 

Second, bureaucrats may anticipate public pressure campaigns when writing draft rules, muting the observed relationship between public pressure and rule change at the final rule stage of the policy process. This is a limitation of all studies of influence during rulemaking comment periods.

### Modeling the Direct Relationship Between Public Pressure and Lobbying Success

For all three measures of lobbying success, I assess the relationship between lobbying success and mass comments by modeling coalition $i$'s lobbying success, $y_i$ as a combination of the relative length of the (lead) organizations comment, whether the coalition is unopposed, the coalition's size, whether it is a business coalition, and the logged number of mass comments. I estimate these relationships using OLS regression. I also estimate hand-coded lobbying success with beta regression and ordered logit, which is more appropriate but less interpretable. For the automated measures of lobbying success, I estimate beta regression models with the same variables.] 

$$
y_i = \beta_0 + \beta_1 log(comments_i) + \beta_2 length_i + \beta_3 unopposed_i + \beta_4 size_i + \beta_5 business_i + \epsilon_i
$$


### Modeling Congressional Support as a Mediator of Lobbying Success

To estimate mediated effects, I estimate the average conditional marginal effect (ACME) and the proportion of the total effect attributed to mediation through congressional support (comments or other communication from Members of Congress supporting the coalition's position on the proposed rule). As developed by Imai et al. (2010), this involves first estimating a model of the potential mediator as a combination of covariates, $X$ (*length*, *unopposed*, *size*, and *business*) and then the outcome as a combination of the mediator, *congressional support*, and covariates, $X$.

The bold arrow in figure \@ref(fig:causal-oversight) indicates the key relationship
that I test in this step: the relationship between the scale of
public engagement and engagement from members of Congress, who may
receive political information (e.g., about the level of public attention or public opinion) from public pressure campaigns and the resulting mass commenting.

```{r causal-oversight, fig.cap = "The Mediator Model: The Relationship Between Public Pressure and Congressional Oversight"}
#TODO add headers, cut out demands, add political information box, shift principal comments over
#TODO add outcome model
knitr::include_graphics(here::here("figs", "causal-oversight-1.png"))
```

Mediator model \@ref(eq:mediator):

$$ congressional\ support_i = \beta_0 + \beta_1 log(comments_i) + \beta_{2-n} X_i + \epsilon_i (\#eq:mediator) $$

Outcome ($y_i = Lobbying\ success_i$) model \@ref(eq:outcome):

$$ y_i = \beta_0 + \beta_1 log(comments_i) + \beta_2 congressional\ support_i + \beta_{3-n} X_i + \epsilon_i (\#eq:outcome) $$
