<!--## Data and Methods {#oversight-methods} -->

### A Census of Comments from Members of Congress in Agency Rulemaking 

<!--
To measure the participation of elected officials and interest groups in rulemaking, I collected a corpus of over 80 million comments via the
regulations.gov API. About 50 million of these comments are on proposed
rules (over 16,000 proposed rules from 144 agencies from 2005 to 2020).
I then linked these comments to other data on the rules from the Unified
Agenda and Office of Information and Regulatory Affairs Reports on draft
rules sent to them for review.
Because metadata on the authors of comments and their
affiliations are inconsistent and incomplete, improving these
data was a significant data-organization task.


Through an iterative combination of automated search methods and hand-coding, I identify the comments submitted by elected officials.

Figure \@ref(fig:congress) shows the number of comments from members of Congress received during rulemaking by a sample of federal agencies. Oversight letters are frequently co-signed by multiple members from the Senate, House, or both chambers. 

```{r congress-hist, fig.cap = "Number of Letters from Members of Congress Received During Rulemaking per Year", out.width = "100%"}
knitr::include_graphics(here::here("figs", "comments-congress.png"))
```
-->

To assess the hypothesis that public pressure campaigns affect the engagement of
political principals, I examine the relationship between mass commenting
and the behavior of members of Congress, while attempting to control for
other reasons that members of Congress may comment on a proposed rule.
The bold arrow in figure \@ref(fig:causal-oversight) indicates the key relationship
that I test in this step. I aim to test the relationship between mass
public engagement and engagement from members of Congress, who may
receive information about public opinion from mass engagement.

```{r causal-oversight, fig.cap = "The Relationship Between Public Pressure and Congressional Oversight"}
knitr::include_graphics(here::here("figs", "causal-oversight-1.png"))
```



I measure the dependent variables---legislator attention and support---in several ways. First, I count the number of times members of Congress
engage the agency across rules and before, during, and after comment
periods on rules where lobbying organizations did and did not go public.
By engaging the agency, I mean that members of Congress raise a rule in
personal correspondence or comments that members send to the agency. I
then code each contact from the member of Congress and each coalition
lobbying on the rule on the same three-point scale: are they asking for
the rule to go further, be scaled back, or published as-is. This is
similar to other hand-coding approaches to policy demands. Next, I use
text analysis to compare the sentiment and rhetoric (phrases and word
frequencies) used by legislators to the language used by each coalition.



### Models testing the relationship between mass engagement and congressional oversight

There are several ways to test for a relationship between mass
engagement and engagement by Members of Congress. The key explanatory
variables of interest are the measures of mass engagement created in
step 1 (how many and what types of comments). For simplicity, in the
equations below, I only include measures related to the number of
comments. Thus $\beta_0$ is the estimate for a rule with no public
comments.

In Model 1, the dependent variable is the total number of comments from
Members of Congress on the rule: *$Y$ = Total comments from Congress*
$\sim$ zero-inflated negative binomial, with one observation per rule.
Let $x$ be the the total number of public comments.

$$\begin{aligned}
    Y \sim \beta_0 + \beta_1x\end{aligned}$$

In Model 2, the dependent variable is the number of comments from
Members of Congress on the rule that support the coalition or
organizations in the coalition: $Y_i$ = *Total comments from Congress
supporting coalition $i$* $\sim$ negative binomial, with one observation
per coalition per rule. Let $x_i$ be the total number of public comments
supporting coalition $i$.

$$\begin{aligned}
    Y_i \sim \beta_0 + \beta_1 x_i\end{aligned}$$

In Model 3, the dependent variable is the share of Congressional
comments supporting the coalition: $Y_i$ = *Share of Members of Congress
supporting coalition $i$* $\sim$ beta, with one observation per
coalition per rule. Let $z_i$ be the *share* of public comments
supporting coalition $i$.

$$\begin{aligned}
    Y_i \sim \beta_0 + \beta_1 z_i\end{aligned}$$
<!--
In Model 4, the dependent variable is rhetorical similarity between
comments of each coalition $i$ and each Members of Congress $j$:
$Y_{ij}$ = *Text similarity score between legislator comment and
coalition $i$ texts*, with one observation per legislator comment per
coalition. Let $z_i$ be the share of public comments supporting
coalition $i$.

$$\begin{aligned}
    Y_{ij} ~ \sim \beta_0 + \beta_1 z_i\end{aligned}$$

#### Limitations.

One challenge is controlling for rule salience, which may affect
both public and legislator attention (indeed, both are endogenous to
rule salience). Another challenge will be controlling for latent public
opinion, which may usually, but not exclusively, be revealed to
legislators through mass engagement.

-->
