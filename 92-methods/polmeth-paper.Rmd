---
title: "Iterative Hand-Coding and Computational Text Analysis: Application to Assessing the Effects of Public Pressure on Policy"
knit: ( function(input, ...) {
          rmarkdown::render(input)
          } 
      )
bibliography: '`r here::here("assets/dissertation.bib")`'
biblio-style: '`r here::here("assets/apsr.bst")`'
link-citations: yes
citecolor: black
urlcolor: black
frontpage: true
spacing: onehalfspacing
always_allow_html: true
keywords: hand-coding, text analysis
#wordcount: 11964
output:
    #bookdown::word_document2: default
    #pandoc("docs/polmeth-paper.tex", format = "docx")
    bookdown::pdf_document2:
      template: '../assets/article-template.tex' # redirect to assets 
      keep_tex: true
      latex_engine: xelatex
      citation_package: natbib
      toc: false
      fig_caption: true
abstract: "This paper describes methods for integrating human coding of texts with computational text analysis and preprocessing to increase the inferential power of hand-coding by several orders of magnitude. Computational text analysis tools can strategically select texts for human coders, including texts that represent larger samples and outlier texts of high inferential value. Preprocessing documents can speed hand-coding by extracting key features like named entities. Moreover, hand-coding and text analysis tools are each more powerful when combined in an interactive workflow. Applying this method to public comments on U.S. Federal Agency regulations, a hand-coded sample of 10,894 hand-coded comments yields equally valid inferences for over 41 million comments regarding the organizations that mobilized them and the extent to which policy changed in the direction they sought. This large sample enables new analyses of the relationships between lobbying coalitions, social movements, and policy change. For example, I find that public pressure to address climate and environmental justice movements has large effects on policy documents but that a small number of national advocacy organizations dominate lobbying coalitions. When tribal governments or local groups lobby without the support of national advocacy groups, policymakers typically ignore them."

---

```{r global.options, include=FALSE}
source(here::here("code", "setup.R"))
       
options(knitr.graphics.auto_pdf = TRUE)

#knitr::opts_chunk$set(fig.path= "../figs/")

knitr::opts_chunk$set(cache = FALSE)

book = FALSE
```





# Introduction 

## Data and Methods



To examine the relationship between public pressure campaigns and lobbying success, I collected a corpus of over 80 million public comments. 58 million of these comments are on rulemaking dockets. 
From 2005 to 2020, agencies posted 42,426 rulemaking dockets to regulations.gov for public comments. Only 816 of these rulemaking dockets were targeted by one or more public pressure campaigns, but this small share of rules garnered 99.07 percent (57,837,674) of all comments. Using text analysis tools to strategically select 10,894 comments for hand coding in an iterative process ultimately yields a new dataset of 41,342,776 as-good-as-hand-coded texts.


First, I develop computational methods to identify lobbying coalitions through text reuse.
I leverage an iterative process loop of text analysis and hand-coding to attribute comments to the organizations, campaigns, and broader coalitions that mobilized them. I first identify comments that share text. I find that a 10-word (10-gram) phrase repeated across more than a few comments is always either text copied from the proposed policy or language inspired or provided by an organized campaign. Thus, for each comment text, I first remove all 10-word phrases that appear in the proposed rule (including the preamble and call for comments). Then, I identify all comments that share ten-word phrases with 99 or more other comments. Text reuse thus allows me to code one sample document that is valid for all commenters using the same form letter. Finally, I collapse these form letter comments into one representative document for hand-coding.

Prior to hand-coding each comment, I attempt to identify the organization(s) that submitted or mobilized each comment by extracting the names of organizations and elected officials from the text of each comment. For comments that do not include the name of an organization or elected official, human coders used an internet search on portions of the comment's text. This often identified the organization that provided the model letter text. As additional organizations were identified by hand, they were added to the entity extraction method, and model text linked to them was added to the clustering method. This was especially for astroturf groups that provide model public comments while obscuring their identity. 
<!--I then identify lobbying coalitions both by hand and by textual similarity. Co-signed comments are always assigned to the same coalition. Likewise, form-letter comments are always assigned to the same coalition.-->
I thus attribute each comment to the organization behind it (its author if submitted by the organization or the mobilizing organization for form letters). I also identify comments submitted by elected officials, with special attention to members of the U.S. Congress.

To study the influence of organizations and coalitions, I collapse these data to one observation per organization or coalition per proposed rule for analysis. I then identify the main substantive comment submitted by each organization's staff or lawyers, which are usually much longer than supporting comments like form letters.

Comments selected for hand-coding are coded for their level of lobbying success. Hand-coding includes recording the type of organization, the lobbying coalition to which each comment belongs, the type of coalition (primarily public or private interests), their policy demands, and the extent to which the change between the draft and final rule aligned with their demands. The level of alignment between policy demands and policy outcomes is a standard approach for assessing lobbying success with these kinds of observational data. 


My approach to measuring lobbying success starts with policy demands raised in comments. The dimensions of conflict on which I judge lobbying success are those issues identified by commenters. Unlike other studies, the issues I use to assess lobbying success do not come from first reading the policy or of any a priori concept of what each policy fight is about. Instead, I read the change between the draft and final rule with an eye for alignment with commenters' requests (including requests that specific parts of the draft policy do not change.)
Using commenter requests to identify the dimensions of the conflict has advantages and disadvantages. Compared to other potential measures of success, it is more likely to focus on things that commenters care about and miss policy issues that other, non-commenting segments of the public might care about.

## Findings

These new data reveal who participates in public pressure campaigns and why. I find that pressure campaigns are almost always a conflict expansion tactic used by less-resourced "grassroots" groups; "astroturf" campaigns are surprisingly rare. However, the resources and capacities required to launch a campaign cause a few larger policy advocacy organizations to dominate. Over 80 percent of public comments were mobilized by just 100 organizations, most of which lobby in the same public interest coalitions. As a result, the public attention that pressure campaigns generate is concentrated on a small portion of policies on which these organizations focus. I also find no evidence of negativity bias in public comments. Instead, most commenters supported draft policies during the Obama administration but opposed those of the Trump administration, reflecting the partisan biases of mobilizing groups.

While most prior studies of public pressure campaigns targeting federal agencies focused on the Environmental Protection Agency, my sample of agency rules spans 60 agencies, allowing new insights into the scale and influence of public pressure across agencies. 
Comparing across agencies, I find that the federal agencies that are most often the targets of public pressure are also the most responsive to public pressure. 

My hand-coded measure of whether commenters got the policy outcome they sought allows me to assess whether public pressure campaigns increase lobbying success. Finally, I assess potential mechanisms by which mass public engagement may affect policy. I focus on congressional oversight as a mediator in policy influence, i.e., the extent to which public pressure campaigns affect policy indirectly through their effects on legislators' oversight behaviors. I find that members of Congress are more likely to engage in rulemaking when advocacy groups mobilize public pressure and that lobbying coalitions are more successful when they mobilize more legislators. However, pressure campaigns are related to policy outcomes under very narrow conditions.
Lobbying coalitions are more successful when they mobilize more members of Congress, but legislators disproportionately align with private interest (e.g., business-led) coalitions, not the public interest coalitions that run most public pressure campaigns.

These new data also allow me to assess the discursive effects of the climate movement and environmental justice movement. I find that agencies are more likely to add language addressing climate change or environmental justice in their final rules when public comments raise climate change or environmental justice concerns. However, the representation of both movements in federal policymaking is limited to a small number of national advocacy organizations. When less-well-resourced groups raise concerns about climate change or environmental justice, their concerns are almost always ignored. 

# Challenges and Opportunities in Studying Political Texts

Studying political texts found in the wild presents several methodological challenges. Some of these challenges are shared by other kinds of texts, such as open-ended responses to questions surveys and lab experiments, but this paper mainly focuses on texts generates by politics and observed by the research. 

Unlike experimental contexts where the outcome is the difference in words or phases used across treatment groups, differences in observational texts arise in systamtic ways from the political process we aim to study. Open-ended survey responses often reflect rhetoric from media or political campaigns. Texts generated by political campaigns are even more directly linked. For example, letter-writing campaigns targeting government officials almost always provide talking points or letter templates; the process generating political action simultaneously generates the text that government officials and political scientists observe. Likewise, social media posts and protest signs employ shared slogans and hashtages. Words represent ideas and issue frames by which campaigns aim to affect politics and policy. 

At the same time, because texts are generated in rich political contexts, much is often left unsaid. Slogns, hashtages, vague policy demands, and even petitions often have much more politically-relevant meaning in light of the policy fight for which they were generated. Moreover, individuals may not even be aware of the implicit demands of their words. Respondents may demand that goverment "act on climate," but whether these words mean regulating power plants, tax credits for electric cars, using the Defense Production Act to require firms to manufacture heat pumps, or thousandss of other policies depends on the polical context in which those words were deployed. 


Many solutions to the the challenges of non-independence, implicit meaning,  key observation that politics is organized. Generally, people need to be mobilized to engage in politics. 


# Hand-coding dynamic data

Before reviewing specific tools and methods, I must address the broader data mangmment system that makes itterative hand-coding and computational text analysis feasible in team setting. 
If working alone, a research may take an iterative approach without little additional infrastructure. One can generate document to hand-code, code it, re-run the automated tools, and generate a second document to code. The logistics of a team-setting, howeever, require that computational work and hand-coding happen more-or-less simultaneously. It is inefficient to distribute one document at a time to coders and make them wait until all coders are done with the first document to re-run the automated tools. 

Here, I focus on a worflow using Google Sheets and Drive, but a similar workflow could be achieved other, similar software. 
A key fearture of google sheets is that edits are visiable to all users simultaniously. There is no need send updated documents; all team members always have access to the most current version of the data. 
A second key feature of google sheets is that they can be accessed and edited with the `googlesheets4` R package. A single R script can pull in updates to the data, use hand-coded data to update algorithms (more on this below), generate a computationally improved version of the data, and immediately update the sheet the hand-coder sees with the improved data. 
Finally, users can include hyperlinks in sheet cells. 
Hyperlinks are easy to generated automatically and can helpfully point to needed documents. 

For the task of coding comments on draft agency rules, I generated spreadsheet 


## Preprocessing

# Ittereatve Processing

The computational takes discussed in this section are typically seen as "pre-processing" steps. In this section, I describe several pre-processing steps and then other steps that can often be done in more a iterative fashion. 

## Preprocessing: digitizing, cleaning, snd summarizing

In many cases, using text data requires that we first extract it from pdf

Where documents are longer than a paragraph, hand-coding can often be sped up by providing a computer-generated summary of the document. Relevant summary information depends on the aim of hand-coding. 

For coding public comments I provided two types of summary information, both focusing on actual sentences in the document. 

First I provided a three-sentence summary of the document using the `textrank` software to identify the three sentences that are best representative of the document as a whole.^[For more on textrank, code for applying it to comments, and comparison to hand-selected summaries, see: https://judgelord.github.io/rulemaking/textrank_comments]

Second, because I slected sentences that contained phrases of substatantive import. Specifically, I extracted all sentences that contained "climate change" or "environmental justice" and put them into spreadsheet columns. 


## Itterative entity tagging with regex tables


Many NLP tools exist for entity extraction. However, political entities go by many aliases, often frustrating the linking of records. Because various datasets of entities are not created with regular expression matching in mind, few provide aliases or variants of names. When datasets include substantial metadata, tools like `fastlink` may help link imperfect records [@fastlink]. through probabilistic matching. In many cases however, researchers must mach records on a single name variable with slightly different strings in each. Below, I outline two approaches to this task that can be done itteratively. 

### Consolidating entity name variants with regex tables

A regex table is one way to link multiple strings to a single entity. A regex table is a lookup table where one of the two columns is a regular expression. Regex tables are used by Google Tag Manager and other tools where a lookup table aims to capture a many-to-one relationship (e.g., many names for the same entity). Consider, for example, the Center for Responsive Politics tracks entities that donate to political campaigns or file lobbying disclosures. These data include the parent commpany or organization of each entity (for example, union locals are members of a national union and subsidary companies are owned by a parent company). A user could generate a regex table from these data by pasting all "child" organizations for each parent together with a regex "or" separator. Table \@ref(tab:) shows a regex table created from the Center for Responsive Politics lobbying data for the Teamsters Union (five aliases) and the company 3M (eight aliases). 

```{r crp, echo = T}
here("data", "Lobbbying_Summary.csv") |>
read_csv() |>
  group_by(parentName) |>
  summarize(pattern = str_c(orgName, collapse = "|") ) |>
  filter(parentName %in% c("Teamsters Union", "3M Co") ) |>
  kable(caption = "Regex Table Created from Center For Responsive Politics Data") 
```

The `legislators` R package uses regex tables to identify U.S. legislators in text using various common permutations of legislator names (e.g., "Elizabeth Warren, Senator Warren|Warren, Elizabeth). As users provided nicknames, this package expands the regex pattern (e.g. "Liz Warren"). Table \@ref(tab:legislators) shows a regex table for members of the 117th congress named Elizabeth. The `legislators` package contains a second regex table of common typos, for example those introduced by ORC (e.g., Senator Varren") that can be used to replace strings as a pre-processing step that increases the number of legislators successfully matched by the main regex table.  Table \@ref(tab:typos) shows a regex table for common typos or OCR-introduced errors for legislators named Elizabeth. Note that the misspelling "Elezabeth" is replaced by "Elizabeth" any time it appears in the provide text, potentially increasing matches for both members of Congress named Elizabeth.

```{r legislators}
library(legislators)

members |> 
  filter(first_name == "Elizabeth", congress == 117) |> 
  select(bioname, pattern) |> pull(pattern)
  kable(caption = 'Regex Table from the `legislators` R Packages, Legislators Named Elizabeth in the 117th Congress')
```


```{r typos}
typos |> 
  filter(str_detect(correct, "elizabeth")) |> 
  kable(caption = "Regex Table of Typos from the `legislators` R Packages, Legislators Named Elizabeth")
```


### Adding to a regex table

Hand-coders frequently generate information that can improve regex tables like the ones above. Indeed the many of the parent organizations are identified qualtiatively by Center for Responsive Politics Staff. The nicknames and typos included in the `legislators` packge were identified by dozens of research assitants hand-coding texts known to mention members of Congress. 



## Identifying lobbying coalitions with itterative n-gram clustering and hand-coding

While many methods of probablistic clustering 

N-gram windows have advantages and disadvantages over other text-reuse methods. Compared to Smith-Waterman, n-gram matching is very fast.^{<!--N-gram matching is fast because it requires relatively few steps to answer "is this exact n-gram from set A among n-grams in set B." In contrast, alighment-focused methods attempt to chart the best alignment of two texts, assessing alignment across the entire document, not just 10 words, for everye alternative possible alignment. -->
More importantly, n-gram matching is generally more inclusive of text that has been re-arranged.^[Smith-Waterman aims to identify the best alingment between two documents. If two sections of text have been flipped within a document, Smith-Waterman will only detect one of them. If each sentence or 10gram were considered a "document," it is possible that a Smith-Waterman alignment threshold may come up with better results than exact matching.]




^[More details and code implementing a n-gram moving window in R are available here: https://judgelord.github.io/rulemaking/payday_comments.html]


# Itterative hand and auto-coding 

## Coding policy positions 


