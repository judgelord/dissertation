[["index.html", "Public Pressure Campaigns and Bureaucratic Policymaking Introduction Motivation Outline of the book", " Public Pressure Campaigns and Bureaucratic Policymaking Devin Judge-Lord 2020-08-12 Introduction Does civic engagement through public pressure campaigns affect agency rulemaking? I examine who participates in public pressure campaigns and why, whether they affect congressional oversight, and whether they affect policy. Answering these questions informs our understanding of bureaucratic politics and interest group lobbying, organizing, and mobilizing tactics. If ordinary people have a voice in bureaucratic policymaking, I argue, it is through public pressure campaings. Thus, understnding the nature and effects of these campaigns is key to understanding modern democracy. With the rise of the administrative state, U.S. federal agencies have become a major site of policymaking and political conflict. By some estimates, upward of 90% of legally binding U.S. federal policy is now written by agencies. Agency rules are revised much more frequently than statutory law (Wagner et al. 2017), and in the years or decades between legislative enactments, federal agencies make legally-binding rules interpreting and reinterpreting old statutes to address emerging issues and priorities. Examples are striking: The effect of the Dodd-Frank Wall Street Reform and Consumer Protection Act was largely unknown until the specific regulations were written, and it continues to change as these rules are revised. Congress authorizes billions in farm subsidies, and leases for public lands, but who gets them depends on agency policy. In the decades since the last major environmental legislation, agencies have written thousands of pages of new environmental regulations and thousands more changing tack under each new administration. These revisions significantly shape lives and fortunes. For example, in 2006, citing the authority of statutes last amended in the 1950s, the Justice Department’s Bureau of Prisons proposed a rule restricting eligibility for parole. In 2016, the Bureau withdrew this rule and announced it would require fewer contracts with private prison companies, precipitating a 50% loss of industry stock value. Six months later, a new attorney general announced these policies would again be reversed, leading to a 130% increase in industry stock value. Agency rulemaking matters. Less clear, however, is how the new centrality of agency rulemaking fits with democracy. In addition to the bureaucracy’s complex relationships with the president and Congress, agencies have complex and poorly understood relationships with the public and advocacy groups. Relationships with constituent groups may even provide agencies with a degree of “autonomy” from their official principals (Carpenter 2001). Participatory processes like public comment periods, where government agencies must solicit public input on draft policies, are said to provide political oversight opportunities (Balla 1998; Mccubbins and Schwartz 1984), democratic legitimacy (Croley 2003; Rosenbloom 2003), and new technical information (Yackee 2006; Nelson and Yackee 2012). While recent scholarship on agency policymaking has shed light on the sophisticated lobbying by businesses and political insiders, we know surprisingly little about the vast majority of public comments which are submitted by ordinary people as part of public pressure campaigns.1 Activists frequently target agency policymaking with letter-writing campaigns, petitions, protests, and mobilizing people to attend hearings, all classic examples of “civic engagement” (Verba and Nie 1987). Yet civic engagement remains poorly understood in the context of bureaucratic policymaking. These occasional bursts of civic engagement in bureaucratic policymaking raise practical and theoretical questions for the practice of democracy.2 These questions, in turn, hinge on unanswered empirical questions: Do these campaigns affect policy? If so, by what mechanisms? Existing research finds that commenters believe their comments matter (Yackee 2015) and that the number of public comments varies across agencies and policy processes (Judge-Lord 2019; Libgober 2018; Moore 2017), but the relationship between the scale of public engagement and policy change remains untested. To address this gap, I assess the relationship between the number of public comments and the amount of change between draft and final policy texts. Next, I assess the relationship between the number of people mobilized by each campaign and whether the campaign achieved its policy goals. Finally, I theorize and test four mechanisms by which public input may affect bureaucratic policymaking. Each mechanism involves a distinct type of information that pressure campaigns may relay to policymakers: technical information, information about the likelihood of political consequences, information about the preferences of elected officials, or information about the preferences of the attentive public. Because scholarship on bureaucratic policymaking has focused on the power of technical information, where insider lobbying is most likely to matter and where outside strategies are least likely to matter, political scientists have largely overlooked mass mobilization as a tactic. I find evidence consistent with the observable implications of mass comment campaigns influencing policymaking through \\[non-null results\\] but no evidence that mass engagement affects rulemaking processes or outcomes through \\[null results\\]. Motivation Leading models of influence in bureaucratic policymaking focus on two key political forces: sophisticated interest group lobbying and political oversight. As bureaucrats learn about policy problems and balance interest-group demands, public comment processes allow lobbying organizations to provide useful technical information and inform decisionmakers of their preferences on draft policies. Agencies may then update policy positions within constraints imposed by their political principals. While this may describe most cases of bureaucratic policymaking, these models do not explain or account for the contentious politics that occasionally inspire millions of ordinary people to respond to calls for public input on draft agency policies. Mass engagement in bureaucratic policymaking has thus largely been ignored by political scientists, leaving a weak empirical base for normative and prescriptive work. Like other forms of mass political participation, such as protests and letter writing campaigns, mass public comments on draft agency rules provide no new technical information. Nor do they wield any formal authority to reward or sanction bureaucrats, as comments from a Members of Congress might. The number on each side, be it ten or ten million, has no legal import for an agency’s response. How, if at all, should scholars incorporate mass engagement into models of bureaucratic policymaking? Outline of the book This project aims to better understand the role of ordinary people in bureaucratic policymaking. I develop theories of why mass engagement occurs and how it may affect policy. To assess these theories, I tackle three related empirical questions: (1) Why does it occur?; (2) How does it affect the oversight behaviors of agencies’ political principals?; and (3) Does mass engagement in bureaucratic policymaking affect policy? Chapter 1 situates agency rulemaking in the context of American politics. I show that rulemaking is a major site of policymaking and political conflict. Chapter 2 explains why agencies (occasionally) get so much mail. The literature suggests two possible explanations for variation in mass engagement; groups may leverage public support as a lobbying resource (“grassroots” mobilization) or groups with more resources may leverage those resources into an impression of public support (sometimes called “astroturf”). I find that public interest campaigns explain variation in mass engagement. Unlike other forms of lobbying, it is not primarily driven by interests with the largest financial stakes and resources. Because the vast majority of comments are inspired by interest-group campaigns, finding their cause requires a method to link comments to the lobbying coalitions that mobilized them. To link individual comments to the more sophisticated lobbying efforts they support, I use text reuse and clustering methods to capture formal and informal coalitions. Chapter 3 asks whether mass engagement affect political oversight. The political information signaled by mass engagement may serve as “fire alarms,” altering principals to oversight opportunities or “warning signs” altering them to political risks. When a coalition mobilizes successfully, elected officials ought to be more likely to engage on their behalf and less likely to engage against them. To assess these hypotheses, I count the number of times Members of Congress engage the agency before, during, and after comment periods on rules where lobbying organizations did and did not go public. I then use text analysis to compare legislators’ sentiments and rhetoric to that used by each coalition. Chapter 4 asks whether mass engagement affects rulemaking and rules. I theorize that the effects of political information on policy depend on the extent to which the strategic environment allows change and how political information is processed, both directly within agencies and indirectly through other actors (e.g., Members of Congress) whose appraisals matter to bureaucrats. The main dependent variable is change in the rule text. I systematically identify changes between draft and final rules, parse these differences to identify meaningful policy changes, and compare them to demands raised in comments to measure which coalition got their desired outcomes. Chapter 5 presents a case study of the environmental justice movement. I identify all rules where “environmental justice” is raised in the comments to assess agency responses both quantitatively and qualitatively. In preliminary analysis, I find that responsiveness to environmental justice activist comments varies in predictable ways across agencies, but I find no evidence that the total number of comments affects rules. Chapter 6 concludes with remarks on the study of bureaucratic policymaking and policy recommendations to better account for the fact that public pressure campaigns and the bursts of civic engagement they mobilize will be an enduring feature of the policy process. References "],["agency-rulemaking-in-american-politics.html", "Chapter 1 Agency Rulemaking in American Politics", " Chapter 1 Agency Rulemaking in American Politics Abstract Large democracies face two big problems. First, they are vulnerable to fleeting passions and demagogues. To combat this, many decisions are left to experts who, ideally, exercise judgment loosely guided by the public. Second, everyone cannot vote on every decision. We thus delegate power to representatives (who then delegate it to deputies), create temporary mini-publics, and solicit input from those most affected or moved by a public decision.3 Most policy is then made by bureaucrats, supposedly guided indirectly through elected representatives and directly by limited public input (mostly limited to more contentious policy debates). Both of these problems converge in the bureaucracy, run by experts who are deputized by elected officials (or by their deputy’s deputy’s deputy) and with procedures that create opportunities for public input. It is far from clear how bureaucratic decisions are to balance expertise, accountability to elected officials, and responsiveness to public input in decisionmaking. As imagined by , mini-publics are representative, selected at random, and deliberative. Besides juries, however, randomly selected deliberative bodies are rare. Instead, citizens more often engage in government decisions when given opportunities to opt-in, such as hearings, petitions, and public comment periods. These mechanisms of engagement generate a different, more contentious flavor of public input than the discourse imagined by scholars who focus on deliberation.↩︎ "],["public-pressure-why-do-agencies-sometimes-get-so-much-mail.html", "Chapter 2 Public Pressure: Why Do Agencies (sometimes) Get So Much Mail?", " Chapter 2 Public Pressure: Why Do Agencies (sometimes) Get So Much Mail? Abstract I examine who participates in public pressure campaigns and why. Scholars of bureaucratic policymaking have focused on the sophisticated lobbying efforts of powerful interest groups. Yet agencies occasionally receive thousands or even millions of comments from ordinary people. How, if at all, should scholars incorporate mass participation into models of bureaucratic policymaking? Are public pressure campaigns, like other lobbying tactics, primarily used by well-resourced groups to create an impression of public support? Or are they better understood as conflict expansion tactics used by less-resourced groups? To answer these questions, I collect and analyze millions of public comments on draft agency rules. Using text analysis methods underlying plagiarism detection, I match individual public comments to pressure-group campaigns. I find that most public comments are mobilized by a few public interest organizations. Over 80% of the 48 million comments on proposed rules posted to regulations.gov were mobilized by just 100 organizations, 87 of which lobby in coalitions with each other. Contrary to other forms of lobbying, I find that mass comment campaigns are almost always a conflict expansion tactic, rather than well-resourced groups creating an impression of public support. Contrary to other forms of political participation, I find no evidence of negativity bias in public comments. Indeed, from 2005 to 2017, most comments supported proposed rules. This is because public comments tend to support Democratic policies and oppose Republican policies, reflecting the asymmetry in mobilizing groups. See the working paper version of this chapter here. "],["oversight-do-public-pressure-campaigns-influence-congressional-oversight.html", "Chapter 3 Oversight: Do Public Pressure Campaigns Influence Congressional Oversight?", " Chapter 3 Oversight: Do Public Pressure Campaigns Influence Congressional Oversight? Abstract This chapter examines the effect of public pressure campaigns on congressional oversight. I assess whether legislators are more likely to engage in rulemaking when advocacy groups mobilize public pressure. This involves collecting and coding thousands of comments from Members of Congress on proposed rules with and without public pressure campaigns. These data also allow me to assess congressional oversight as a mediator in policy influence, i.e., the extent to which public pressure campaigns affect agency decisionmakers directly or indirectly through their effects on elected officials’ oversight behaviors. "],["policy-influence-do-public-pressure-campaigns-influence-bureaucratic-policymaking.html", "Chapter 4 Policy Influence: Do Public Pressure Campaigns Influence Bureaucratic Policymaking? 4.1 Data 4.2 Methods 4.3 Modeling the direct relationship 4.4 Examples of hand-coded lobbying success", " Chapter 4 Policy Influence: Do Public Pressure Campaigns Influence Bureaucratic Policymaking? Abstract I assess whether public pressure campaigns increase lobbying success in agency rulemaking using a mix of hand-coding and computational text analysis methods. To measure lobbying success, I develop computational methods to identify lobbying coalitions and estimate lobbying success for all rules posted for comment on regulations.gov. These methods are validated against a random sample of 100 rules with a mass-comment campaign and 100 rules without a mass comment campaign that I hand-code for whether each coalition got the policy outcome they sought. I then assess potential mechanisms by which mass public engagement may affect policy. Each mechanism involves a distinct type of information revealed to decisionmakers. Of primary interest is the extent to which public pressure campaigns affect agency decisionmakers directly or indirectly through their effects on elected officials’ oversight behaviors. I test this by assessing congressional oversight as a causal mediator using a subset of rules where I collect and code correspondence from Member of Congress to agencies about proposed agency rules. 4.1 Data I create an original dataset that combines several sources of data on U.S. federal agency rulemaking. The core data are the texts of draft and final rules and public comments on these proposed rules. This includes all 16 thousand proposed rules from 144 agencies (as defined by regulations.gov) that were open for comment on regulations.gov between 2005 and 2018, that received at least one comment from an organization, and that saw a final agency action between 2005 and 2019. There are over 50 million comments on this set of rules. I scrape draft and final rule texts from federalregister.gov and comments submitted as attachments or by mail from regulations.gov. I retrieve comments submitted directly on regulations.gov and metadata on rules and comments (such as the dates that the proposed rule was open for comment and whether the agency identified the organization submitting the comment) from the regulations.gov API. I add additional metadata on rules (such as whether the rule was considered “significant”) from the Unified Agenda published by the Office of Information and Regulatory Affairs (reginfo.gov). Finally, to better capture positions expressed by Members of Congress on proposed rules, I supplement congressional comments posted on regulations.gov with Freedom of Information Act Requests for all communication from Members of Congress to each agency on proposed rules from 2007 to 2019.4 The combined dataset has over 50 million observations of one public or legislator comment on a proposed rule. I attempt to identify the organization(s) that submitted or mobilized each comment by extracting all organization names from the comment text. For comments that do not reference an organization, I am often able to identify organizations with an internet search using the comment’s text. I then identify lobbying coalitions by clustering comments that use similar phrases or word frequencies. Co-signed comments are always assigned to the same coalition. Likewise, form-letter comments are always assigned to the same coalition.5 Because my hypotheses are about the influence of organizations and coalitions, for analysis, I collapse these data to one observation per organization or coalition per proposed rule and identify the main substantive comment submitted by each organization’s staff or lawyers, which are usually much longer than supporting comments like form letters. For hand-coding, I first select a random sample of 100 proposed rules with a mass-comment campaign and then selecting a matched sample of 100 proposed rules without a mass comment campaign. Matching prioritizes, presidential administration, policy area (following Policy Agendas Project coding), rule significance, department, agency, subagency, and proposed rule length, respectively.6 This hand-coded sample is several times larger than leading studies using hand-coding and includes rules with very large and small numbers of comments that previous studies exclude. The full sample is four hundred times larger.7 4.2 Methods The most direct way to assess the hypothesis that mass engagement increases lobbying success is to assess the magnitude of the relationship between the number of comments that a coalition mobilizes and its lobbying success. However, public pressure campaigns may only be effective under certain conditions. Thus, I first assess the main relationship and then assess evidence for or against different potential causal pathways of influence. 4.2.1 The Dependent Variable: Lobbying Success The dependent variable is the extent to which a lobbying coalition got the policy outcome they sought, which I measure in three ways. First, on a sample of rules, I hand-code lobbying success for each lobbying coalition, comparing the change between the draft and final rule to each organization’s demands on a five-point scale from “mostly as requested” to “significantly different/opposite than requested.” To do this, I first identify organizational comments. For each organization, I identify the main overall demand and the top three specific demands and the corresponding parts of the draft and final rule texts.8 I then code overall lobbying success and lobbying success on each specific demand for each organization and coalition. Both the overall score and average score across specific demands both fall on the interval from -1 (“significantly different”) to 1 (“mostly as requested”). Second, I use methods similar to automated plagiarism detection algorithms to identify changes between a draft and final rule that were suggested in a comment. Specifically, I count the number of words in phrases of at least ten words that appear in the comment and final rule, but not the draft rule. To do this, I first identify new or changed text in the final rule by removing all 10-word or longer phrases retained from the draft rule. I then search each comment for any 10-word or longer phrases shared with the new rule text and count the total number of shared words in these shared phrases. Finally, I normalize this count of “copied” words across shorter and longer comments by dividing it by the total number of words in the comment. This measure falls between 0 (zero percent of words from the comment added to the final rule) and 1 (100 percent of words from the comment added to the final rule). As a robustness check, I also use the non-normalized version of this variable, i.e. the raw number of “copied” words. Third, I capture a broader dimension of lobbying success by modeling the similarity in word frequency distributions between comments and changes to the rule. New or changed text is identified as described above, except that I also include the rule’s preamble and the agency’s responses to comments. Agencies write lengthy justifications of their decisions in response to some comments but not others. By including preambles and responses to comments, this measure captures attention to a comment’s demands and the extent to which the agency adopts a comment’s discursive framing (i.e. the distribution of words it uses). I us cosign similarity to scale the word frequencies used by each comment relative to those in changes between draft and final rule.9 This measure falls between 0 (no common words) and 1 (exactly the same distribution of words). To assess the performance of these automated methods (text-reuse and word-frequency similarity), I calculate the correlation between these scores and my hand-coded 5-point scale for rules in the hand-coded sample where a final rule was published. As the automated methods apply at the organization-level, coalition scores are those from the lead organization–by default, the organization(s) with the longest comment. At the coalition level, the correlation between hand-coded influence is __ with the text-reuse method and __ with the word-frequency method. 4.2.2 The Main Predictor Variable The number of supportive comments generated by a public pressure campaign (the main variable of interest) is a tally of all comments mobilized by each organization or coalition that ran a mass-comment campaign on a proposed rule. Because the marginal impact of additional comments likely diminishes, the number of comments is logged. This does not include the main substantive comments submitted by an organization’s staff or lawyers. Nor does it include comments that are not affiliated with the organization or coalition. If an organization mobilizes more than 1000 comments or 100 identical comments on a proposed rule, I code that organization, its coalition, and the proposed rule as having a mass comment campaign. Where organizational comments are not supported by a mass comment campaign log mass comments takes a value of 0. 4.2.3 Explanatory variables Other predictors of lobbying success in the models below are the length of the (lead) organization’s comment, whether the coalition lobbies unopposed, the size of the lobbying coalition, and whether the coalition is business-led. Comment length is normalized by dividing the number of words in the comment by the number of words in the proposed rule, thus capturing the complexity of the comment relative to the complexity of the proposed rule. The number and type(s) of organization(s) is an attribute of each coalition (e.g. a business-led coalition with N organizational members). Coalition size is the number of distinct commenting organizations in the coalition (including those that co-sign a comment). For organizations lobbying alone, coalition size is 1. A coalition is unopposed when no opposing organizations comments. I code a coalition as business-led if the majority of commenting organizations are for-profit businesses, or if upon investigation, I find it to be primarily led or sponsored by for-profit businesses.10 4.2.4 Limitations The two main limitations of this design both bias estimates of public pressure campaign influence toward zero. First, lobbying success may take forms other than changes in policy texts. Agencies may speed up or delay finalizing a rule, extend the comment period, or delay the date at which the rule goes into effect. Indeed, commentors often request speedy or delayed rule finalization, comment period extensions, or delayed effective dates. I capture these potential outcomes in my hand-coding but not in the two automated methods, which apply only to observations with a final rule text. Likewise, there there is no change between draft and final rule, both automated methods necessarily record lobbying success as 0, even if a comment asks an agency to publish a rule without change.11 Second, bureaucrats may anticipate public pressure campaigns when writing draft rules, muting the observed relationship between public pressure and rule change at the final rule stage of the policy process. 4.3 Modeling the direct relationship For all three measures of lobbying success, I assess the relationship between lobbying success and mass comments by modeling coalition \\(i\\)’s lobbying success, \\(y_i\\) as a combination of the relative length of the (lead) organizations comment, whether the coalition is unopposed, the coalition’s size, whether it is a business coalition, and the logged number of mass comments. I estimate OLS12 regression: \\[ y_i = \\beta_0 + \\beta_1 log(comments_i) + \\beta_2 length_i + \\beta_3 unopposed_i + \\beta_4 size_i + \\beta_5 business_i + \\epsilon_i \\] 4.3.1 Modeling mediated relationships To estimate mediated effects, I estimate the average conditional marginal effect (ACME) and the proportion of the total effect attributed to mediation through congressional support (comments or other communication from Members of Congress supporting the coalition’s position on the proposed rule). As developed by Imai et al. (2010), this involves first estimating a model of the proposed mediator as a combination of covariates, \\(X\\) (length, unopposed, size, and business) and then the outcome as a combination of the mediator, congressional support, and covariates, \\(X\\). Mediator model: \\[ congressional\\ support_i = \\beta_0 + \\beta_1 log(comments_i) + \\beta_{2-n} X_i + \\epsilon_i \\] Outcome model: \\[ y_i = \\beta_0 + \\beta_1 log(comments_i) + \\beta_2 congressional\\ support_i + \\beta_{3-n} X_i + \\epsilon_i \\] 4.4 Examples of hand-coded lobbying success 2015 Waters of the United States Rule: In response to litigation over which waters were protected by the Clean Water Act, the Environmental Protection Agency and Army Corp of Engineers proposed a rule based on a legal theory articulated by Justice Kennedy, which was more expansive than Justice Scalia’s. The Natural Resources Defense Council submitted a 69-page highly technical comment “on behalf of the Natural Resources Defense Council…, the Sierra Club, the Conservation Law Foundation, the League of Conservation Voters, Clean Water Action, and Environment America” supporting the proposed rule: “we strongly support EPA’s and the Corps’ efforts to clarify which waters are protected by the Clean Water Act. We urge the agencies to strengthen the proposal and move quickly to finalize it…” I coded this as support for the rule change, specifically not going far enough. I also coded it as requesting speedy publication. NRDC makes four substantive requests: one about retaining language in the proposed rule (“proposed protections for tributaries and adjacent waters…must be included in the final rule”) and three proposed changes (“we describe three key aspects of the rule that must be strengthened”).13 These demands provide specific keywords and phrases for which to search in the draft and final rule text. A coalition of 15 environmental organizations mobilized over 944,000 comments, over half (518,963) were mobilized by the four above organizations: 2421,641 by Environment America, 108,076 by NRDC, 101,496 by clean water action, and 67,750 by the Sierra Club. Other coalition partners included EarthJustice (99,973 comments) and Organizing for Action (formerly president Obama’s campaign organization, 69,369 comments). This is the upper tail end of the distribution. This coalition made sophisticated recommendations and mobilized a million people. The final rule moved in the direction requested by NRDC’s coalition, but to a lesser extent than requested–what I code as “some desired changs.”\" As NRDC et al. requested, the final rule retained the language protecting tributaries and adjacent waters and added some protections for “other waters” like prairie potholes and vernal pools, but EPA did not alter the exemptions for ditches and waste treatment systems. Comparing the draft and final with text reuse allows us to count the number words that belong to 10-word phrases that appear in both the draft and final, those that appear only in the draft, and those that appear only in the final. For the 2015 Waters Of The U.S. rule, 15 thousand words were deleted, 37 thousand words were added, and 22 thousand words were kept the same. This means that more words “changed” than remained the same, specifically 69% of words appearing in the draft or final were part were either deleted or added. For this coalition, the dependent variable, coalitions success is 1, coalition size is 15, business coalition is 0, comment length is 69/88, 0.78, and log mass comments is log(943,931), 13.76. 2009 Fine Particle National Ambient Air Quality Standards: In 2008, the EPA proposed a rule expanding air quality protections. Because measuring small particles of air pollution was once difficult, measurements of large particulates were allowed as a surrogate measure for fine particles under EPA’s 1977 PM10 Surrogate Policy. EPA proposed eliminating this policy, thus requiring regulated entities and state regulators to measure and enforce limits on much finer particles of air pollution. EPA received 163 comments on the rule, 129 from businesses, business associations such as the American Petroleum Institute and The Chamber of Commerce, and state regulators that opposed the rule. Most of these were short and cited their support for the 63-page comment from the PM Group, “an ad hoc group of industry trade associations” that opposed the regulation of fine particulate matter. Six state regulators, including Oregon’s, only requested delayed implication of the rule until they next revised their State Implementation Plans (SIPs) for Prevention of Significant Deterioration (PSD). EarthJustice supported the rule but opposed the idea that the cost of measuring fine particles should be a consideration. On behalf of the Sierra Club, the Clean Air Task Force, EarthJustice commented: “We support EPA’s proposal to get rid of the policy but reject the line of questioning as to the benefits and costs associated with ending a policy that is illegal.” The EarthJustice-led coalition also opposed delaying implementation: “EPA must immediately end any use of the Surrogate Policy – either by”grandfathered\" sources or sources in states with SIP‐approved PSD programs – and may not consider whether some flexibility or transition is warranted by policy considerations.\" The final rule did eliminate the Surrate Policy but allowed states to delay implementation and enforcement until the next scheduled revision of their Implementation Plans. I code this as the EarthJustice coalition getting most of what they requested, but not a complete loss for the regulated coalition. For the PM Group coalition, the dependent variable, coalitions success is -1, coalition size is 129, business coalition is 1, comment length is 63/85, 0.74, and log mass comments is 0. For the State of Oregon’s coalition, the dependent variable, coalitions success is 2, coalition size is 6, business coalition is 0, comment length is 5/85, 0.06, and log mass comments is 0. For the EarthJustice coalition, the dependent variable, coalitions success is 1, coalition size is 3, business coalition is 0, comment length is 7/85, 0.08, and log mass comments is 0. Many agencies provided records of their congressional correspondence going back to 2005 or earlier.↩︎ The same comment text is attributed to each signatory of a comment. For more on how I identify organizations and coalitions, see Chapter 2, “Why Do Agencies (Sometimes) Get So Much Mail?”.↩︎ For more on policy area coding, see “Trends in U.S. Executive-branch Policymaking 1980-2016” in Chapter 1.↩︎ Except for comment texts and attachments, these data are available separately and combined on github.com/judgelord/rulemaking.↩︎ This does not capture rule changes on which an organization did not comment. The codebook is available here. See examples of coded cases here.↩︎ For the subset of rules with five or more organizational comments, I create a more sophisticated measure of word frequency similarity by averaging the absolute value of differences in topic proportions \\(\\theta\\) between the comment and new rule text across 45 LDA models of all organizational comments estimated with 5 through 50 topics, normalized by the number of topics \\(k_n\\) and the number of models such that \\(y_i\\) falls between 0 (completely different estimated topic proportions) and 1 (exactly the same topic proportions), \\(y_i = \\sum_{5}^{n=50}(\\frac{\\sum|\\theta_{rule\\ change_i|k=n}-\\theta_{comment_i|k=n}|}{n})*\\frac{1}{45}\\). For more on these methods of measuring textual similarity, see “Measuring Change and Influence in Budget Texts”.↩︎ For more on how I identify types of organizations and coalitions, see Chapter 2, “Why Do Agencies (Sometimes) Get So Much Mail?”.↩︎ Time allowing, I will hand-code a larger sample of proposed rules where no final rule was published so that these cases can be included in the analysis. Likewise, I will identify cases where organizations requested rules to be published as-is and recode these cases by hand.↩︎ See OLS model estimates with simulated data. I also estimate hand-coded lobbying success with beta regression and ordered logit, which are more appropriate but less interpretable. For the automated measures of lobbying success, I estimate beta regression models with the same variables.↩︎ These three aspects are: (1) “The Rule Should Categorically Protect Certain “Other Waters” including Vernal Pools, Pocosins, Sinkhole Wetlands, Rainwater Basin Wetlands, Sand Hills Wetlands, Playa Lakes, Interdunal Wetlands, Carolina and Delmarva Bays, and Other Coastal Plain Depressional Wetlands, and Prairie Potholes. Furthermore, “Other ‘Isolated’ Waters Substantially Affect Interstate Commerce and Should be Categorically Protected Under the Agencies’ Commerce Clause Authority.” (2) “The Rule Should Not Exempt Ditches Without a Scientific Basis” (3) “The Rule Should Limit the Current Exemption for Waste Treatment Systems”↩︎ "],["the-environmental-justice-movement-and-technocratic-policymaking.html", "Chapter 5 The Environmental Justice Movement and Technocratic Policymaking", " Chapter 5 The Environmental Justice Movement and Technocratic Policymaking Abstract This chapter explores the role of public comments in rulemaking by focusing on their role in the environmental justice movement. Environmental justice concerns focus on the unequal access to healthy environments and protection from harms caused by things like pollution and climate change. The ways in which agencies consider environmental justice highlights how rulemaking has distributive consequences, how the public comment process creates a political community, and how claims raised by activists are addressed. Examining over 20,000 rulemaking processes at agencies known to address environmental justice concerns, I find that when public comments raise environmental justice concerns, these concerns are more likely to be addressed in the final rule. Effects vary across agencies, possibly due to the alignment of environmental justice aims with agency missions. While we cannot infer that agencies addressing environmental justice concerns is caused by the public comments themselves, comments may be a good proxy for mobilization in general. Furthermore, the correlation between mobilization and policy changes is largest and most significant in agencies with missions focused on environmental and distributional policy, i.e. the kinds of agencies we may expect to be most responsive to environmental justice concerns. See the working paper version of this chapter here. "],["references.html", "References", " References Balla, Steven J. 1998. “Administrative Procedures and Political Control of the Bureaucracy.” American Political Science Review 92 (03): 663–73. https://doi.org/10.2307/2585488. Balla, Steven J, Alexander R Beck, William C Cubbison, and Aryamala Prasad. 2018. “Where’s the Spam? Mass Comment Campaigns in Agency Rulemaking.” Carpenter, Daniel P. 2001. The forging of bureaucratic autonomy: Reputations, networks, and policy innovation in executive agencies, 1862-1928. Princeton University Press. Croley, Steven. 2003. “White House Review of Agency Rulemaking: An Empirical Investigation.” Harvard Law Review 70 (3): 821–35. http://chicagounbound.uchicago.edu/cgi/viewcontent.cgi?article=5219{\\&amp;}context=uclrev. Cuéllar, Mariano-Florentino. 2005. “Rethinking Regulatory Democracy.” Administrative Law Review 57 (2). https://about.jstor.org/terms https://papers.ssrn.com/sol3/papers.cfm?abstract{\\_}id=595181{\\&amp;}download=yes. Judge-Lord, Devin. 2019. “‘Why Do Agencies (Sometimes) Get so Much Mail?’.” In. Southern Political Science Association Annual Conference. Libgober, Brian. 2018. “What biased rulemaking looks like.” https://libgober.files.wordpress.com/2018/09/what-biased-rulemaking-looks-like.pdf. Mccubbins, Mathew D, and Thomas Schwartz. 1984. “Congressional Oversight Overlooked: Police Patrols versus Fire Alarms.” American Journal of Political Science 28 (1): 165–79. Moore, Emily H. 2017. “Public Participation in Rulemaking.” {https://graduate.artsci.wustl.edu/emilymoore/}. Nelson, David, and Susan Webb Yackee. 2012. “Lobbying Coalitions and Government Policy Change: An Analysis of Federal Agency.” Source: The Journal of Politics 74 (2): 339–53. https://doi.org/10.1017/s0022381611001599. Potter, Rachel. 2017. “More than spam? Lobbying the EPA through public comment campaigns.” Brookings Series on Regulatory Process and Perspective. https://www.brookings.edu/research/more-than-spam-lobbying-the-epa-through-public-comment-campaigns/. Rosenbloom, David H. 2003. Administrative law for public managers. Westview Press. Sant’Ambrogio, Michael, and Glen Staszewski. 2018. “Public Engagement in Agency Rulemaking.” Administrative Conference of the United States. https://www.acus.gov/report/public-engagement-rulemaking-draft-report. Verba, Sidney., and Norman H. Nie. 1987. Participation in America: political democracy and social equality. University of Chicago Press. https://books.google.com/books?id=9K5fdvfmGREC{\\&amp;}dq=verba+and+nie+participation+in+america{\\&amp;}lr={\\&amp;}source=gbs{\\_}navlinks{\\_}s. Wagner, Wendy, William West, Thomas Mcgarity, and Lisa Peters. 2017. “Dynamic Rulemaking.” N.Y.U. Law Review 21 (183): 92–2017. Yackee, Susan Webb. 2006. “Sweet-talking the fourth branch: The influence of interest group comments on federal agency rulemaking.” Journal of Public Administration Research and Theory 16 (1): 103–24. https://doi.org/10.1093/jopart/mui042. ———. 2015. “Participant Voice in the Bureaucratic Policymaking Process.” Journal of Public Administration Research and Theory 25 (2): 427–49. https://doi.org/10.1093/jopart/muu007. "]]
