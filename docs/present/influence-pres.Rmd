---
# knit: ( function(input, ...){rmarkdown::render(input) } )
# rmarkdown::render("docs/present/methods-pres.Rmd")
knit: ( function(input, ...){xaringan::infinite_moon_reader(input) } )
title: "Do Public Pressure Campaigns Influence Bureaucratic Policymaking?"
author: "Devin Judge-Lord <br>  Harvard University"
bibliography: '`r here::here("assets/dissertation.bib")`'
biblio-style: '`r here::here("assets/apsr.bst")`'
link_col: cyan
date: "Papers, slides, & data:  [judgelord.github.io]()"
titletext_fontfamily: "Roboto:wght@100"
middle_fontfamily: "Roboto"
font_family: "Roboto:wght@100"
urlcolor: cyan
output:
  xaringan::moon_reader:
    lib_dir: libs
    mathjax: "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-AMS_HTML"
    css: xaringan-themer.css
    nature:
      ratio: '16:10'
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
editor_options: 
  chunk_output_type: console
---


```{r setup, include = FALSE}
short = "true"
exclude_stepwise = "false"
exclude_extra = "false"

# cache everything 
knitr::opts_chunk$set(warning = FALSE, 
                      message = FALSE,
                      echo = FALSE, 
                      #fig.path = "Figs/",
                      fig.topcaption=TRUE,
                      cache = TRUE, 
                      fig.align = 'center',
                      fig.retina = 2,
                      dpi = 100)



# Xaringan: https://slides.yihui.name/xaringan/
library("xaringan")
library("xaringanthemer")
library("here")
library("tidyverse")
library("magrittr")
library("knitr")
library("kableExtra")


style_mono_light(base_color = "#3b444b",
          inverse_link_color	
 = "#B7E4CF",
          #background_image = "Figs/ej-superfund-light.jpeg",
          background_color = "white", #FAF0E6", # linen
          header_font_google = google_font("PT Sans"), 
          text_font_google = google_font("Old Standard"), 
          text_font_size = "29px",
          padding = "10px",
          code_font_google = google_font("Inconsolata"), 
          code_inline_background_color    = "#F5F5F5", 
          table_row_even_background_color = "#ddede5", extra_css = list(".remark-slide-number" = list("display" = "none"))
 )
```

```{r, eval = FALSE, include= FALSE}

# setup
devtools::install_github("yihui/xaringan")
devtools::install_github("gadenbuie/xaringanthemer")
install.packages("webshot")
# webshot::install_phantomjs()

library(webshot)

# export to pdf
pagedown::chrome_print("docs/present/methods-pres.Rmd")

file <- here("present/methods-pres.html")
webshot(file, "methods-pres.pdf")
```


class: inverse center

# The Broader Project: Public Pressure


Mobilization


(grassroots, astroturf, elected officials)

↓

Getting policymakers' attention and framing policy debates

↓

Substantive policy influence

↓

Surviving judicial review

???
Hi, my name is Devin Judge-Lord; I'm a postdoc at Harvard Univ.
I am so grateful to be able to share this with you.
- My work is at the intersection of two literatures that have not had much contact...On the one hand, social movements and civic engagement have long been a core interest of political scientists. We are highly uncertain about the size of the effects of actions like letter writing, signing petitions, and protests, but there is little doubt that these things matter at a large enough scale.
- On the other hand, bureaucratic policy processes like agency rulemaking are often discussed as if they operate with technocratic rationality. In a sense, mass politics is missing from a lot of the administrative law literature and even from political science theories and formal models of bureaucratic policymaking. 
- The paper I am presenting today is a small part of a larger book project: 
---

class: inverse center

# The Broader Project: Public Pressure


Mobilization


(grassroots, astroturf, elected officials)

↓

<mark>Getting policymakers' attention and framing policy debates</mark>

↓

<mark>Substantive policy influence</mark>

↓

Surviving judicial review


???

It is a middle step in the causal chain from mobilization to substantive policy influence. 
As I turn this chapter into an article,
I am struggling to include just enough and not too much of what comes before and after.

I need enough about mobilization to be clear about who is empowered.

I need enough substantive policy influence, to be honest about the limited power of framing and discourse effects in any one policy fight. 
But, this paper is really about the aggregate effects of many linked campaigns on the discourse and issue frames in the policy process---that is, how a movement affects policymaking 
across institutions and over time.

---

## Preview

**Data:** policy demands and lobbying success for 40 million public comments to 60 agencies

--

###Findings
 
**1. Framing:** Public pressure affects policy debates.

- Activists get climate and environmental justice language into policy documents
- Public pressure helps
- Institutions affect responsiveness

--

**2. Lobbying success:** **No evidence of direct effect** of public pressure on policy substance. 
--
But, pressure has **indirect effects**: pressure campaigns mobilize legislators, and coalitions with more members of Congress are more likely to win.

???
Today, I'm going to show that public pressure can affect the policy process, but with a lot of caveats. And the caveats are often the most interesting part. 
The first caveat is that baseline rates of addressing EJ are very low from the Clinton through the Trump administrations. 

There is a lot of potential for change, which is a tough road for the movement, but helpful for the scientist trying to measure change.

---

## Who does notice and comment rulemaking empower? 

--

### If groups build the power to pass a Green New Deal, what happens when 30+ agencies write the actual policies? 

???
For example, if a GND is passed, what happens when dozens of agencies write the implementing rules?

Or consider that the Biden Administration has pledged to spend 40% of climate and energy spending, including the infrastructure bill in disadvantaged and EJ communities. There is much discussion this week because the administration omitted race from the definition of an EJ community because they are worried the courts will block it. 

Even without IRA or a GND, the Biden-Harris administration is taking major policy action through rulemaking.

--

### Will organized public pressure carry over to agency policymaking? 
--
If so, by what mechanisms?

- General level of public attention?

- Specific pressure to address issues like climate and environmental justice?

- By targeting more receptive institutions?

- By mobilizing political oversight? 


???
Will organized public pressure carry over to agency policymaking? 

- If so, what kinds of politics are influential? Is it general attention? Is it specific pressure groups like environmental justice activists?

- are some institutions more receptive to pressure on EJ than others? 

This is where I turn to the literature on bureaucratic politics. 

---

## Theory: Information is the currency of lobbying

### Scholars focus on *technical* information

- Bureaucratic policymaking, especially, is about expertise (Wagner, 2010)

--

### Does *political* information matter?

- Coalition size? (Nelson and Yackee 2012 find it does)

- The scale of public pressure? (Schattschneider would say that "expanding the scope of conflict" matters, but Balla et al. [2020] find that "legal imperatives trump political considerations")

- Positions of political principals (McCubbins & Schwartz 1984)

- Who is affected? (Lowi would say it should)


???

Everything we know about lobbying, especially lobbying in agency rulemaking, tells us that information is the currency of lobbying. 

Information causes policymakers to change their minds, *especially* when policymakers are experts and lawyers.

Indeed, research shows that business groups dominate lobbying in rulemaking *because* they can generate and provide relevant information. 

Thus far, by "information," scholars generally mean technical and legal information.

- I ask about the role of a different kind of information--what we might call political information. Building on Nelson and Yackee's finding that coalition size matters, we might also wonder if groups raising distributive claims would matter (Lowi would say so) and whether public pressure or public attention matters. Here scholarship is skeptical. Balla et al. concluded that mass comment campaigns don't matter because "legal imperatives trump political considerations." in responding to comments. That technocratic rationality dominates, meaning that political information is not affecting the policy process. I want to test this claim. 

When I state my hypotheses, I put them in the direction that EJ campaigns do affect policy while noting equally powerful intuitions going the other direction. 
I really think there are good intuitions going both ways. 
These are very much two-tailed tests. 

---

### Data: Rates of Addressing Climate & Environmental Justice 2005-2020

```{r cj-data-agencies, out.width = "90%"}

knitr::include_graphics("Figs/cj-data-agencies-2.png")
```


---


### Data: Comments on Draft Rules

Of 13,111 relevant agency rules, less than 15% addressed environmental justice, and 8%  addressed climate change, despite growing activist demand.

Of 39,392,957 public comments 

- 20% <!--, 17,857,018 (421,880 unique)--> raise "climate change"

- 2% <!--, 3,248,697 (2,138 unique)--> also mention environmental justice or climate justice

- 82% of all comments raising EJ also mention climate change, but only 14% of **unique** comments raising EJ also mention climate change

---



background-image: url(https://www.sierraclub.org/sites/www.sierraclub.org/files/uploads-wysiwig/SIERRA%20Forefront%20Of%20Change%20WB.jpg)
background-size: cover


`"The amount of methyl-mercury and other bioaccumulative chemicals consumed by Alaskans (especially Alaskan Natives) could potentially be much higher than is assumed" - Heather McCausland of the Alaska Community Action on Toxics (ACAT)`

---

background-image: url(Figs/earthjustice.png)
background-size: contain
background-position: bottom
background-color: white


`"Such an approach ignores the cumulative pollution burdens experienced by environmental justice communities." - Amanda Goodin, Staff Attorney, Earthjustice on behalf of Communities for a Better Environment et al.`

---

background-image: url(https://upload.wikimedia.org/wikipedia/commons/7/7c/DC-Climate-March-2017-1510718_%2833551761583%29.jpg)
background-size: cover

`"Attached are files containing the names of 11,478 individuals who have submitted public comments urging the Bureau of Land Management (BLM) to strengthen the proposed regulations on methane waste and pollution on federal and tribal lands"` 

???

- `"...I am committed to taking a stand against oil and gas exploration on public lands, as this is an Environmental Justice issue here in our vulnerable communities..."`


---

# Measuring influence

### I. Getting policymakers' attention/engagement/response

- Adding policy language 
- Changing policy language

### II. Getting substantive policy demands

-  Lobbying success for all comments on a random sample of 150 rules from 60 agencies, 2005-2020
 - 10,894 hand-coded documents representing over 41 million comments 
 - 284 coalitions 


---

class:  inverse 

# Model Results 

--

## (1) Variation across agencies:  
Pr(Policy Change | President) ~ Policy Demands  
\+ Coalition Size  (number of organizations)
\+ Public Pressure Campaign  (number of comments mobilized)
\+ Policymaker (Agency) Receptivity (past rate of responsiveness)

--

## (2) Variation within agencies:  
Pr(Policy Change | President + Agency) ~ Policy Demands  
\+  Coalition Size  
\+  Public Pressure Campaign

???
Both of my DVs---adding EJ language and changing existing EJ language--are dichotomous, so I m going to use logit regression. 

The main models use president and agency fixed effects, so I'm focusing on variation within presidential administrations and within the agency, but I also assess differences across presidents and agencies by estimating the same models with indicators rather than fixed effects.

Because log odds coefficients are hard to interpret and because my models have interactions, I am going to skip the regression table and just show you predicted probabilities. 

---

background-image: url(Figs/ej-table-paper.png)
background-size: contain
background-color: white

---

background-image: url(Figs/cj-table.png)
background-size: contain
background-color: white

???

Because I expect decreasing marginal effects of additional comments, both overall (i.e., public attention) and those that mention EJ (coalition size), I use logged values. 

However, the conclusions are largely the same if I use a quadradic function instead. That analysis suggests that medium levels of attention are not so impactful. A small level of attention matters, and then it takes a very large amount of additional pressure to move things, which makes sense. For hypothesis testing, however, all of the effects are in the same direction, so I am going to present predicted probabilities from these models here.

---

exclude: `r exclude_stepwise`

## Probability of Adding "Environmental Justice" to Policy

```{r ej-m-PR-president-median-2, out.width = "100%"}
knitr::include_graphics("Figs/ej-m-PR-president-median-2.png")
```

???

All of the plots I'll show have predicted probability of change on the x-axis and some predictors we care about on the y-axis. This one also shows predicted values for two different agencies, the EPA and the PHMSA. 


---

## Probability of Adding "Environmental Justice" to Policy

```{r ej-m-PR-president-median-1, out.width = "100%"} 

knitr::include_graphics("Figs/ej-m-PR-president-median-1.png")

```


---

## Receptive Policymakers are More Responsive

```{r ej-m-PR-shareI, fig.cap = "", out.width = "100%", fig.show = "hold", fig.subcap=""}

knitr::include_graphics("Figs/ej-m-PR-shareI-1.png")

```

---

## Policymakers Respond to Public Pressure


```{r ej-m-PR-comments-agencyFE, out.width = "100%", fig.show = "hold"}

knitr::include_graphics("Figs/ej-m-PR-comments-agencyFE-1.png")

```

---

## Policymakers are More Responsive to Larger Coalitions

```{r climate-m-PR-climatecomments-agencyFE,  out.width = "100%"}


knitr::include_graphics("figs/climate-m-PR-climatecomments-agencyFE-1.png")

```

---

class: inverse center middle

# Results: Substantive Lobbying Success 


---

## Larger coalitions → more likely to win 
(coalition-level OLS Regression)


```{r m-influence, out.width = "30%"}
knitr::include_graphics(here::here("figs", "m-influence.png"))
```

---

## Pressure campaigns mobilize legislators 
(coalition-level Poisson Regression)

```{r m-congress, out.width = "50%"}
knitr::include_graphics(here::here("figs", "m-congress.png"))
```

---

## When organizations mobilize more legislators, they are more likely to win 
(within-organization OLS)

```{r m-diffindiff, out.width = "50%"}
knitr::include_graphics(here::here("figs", "m-diffindiff.png"))
```

---

class: inverse

## To sum up: large framing effects, but limited substantive policy influence 


- Policymakers rarely address climate and environmental justice 
--
, but they are much more likely to do so when pressured
--
, especially receptive policymakers

--
- Some evidence that larger coalitions win
- No evidence of direct effects of pressure campaigns on substantive policy
- Some evidence of indirect effects by mobilizing members of Congress

???
- With larger EJ coalitions and more public attention, agencies are more likely  📈 to **add** EJ language **where there was none**
- BUT agencies also anticipate public attention, making **existing** EJ analyses less likely 📉 to **change** on higher-salience rules.

???
The second caveat is that pressure matters to receptive policymakers--policymakers who are used to thinking about ej at institutions with organizational processes for addressing EJ. 

Third is that the scale of public attention makes policymakers more likely to address EJ when they did not, but higher-salience policies that already addressed EJ are less likely to change. I suspect this is because agency officials have already dotted their i's and crossed their t's in these cases, and there is little room left for policy to move. 

--




HOWEVER, groups raising climate and environmental justice are usually "big green" (White) advocacy organizations.  
--

- Big greens are much more likely to have their substantive policy demands met than tribes or frontline EJ groups.

--

*Thank you!* Papers, slides, & data:  [judgelord.github.io]()


???

Finally, assessing the normative implications requires us to understand who is advocating for whom. In the case of comments on federal agency rules, it is "big green" national advocacy groups that have historically led a very White environmental movement. 
These "big greens" are more likely to get substantive policy demands met than Tribes and frontline community groups. So, a lot hinges on the extent to which these national organizations represent EJ communities.

















---

class: inverse

# Thank you!

- Paper, slides, data: [judgelord.github.io](judgelord.github.io/)
- Rules relevant to climate or environmental justice currently open for comment: [judgelord.github.io/rulemaking/open]( https://judgelord.github.io/rulemaking/open)

<!--

Questions
- More or less on mobilization or hand-coded influence? 
- Aggregate impact of related campaigns $\rightsquigarrow$   social movement impact?
- Should "climate change" replication be in this paper? 
- Other discursive terms/movements?

-->

Next 
- [Audit study](): What causes institutional receptivity?
- [Co-framing](): EJ + "health", "disaster", "climate" & changes in term frequency
- [Surveys]() to compare comments to public opinion
- [Lobbying networks]()
- [Feedback](): The mobilizing and demobilizing effects of the policy process

???

Carving out a paper: More or less on mobilization or hand-coded influence? 

Is the aggregate influence of related campaigns a defensible working deff of a social movement, or is there a better way of talking about this? 

co-framing (Baumgartner and Jones) 

<!--
Next Steps
- More on coalition structure and policy success, especially opposing coalitions
- Better (hand-coded) measures of policy change
- Model changes in texts that already discuss climate/EJ/CJ

Framing 
- More on how social movements may impact policy. Petitioning and protest? Lobbying?
-->


exclude: `r exclude_extra`
class: inverse center middle 

# Extra Slides

---


```{r ej-winrates}
library(kableExtra)
library(scales)
load(here::here("data", "winrate.Rdata"))
winrates %>% 
  mutate(`EJ Success Rate` = `EJ Success Rate` %>%
           percent(accuracy = 1) %>% replace_na("-"),
         `Overall Success Rate` = `Overall Success Rate` %>%
           percent(accuracy = 1)) %>% 
  dplyr::select(-`N raising EJ`) %>% #mutate(across(where(is.numeric), pretty_num)  ) %>%
      knitr::kable(caption = "") %>% 
      kableExtra::kable_styling(font_size = 22, position = "center")  #kable3(caption = "Hand-coded Lobbying Success by Type of Organization, 2005-2020")
```


---

exclude: `r short`

## Lobbying Success by Campaign Size

```{r coded-coalition-success, out.width = "60%"}
knitr::include_graphics(here::here("figs", "coded-coalition-success-1.png"))
```

---

### *Distributive Claims Hypothesis*: 

> Policymakers are more likely to address distributive justice when groups raise distributive justice concerns.

✅ Pro: "norms and values are set communicatively" (Habermas 1996), and "identifying interests" shapes policy (Gellhorn 1972)   


❌ Con: Bureaucratic policymaking is about "technical expertise" (Epstein et al. 2014)

???
First, we might expect policymakers are more likely to address distributive justice when groups raise distributive justice concerns.

Conversely, if bureaucratic policymaking is about "technical expertise"

---

### *Pressure Hypothesis*

> Policymakers are more likely to address concerns as more groups raise them.

✅ :
Coalition size (Nelson and Yackee  2012) & pressure (Gillion 2013; Wasow 2020)

❌ : "Informational value" (Epstein et al. 2014; Gailmard & Patty 2017; Libgober 2018)

???

On the one hand, we might expect policymakers to respond to the scale of pressure to address EJ.

On the other hand, all of the formal models to date are focused on providing new information. 
I really like Brian Libgober's model of how comments inform rulemaking, but it relies on telling policymakers something they didn't already know. 

---

### *Policy Receptivity Hypothesis*

> Policymakers that more frequently address certain concerns will be more responsive to groups raising those concerns.

<!-- "Legibility" (Scott 1998), -->
✅ : "Opportunity structure" (Marks & McAdam 2007; McAdam 2010) "fit inside the legal narrative" (Scott 1998; Deloria 2009; Hilson 2002; Delaney 2017)   

❌ : "New information" Farina (2018)

???
Regarding policy receptivity
Hilson argues that 
> "[Political opportunity] must be seen not just in terms of openness (in other words, access to the administration), but also in terms of political receptivity to the claims being made." (Hilson 2002, p. 242)

Claims may be more impactful when they are more legible to policymakers and fit inside their existing legal narrative and self-concept of what their job is.

Other legal scholars and formal models suggest that information should matter most when it is not something policymaker is already thinking about. If they are right, then we should expect the opposite---that activists raising EJ concerns are most influential at agencies that DON'T usually think about EJ---where it presents them with something new.


---

### *Public Attention Hypothesis*

> Policies are more likely to change when they receive more public attention (e.g., more public comments).

✅ : Leech (2010)

❌ : Lowery (2013), Balla et al. (2020)

???

Finally, there is this debate over whether high or low salience policies are more likely to change. 

We might expect that policies are more likely to change when there is more overall public attention, regardless of pressure to address EJ in particular. 

On the other hand, if it is easier to lobby out of the public spotlight, we might expect low-salience policies to be more likely to change. 

Balla et al.'s study of responses to comments in rulemaking suggests no effect. Legal scholars tend to see the scale of public attention is just not a legal concern. 

And again, formal models don't really have a parameter for public attention or pressure. 

SKIPPING 
*Conditional Attention Hypothesis*

> Policies are more likely to address an issue when they receive more public attention (e.g., more public comments) *and* groups raise that issue.

---

exclude: `r short`

background-image: url(https://assets.nrdc.org/sites/default/files/styles/full_content/public/media-uploads/midwesttoxicdoughnut_25_002chicago_steel_mills-july_1965_vl_2400.jpg?itok=yXMNtQ-Y)
background-size: cover


## Why EJ?

1. Variation in issue framing: "environmental" policy is inconsistently racialized +  inconsistently focused on *distributions* of costs and benefits

1. Distinct phrase (few false positives) without many synonyms (few false negatives)

1. E.O. 12898 "Federal Actions to Address Environmental Justice" (1993)


???

--> Administrative Procedures Act lawsuits: 
> "environmental justice analysis can be reviewed under NEPA and the APA" (*Protect Our Communities Foundation v. Salazar* 2013; *Communities Against Runway Expansion, Inc. v. FAA* 2004)


Environmental Justice For All Act (2020) introduced by Senator Harris

The day after Biden's executive order was launched, Rep. Cori Bush (D-MO) and Sens. Tammy Duckworth (D-IL), and Ed Markey (D-MA) introduced the Environmental Justice Mapping and Data Collection Act of 2021, which builds on many of the concepts in the executive order and would create a whole-of-government initiative, including data infrastructure and funding to “identify communities most at risk from environmental injustices.”

---

exclude: `r short`

background-image: url(Figs/clinton-ej.jpg)
background-size: cover


`“Addressing disproportionately high and adverse human health or environmental effects of programs, policies, and activities on minority populations and low-income populations.”`

---


background-image: url(Figs/brianadams.png)
background-size: cover

## Example: Safe Levels of Mercury 
--
(For Whom?)

???
Before the politics of who gets what, there is the politics of who the whos *are*. What are the groups or communities deserving consideration? 

- 2000 Notice: "the U.S. population."

--

- 2002 Draft: Regulated entities + "Other types of entities not listed could also be
affected."

--

- 2011 Draft: disparate impacts on "vulnerable populations" including "African Americans," "Hispanics,"
"Native American," and "Other and Multi-racial" groups.

--

- 2012 Final Rule: EJ analysis adds "minority, low income, and indigenous"
--

- 2020 Rollback: "These communities may experience foregone benefits" 

--

- 2021 Draft: 2012 Final Rule catagories + "differentiated subsistence fisher
populations" + "children exposed prenatally"



???
2011 Draft: Five pages of EJ analysis of the disparate impacts
on

- 2020 Rollback: "While these communities may experience foregone benefits as a result of this action, the potential foregone [health benefits] are small." 

⁉️

EJ on 30/51 pages in Biden rule

---

## Data: ~25,000 Policy Documents

13 thousand draft and final rule pairs* from 40 agencies, 1993-2020

```{r ej-data, fig.show = "hold", out.width = "100%"}

knitr::include_graphics("Figs/ej-data-ejpr-1.png")  
```

*A few rulemaking dockets have more than one draft or final rule.

---

exclude: `r short`


## Data: ~42,000,000 Public Comments


```{r ej-comments, out.width = "80%"}

knitr::include_graphics("Figs/ej-comments-1.png")
```


???

I collected all 40 million comments on these draft and final rule pairs. 
Almost 5 million mentioned EJ.

The top row of plots with the purple border shows proposed rules that did address EJ; I use this set to assess whether EJ language changed between the draft and final. 

The lower two rows in the red box show proposed rules that did not address EJ, and the middle row shows rules where EJ language was added. 

Each point is a rule. 
Blue ones are ones where commenters raised EJ concerns. 
Red ones are where no comments raised EJ concerns.
Y-axis is the total number of comments the rule received. 

---


## Data: ~42,000,000 Public Comments

~4,800,000 (~28,000 unique) comments raise EJ concerns


```{r percent-match, fig.show = "hold", out.width = "45%", fig.cap="Example: Identifying Coalitions by the Percent of Matching Text in a Sample of Public Comments"}

knitr::include_graphics("Figs/comment_percent_match_plot.png") 
```

???

Using n-gram matching methods similar to plagiarism detection, I collapse form letters and petitions into representative texts. As most comments are form letters, that 5 million collapses to 28 thousand unique texts.
This figure shows the percent of shared text in a sample of documents.
The black squares on the diagonal show that each comment has a perfect overlap with itself. 
The block of grey partial matches reflects a public pressure campaign, with a lot of shared 10-grams. 
I then attach these to lobbying coalitions by hand. 



exclude: `r short`

## Who does notice and comment rulemaking empower? 

```{r ejcommentsbyrace, fig.cap = 'Estimated Racial Distribution from Census Surnames of Commenters raising "Environmental Justice" Concerns in Rulemaking', fig.height = 1.5, fig.width=2, out.width = "40%", fig.show = "hold"}
load(here::here("data", "ej_race.Rdata"))

Race %>% ggplot() + 
  aes(y =Race, x = Probability) + 
  geom_col() +
  theme_minimal() +
  theme(panel.grid.major.y = element_blank())
```

---

exclude: `r short`

## Hand-coded coalitions and policy change

```{r datasheet, fig.show = "hold", out.width = "100%", fig.cap="Example: Identifying Coalitions by Hand"}

knitr::include_graphics("Figs/datasheet.png")
```


???

I obtain uncertainty by using indicators rather than fixed effects. 


---

## Results: Adding EJ to Policy

```{r ej-m-PR-ejcomments-agencyFE,  out.width = "90%"}
knitr::include_graphics("Figs/ej-m-PR-ejcomments-agencyFE-1.png")
```

- ✊🏿 🔁   Coalition Size 📈 



- ✊🏿  Distributive Claims 📈 
- 📢  General Public Pressure/Attention 📈  (when comments don't raise EJ)

---

## Results: Changing Existing EJ Language

```{r ej-mejPR-ejcomments-agencyFE, out.width = "90%"}

knitr::include_graphics("Figs/ej-mejPR-ejcomments-agencyFE-1.png")
```


- ✊🏿🔁  Coalition Size 📈 

---

## Results: Changing Existing EJ Language

```{r ej-mejPR-comments, out.width = "90%"}

knitr::include_graphics("Figs/ej-mejPR-comments-agencyFE-1.png")
```


- ✊🏿  Distributive Claims 📈 
- 📢  General Public Attention 📉 (when comments don't raise EJ)

---

## Results: To sum up, yes, movements matter

Strong framing effects, but substantive victories are rare and dominated by surrogates.

|                                            | Add EJ Language | Change EJ Language |
| ------------------------------------------ | :--: | :--: |
| ✊🏿  Distributive Claims (EJ Concerns)     | 📈 | 📈 |
| ✊🏿 🔁  Coalition Size /  Pressure        | 📈 | 📈 |
| 🏛️  Policy Receptivity                             | 📈 | 📈 |
| 📢  General Public Attention               | 📈 | 📉 |


<!--| 📢x✊🏿 Specific (Conditional) Pressure            | 📉 | ❓ | -->

With larger coalitions and more public attention, agencies are more likely  📈 to **add** EJ language **where there was none**, BUT agencies also anticipate public attention, making **existing** EJ analyses less likely 📉 to **change** on higher-salience rules.

--

[Replicates with "climate change"](https://judgelord.github.io/research/cj/)



---



# The EJ Frame

- Emerged in local campaigns against toxic waste sites and pesticides
- Began appearing in federal policy documents in the 1980s
- A fusion of the American Indian Movement, Chicano movement, farmworker movement, civil rights movement, and union organizing

> People of color [raised] social justice concerns such as self-determination, sovereignty, human rights, social inequality, loss of land base, limited access to natural resources, and disproportionate impacts of environmental hazards and linked them with traditional working-class environmental concerns such as worker rights and worker health and safety to develop an environmental justice agenda. (Taylor 2012, p. 1)

???

Dorceta Taylor describes how black-led campaigns like Johnson's fused with the American Indian Movement, Chicano, and farmworker movements and drew on the civil rights and union organizing to push the EJ frame onto the national policy agenda 
and EJ began appearing in federal documents in the 1980s. 

My aim is to add some more systematic quantitative data on the broad trends about the influence of EJ movement on the federal policy agenda that Taylor and others have mapped.


---

exclude: `r short`

## Do Social Movements Cause Policy Change?

**Yes.** (Dahl 1956; Lipsky 1968; Piven & Cloward 1977; Tarrow 1994; Andrews 1997; McAdam 1982, 2001; McAdam & Su 2002, McCammon et al. 2011; Cress & Snow 2000; Weldon 2002)



- **Activists shape parties** (Cohen et al. 2008, Schlozman 2015, Skocpol & Williamson 2016)



- **Petitioning government builds movements** (Carpenter 2021)



- **Redistributive policies often require social movements** (Lowi & Nicholson 2015)



- **Protests affect policy by informing** (Gillion 2013, Gause 2022) **and seed policy agendas** (Wasow 2020)



- **No movement, no policy** (Skocpol 2013)

???

The big question is whether social movements impact policy.

- There is a lot of research on social movements, much of it from sociology, but political scientists tell us that social movements do 
- shape party agendas
- that the act of petitioning the government builds power
- In Lowi's words, "redistributive policies have been associated with social classes and movements from the beginning"--some kinds of policy only come about when groups form and mobilize to affect policy
- We don't know a lot about policy outcomes, but there is some great work showing specific policy effects of civil rights protests, for example.
- And as Theda Skocpol found in her study of why climate legislation failed, a movement is often a necessary condition for policy change

Despite excellent recent work (like Dan Gillion and Omar Wa-So's work on civil rights protests, Jamila Michener), there is still demand for a better understanding of the role of social movements in policymaking. 

---


## Research tends to explain social movement emergence rather than specific impacts (McAdam 2017) 

- **"limited research on [social movement] influence"** (Andrews & Edwards 2004)

- **The DV is rarely specific policy outcomes or systematic impact**


???
In the paper I note some of the great work in political science studying the impact of social movements on policy change, but much of the systematic work focuses on explaining mobilization. Movement activity and structure are the typical DVs. 


--

## My DV: Systematic impact on specific policy documents (**agency rules**)

???
I am trying to add to recent work on policy impact by looking systematically at change in very specific policy documents---executive branch agency rules, which now make up 90% of US law; 

Whether executive branch agencies are implimenting recent legislation or using authority of centry-old statutes,  Rulemaking is where the teeth of federal law are forged and reforged over time.

So, I'm interested in the extent to which social movements can affect agency rulemaking.

--

## Aggregate impact of EJ campaigns across institutions and over time $\rightsquigarrow$ EJ movement impact

???
And I'm thinking about social movement impact as the aggregate impact of EJ campaigns across institutions and over time. 

Put differently, 
Whom does notice and comment rulemaking empower? 

---


---

exclude: `r exclude_extra`

# Next steps: methods

- Test policy receptivity using rates of addressing EJ out of sample (direct rules & notices)
- Better measures of policy change
- cor(EJ Language, Lobbying Success)
- Alternative estimation (hierarchical Bayes)
- Cumulative effects over time (hazard models)
- Placebo test? 

---

exclude: `r exclude_extra`


### Replication & Extention: Climate Change & Climate Justice 

```{r cj-data-agencies2, out.width = "90%"}

knitr::include_graphics("Figs/cj-data-agencies-2.png")
```

---

exclude: `r exclude_extra`


```{r cj-table2,  out.width = "80%"}

knitr::include_graphics("Figs/cj-table.png")
```

---

Photo credits: 

- [Brian Adams]( https://brianadams.photoshelter.com/portfolio/G00005IRIxI9ewPM/I0000768RCEwKxsQ)


---
 
Human coding and computational text analysis are more powerful when combined in an iterative workflow.
<!--I show how search and text-reuse tools can aid common hand-coding tasks. 
Human coding can both inform and be informed by rule-based information extraction---iteratively structuring queries on unstructured text.-->

1. Text analysis tools can strategically **select texts for human coders**---texts representing larger samples and outlier texts of high inferential value.
2. Preprocessing can **speed up hand-coding** by extracting features like names and key sentences. 
3. Humans and computers can iteratively **tag entities** using regex tables 
<!--(e.g., identify organizations)--> and **group texts by key features** (e.g., identify lobbying coalitions by common policy demands)


Applying simple search and text-reuse methods to public comments on all U.S. federal agency rules, a **sample of 10,894 hand-coded comments** yields **41 million as-good-as-hand-coded comments** regarding both the organizations that mobilized them and the extent to which policy changed in the direction they sought. <!--This large sample enables new analyses of lobbying coalitions, social movements, and policy change.-->

<!--
# The Broader Project: Public Pressure

Mobilization


(grassroots, astroturf, elected officials)

↓

Getting policymakers' attention and framing policy debates

↓

Substantive policy influence

↓

Surviving judicial review



# 50 million public comments on proposed agency rules

---->




<!--
# Challenges and Opportunities in Studying Political Texts
--

--->

# Hand-coding dynamic data

<!--
- 15 coders 

```{r greyhound, fig.cap= "Incorrectly Labeled Coalition Identified by Automated Check"}
knitr::include_graphics(here("figs", "greyhound.png"))
```

--->

Workflow: `googlesheets4` allows analysis and improving data in real time. For example, in Fig. 1: 

- The "org_name" column is populated with a guess from automated methods. As humans identify new organizations and aliases, other documents with the same entity strings are auto-coded to match human coding.   
- As humans identify each organization's policy "ask," other texts with the same ask are put in their coalition.   
- If the organization and coalition become known, it no longer needs hand coding.

**Fig. 1:** Coded Public Comments in a Google Sheet

```{r datasheet2, fig.show = "hold", out.width = "100%"}
 
knitr::include_graphics(here("figs", "datasheet.png"))
```


# Regex tables to tag entities


- **Deductive:** Start with databases of known entities.


<!--### Consolidating entity name variants with regex tables-->


```{r regex-crp}
#CRP data
here("data", "Lobbbying_Summary.csv") |>
read_csv() |>
  filter(parentName %in% c("Teamsters Union", "3M Co") ) |>
  filter(!orgName %in% "3M Cogent") |>
  mutate(orgName = orgName %>% str_replace(".*Teamsters.*", "Teamsters")) |> 
  distinct(orgName, parentName) |> 
  group_by(parentName) |>
  summarize(pattern = str_c(orgName, collapse = "|") ) |>
  mutate(pattern = pattern %>% str_replace("Teamsters & ", "Teamsters|Teamsters.{1:4}") |>
           str_replace("Maint ", "Maint[a-z]* ") |>
           str_replace("Trucking Pension", "(Trucking|Pension)") |>
           str_replace_all("&|and", "(and|&)") |>
           str_remove_all(" Div")) |>
  rename(Entity = parentName, Pattern = pattern) |>
  knitr::kable(caption = "Lookup Table Deduced from Center for Responsive Politics Lobbying Data, Collapsed into an Initial Regular Expression Table", full_width = T) |>
  kableExtra::kable_paper()
```

- **Inductive:** Add entity strings that frequently appear in the data to regex tables.  
- **Iterative:** Add to regex tables as humans identify new entities or new aliases for known entities. Update data (Google Sheets) to speed hand coding.

<!--### Add to regex tables as hand coders identify new aliases-->

**Fig 2:** Iteratively Building Regex Tables

```{r, out.width = "100%", fig.show='hold'}
#knitr::include_graphics("https://docs.google.com/drawings/d/e/2PACX-1vRSJJgSbMdFtkpJjB94Tw5cmFElhDKrHEkOLaUb5-qFdIY-sQp4qGyC-FZoZ7UmWSLfoMZDueAkemhU/pub?w=752&h=238") 

knitr::include_graphics(here::here("figs", "methods-regex.png"))

#knitr::include_graphics(here::here("legislators" , "man", "figures", "logo.png") %>% str_remove("dissertation"))
```

<!--
The `legislators` package adds name variants (e.g., "Liz Warren") to standard legislator names.

```{r legislators, eval = FALSE}
library(legislators)

members |>
  filter(first_name == "Elizabeth", congress == 117) |>
  select(Entity = bioname, Pattern = pattern) |>
  mutate(Pattern = Pattern |> 
           str_replace_all("\\\\b" , "\\\\\\\\b") %>% 
           str_remove_all("na |\\|herring, e\\\b|\\|herring, elizabeth|\\|liz herring|\\|warren, l\\\\b") %>% 
           str_replace("representative fletcher\\\\b.\\{1,4\\}tx", "\\|representative fletcher") %>% 
           #str_replace("fletcher\\|fletcher", "fletcher") %>% 
           str_replace("senator warren\\\\b.\\{1,4\\}ma", "\\|senator warren") |>
  str_replace_all("\\\\b", "\\\\\\b") )%>% 
  kable( caption = 'Regex Table for Legislators Named Elizabeth in the 117th Congress', full_width = T)|>
  kableExtra::kable_paper()
```

-->

For example, the `legislators` package uses a regex table, adding variants (e.g., "AOC") to standard legislator names to detect them in messy text.

---


### Results: Who mobilizes public comments?

```{r}
# code in the top_orgs.R
load(here("data", "org_counts_summary.Rdata"))

top5 <- org_counts_summary[1:5,] %>% tally(comments)
top10 <- org_counts_summary[1:10,] %>% tally(comments)
top100 <- org_counts_summary[1:100,]  %>% tally(comments)
```

<!--Iteratively linking comments to the organizations that wrote or mobilized them (and thus strings to identify similar documents), I find that a few advocacy organizations mobilize the vast majority of comments. 
-->
Of 58 million public comments on proposed agency rules, the top 100 organizations mobilized `r top100 |> prettyNum(big.mark = ",")`. The top ten organizations mobilized `r top10 |> prettyNum(big.mark = ",")`.


```{r}
# code in the top_orgs.R
load(here("data", "org_counts_summary.Rdata"))

top10 <- org_counts_summary[1:10,]
top100 <- org_counts_summary[1:100,]

# Pretty up for presentation
org_counts_summary %>%
  dplyr::select(org_name, rules, campaigns, percent, comments, average) %>% 
  mutate(comments = comments %>% prettyNum(big.mark = ","),
         average = average %>% prettyNum(big.mark = ",")) %>% 
  rename(Organization = org_name,
         Comments = comments,
         `Rules Lobbied On` = rules, 
         `Pressure Campaigns` = campaigns,
         `Percent (Campaigns /Rules)` = percent, 
         `Average per Campaign` = average) %>% 
  head() %>% 
  #mutate(Organization = Organization %>% 
           #str_rpl("Natural Resources Defense Council", "NRDC") %>% 
           #str_rpl("World Wildlife Fund", "WWF") %>% 
           #str_rpl("Pew Charitable Trusts", "Pew")) %>% 
  kable(caption = "The Top 5 Organizations Mobilized 20 Million Public Comments",
         full_width = T, font_size = 55, align= c('l', rep('c', 5))) |>
  kableExtra::kable_paper()
```


# Grouping with text reuse {.mybreak}

**Fig. 3:** Iteratively Group Documents

```{r, out.width = "100%", fig.show='hold'}
#knitr::include_graphics("https://docs.google.com/drawings/d/e/2PACX-1vSy9MpvnXV0nbZj4kUPnWC38bwk5xiBXjRiGC3fyrddwoZVT19SmfT4WRr_AvOH5lxeKyrTenqLlKmm/pub?w=953&h=603")

knitr::include_graphics(here::here("figs", "methods-ngrams.png"))

```


<!--## Collapsing form letters with text reuse-->


**Fig 4:** Identifying Groups of Linked Documents with Text Reuse (a 10-gram Window Function)

```{r percent-match2, fig.show = "hold", out.width = "55%"}

knitr::include_graphics(here::here("figs", "comment_percent_match_plot-2.png")  )
```

- Document A shares no 10-word phrases with the others 
- B, C, and D share some text (they are part of an organized mass comment campaign) 
- E and F are the same text that was submitted twice

---

### Results: Most public comments result from organized pressure campaigns

**Fig. 5:** Public Comments on Regulations.gov, 2005-2020

```{r comments-mass, out.width = "100%", fig.show = "hold"}
knitr::include_graphics(here::here("figs", "comments-mass-1.png"))
```

Comments that share a 10-gram with 99 or more others are part of a mass comment campaign.

---

# Grouping with key phrases

1. Humans identify groups of selected documents (e.g., lobbying coalitions)
2. Humans copy and paste key phrases
3. Computer puts other documents containing those phrases in the same group (coalition)

*Preprocessing tip:* 
<!--**Digitized text** allows humans to paste text exactly matching machine-read strings.  -->
**Summaries** speed hand-coding (e.g., use `textrank` to select representative sentences).


<!--
## Hand-coded coalitions and key demands

---

## Iteratively adding texts to groups with text reuse

---

## Selecting Texts of High-inferential Value {#select}

---

## Inferring Lobbying Success From the Success of Others in a Coalition {#success}

### Coding commenter demands

<!--
## Coding policy positions {#spatial}


```{r spatial-coding, fig.cap= "Coding the Spatial Position of Comments on Proposed Policy Changes", fig.height=4, fig.width=5.5}
include_graphics(here("figs", "spatial-coding-1.png"))
```





---

### Comments from Legislators Correlate with Public Pressure

---

### The Dependent Variable: Lobbying Success

---
--->



**Fig. 7:** Policy Text Change by Coalition Size

```{r ej-m-PR-ejcomments-agencyFE2,  out.width = "100%", small.mar = TRUE}
knitr::include_graphics("Figs/ej-m-PR-ejcomments-agencyFE-1.png")
```



```{r ej-winrates2, eval=FALSE}
load(here::here("data", "winrate.Rdata"))
winrates %>% 
  mutate(`EJ Success Rate` = `EJ Success Rate` %>%
           percent(accuracy = 1) %>% replace_na("-"),
         `Overall Success Rate` = `Overall Success Rate` %>%
           percent(accuracy = 1)) %>% 
  dplyr::select(-`N raising EJ`) %>% #mutate(across(where(is.numeric), pretty_num)  ) %>%
      knitr::kable(caption = "Lobbying Success by Type of Organization, 2005-2020") %>% 
  kable_paper() |>
      kableExtra::kable_styling(position = "center")  #kable3(caption = )
```





# Next steps

- Compare exact entity linking (regex tables) to probabilistic methods (`linkit`, `fastlink`, supervised classified with hand-coded training set)
- Compare exact grouping (e.g., by policy demands) to supervised probabilistic classifiers/clustering

<!--
An example dashboard is available here: <https://judgelord.github.io/correspondence/FERC/DOE_FERC-letter-coding.html

<!--
```{r check-company, fig.cap= "Checking for Disparities Among Coders in Real Time", out.width="100%"}
include_graphics(here("figs", "check-company.png"))
```


```{r check-backslashes, fig.cap= "Checking for Incorrect Coding in Real Time", fig.height=4, fig.width=5.5}
include_graphics(here("figs", "check-backslashes.png"))
```

# References 

```{r, include=FALSE}
knitr::write_bib(c('knitr','rmarkdown','posterdown','pagedown'), 'packages.bib')
```



<!--
TODO 
Bloomington

The textrank algorithm creates a bipartite network of sentence pairs that share words and then applies Google Pagerank on the sentence network to rank sentences [@textrank]. This is the summary method I use, in part because many federal agencies used the texrank algorithm to summarize public comments. This means that the sentences chosen by textrank may be especially likely to be seen and quoted by agency officials responding to comments.  

Topic models also provide helpfuls summary information. Frequent and exclusive words associated with with the topic(s) occuring in the highest proportions in the document may be helpful summary information. 

Similarly, the results of supervised machine classification can also simplify human coding [@Sebok2021]. For example @Chou2019 use k-means clustering to classify sentences in audit reports and select sentences closted to cluster centroids as summaries for human auditors. Humans with basic domain knowledge are well suited to check and correct the results of machine coding. Human coders might dig deeper in to cases where the machine coding looks off (given other available metadata, like associated entities, summaries, or topics). As humans do such a task, they likely learn how the classifier(s) tend to error. Future human and computational attention can then be paid to those cases. 


Nothing is language dependent. Whereas statistical models of word frequencies require language-dependent preprocessing for dimension-reduction [@Lucas2015], regex tables and text reuse are not.


# Classification 
"The tasks of estimating category percentages (quantification) or classifying individual documents (classification) both begin by analyzing a small subset of documents with (usually hand-coded) category labels." [@Jerzak2019]

The brand of linguistics that focuses on the combinations of words we use emphasises the recurring nature of phrases. 
"utterances are formed by repetition, modification, and concatenation of previously-known phrases consisting of more than one word...we speak mostly by stitching together swatches of text that we have heard before"
[@Becker1975]

nu- merical representation of text documents can have an outsized impact on the results (see Denny and Spirling, 2016; Levy, Goldberg, and Dagan, 2015). 

A corpus may lack textual discrimination regarding word frequencies [@Jerzak2019], but still be classified by phrases long enough that they are highly unlikely to appear by chance. 

Another advangage of key phrases is that it cleanly allows texts to appear in multiple groups. 
--> 