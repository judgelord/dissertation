\documentclass{article}
\usepackage[text={6.5in, 9in}, centering, top=1in]{geometry}
\usepackage{setspace, amsfonts, amsmath, amssymb, dcolumn, multirow, graphicx, setspace, caption, rotating, booktabs, fancyhdr, hyperref, lastpage}
\pagestyle{fancy}
 \fancyhead[R]{Code \#737 \\Methods / 2\textsuperscript{nd} Field\\Page \thepage \hspace{1pt} of \pageref{LastPage}}
%\usepackage[round, sort, comma]{natbib}
%\setcitestyle{notesep={, },round,aysep={},yysep={,}}
\usepackage[authordate, backend=biber, doi=false, url=false, isbn=false, uniquename=false, maxcitenames = 2, uniquelist=false, natbib]{biblatex-chicago}
\addbibresource{Mendeley.bib}
%\setlength{\parindent}{0pt}
\setlength{\parskip}{1em}
\title{Measuring Political Influence with Topic Modeling and Text Reuse}
\author{}
\begin{document}
\maketitle
\tableofcontents
\doublespace
\bigskip

\section{Introduction}
This paper reviews methods of analyzing texts to address core political science questions about the relative influence of actors and ideas.

Spoken and written words have always been central to the study of politics, but coding large amounts of text is often prohibitively time-consuming and often unreliable \citep{King2003}. For decades, automated methods have been used to capture descriptive statistics like word frequencies, but increasing computational power now enables scholars to perform complex search operations and classify documents or words along latent dimensions. Summarizing, matching, and classifying texts are core subjects of a growing field of computational linguistics that is producing increasingly accessible tools for extracting meaning from text \citep{Bird2009}. 
While political scientists have mostly used these new tools for descriptive purposes, a few recent studies have begun to use them more directly in theory testing with survey experiments or network analysis. 

I compare and contrast two approaches: clustering methods like topic models and matching methods like plagiarism detection algorithms.\footnote{Text analysis includes a broad range of methods beyond the scope of this essay including automated markup \citep{Cardie2008}, capturing event data \citep{Hanna2013Computer-aidedMovements}, sentiment analysis \citep{Ceron2014,Titov2008}, argument structure, and clarity. Clarity or ``readability'' scores are based on the length of words and the number of words per sentence. \citet{Owens2013} measure the clarity of supreme court to argue that justices strategically obfuscate opinions to avoid judicial review. \citet{Baerg2014} finds that the preferences of central bankers is associated with the preciseness of the language they use.} I discuss the mechanics of two specific methods, \textit{Latent Dirichlet Allocation} and \textit{Smith-Waterman} alignment as well as conditions for inference and applications related to the study of political influence. Because these approaches capture different kinds of political influence, I argue that combining them presents an opportunity for scholars to improve research designs. However, they should not always be combined in the same way. For example, sometimes a matching sequence of words in two texts is especially strong evidence of shared ideas, bolstering claims of similarity from a clustering methods that only capture the similar frequency of word use. In other contexts, matching sequences represent meaningless repetition that could lead clustering methods to identify unhelpful or even misleading clusters.\footnote{This may result when policy texts of certain types share a common preamble, quote past policy, or retain content from one version to the next, unhelpfully masking differences or similarities to other types of text in modeling.  This may even lead to erroneous conclusions. For example, interest group comments in support of a proposed regulation are less likely to quote the proposed regulation than those who are seeking changes to the regulation, potentially leading researchers to conclude that the final regulation more closely resembled that ideas of the latter group.} In this case local alignment methods may be helpful in filtering out noise so that topic models are better able to identify meaningful topics.

Topic modeling and text-reuse methods are both used to study the relationships among politically relevant texts, including speeches, policies, and media, but conceptually and mechanically, they are very different approaches. Whereas topic models are often used to identify issues or issue frames, local alignment algorithms can be used to identify precise claims or legal rules created by a specific sequence of words. I argue that, for broad classes of common research questions, these differences present opportunities to use both in tandem. In the final section I suggest three ways that a combination of text analysis methods may help measure influence in policymaking to address key questions: Whose words end up in law? To what extent can these methods help political scientists make inferences about who wins and who loses? How might existing methods be retooled or combined give us answers to these questions and under what conditions should we accept these answers as valid?

\subsection{Working with Textual Data}
Before comparing the approaches, it is necessary to introduce several common procedural ``preprocessing'' steps and choices required for either approach. Textual data must often be ``cleaned'' and the unit of analysis must be determined. The unit of analysis could be a policy report, survey response, or a section, paragraph, or sentence thereof. Modeling approaches often involve assumptions about how the words in the document were chosen, and the units of text and modeling assumptions ought to align with the aims of the research.  This may mean excluding certain portions of the text from parts of the analysis.

Cleaning text involves removing content that lacks meaning and may muddle results. This includes any erroneous characters introduced by the conversion of formatted text to character strings. For approach that treat documents as a ``bag of words,'' such as methods based on word frequency, this also often includes removing filler words (``stopwords'') and words that only appear in a small number of documents. It also may include truncating words to their stems by removing suffixes so that different conjugations are counted as the same concept. Removing stopwords, rare words, and suffixes may significantly affect topic model results \citep{Fokkens2013}\footnote{The R package \textit{pretext} \citep{Denny2017} provides estimates of the effect of these and other preprocessing decisions on topic model results.} For text reuse, removing words and suffixes simply makes it more likely that matches will be identified in lightly edited texts. Next, each word \footnote{For simplicity, I focus on the most common case where documents are tokenized by individual words rather than by sentences or by all possible strings of strings of 1:n words (``n-grams''). For example, one of the most efficient text reuse methods to calculate global alignment is to compare the proportion of two character (or word) pairs (``bi-grams'') shared by two documents \citep{Dice1945}, but other methods are more useful than this generic kind of global alignment for most political science applications.} in the entire vocabulary of the text corpus is assigned a number and each document is converted to a vector of these numbers. Each element of the vector is called a token. 
Additional steps specific to each method are discussed below. %Representing documents as a vector of tokens, each taking the value of a word (or sometimes more than one word).


% similarities 
\subsection{Inferring Influence Within Copora}
Inferring influence under either method depends either on the completeness of the dataset or some external information. If two texts used very similar strings or very similar distributions of words, we can only infer that  one may have influenced the other to the extent that we are confident no third, earlier text may have influenced both.

Researchers may be centrally concerned with who expressed an idea first or in the path information took between two points. The former requires that no earlier texts are missing from the data. The latter, also requires that no intermediate texts are missing. In the case of drafts of legislation, one of the few areas in political science to use text reuse, it is interesting that two documents share content even if we cannot say why. The sponsor of the original bill may have lobbied to get the language into a later bill, the sponsor of the later bill may have lifted the section on their own, or some third party may have lobbied each sponsor separately. These findings similarity allow us to infer \textit{influence} only when we have external evidence of the mechanism or are able to rule out other plausible mechanisms. 

The result is somewhat paradoxical: we are least able to infer causal influence with respect to exactly the types of texts that are likely to be most influential. We are most confident in the originality of text like extemporaneous speeches. If we observe later texts matching such a speech, we can be fairly confident in a causal connection, but the most forceful political ideas are often the more carefully crafted ones. The most influential types of texts, such as legal texts and prepared policy recommendations, may often be adapted from other sources and it may be difficult to infer who is influencing whom from the texts alone.

\subsection{Conceptual Similarities and Differences}
To compare topic modeling and text reuse approaches, it is necessary to disaggregate each into two types. Topic modeling generally means either classifying documents by ``topic'' (a single-member model) or estimating latent ``topics'' that shape the distribution of words in documents that are a mixture of topics (a mixed-member model). Text reuse algorithms calculate either global alignment scores (overall similarity) or local alignment (matching segments of text). While scholars have used each of these for nominally similar purposes, selection of each approach rests (often implicitly) on differences in the type of data and research objectives. 

Each of these four approaches is a way of describing a different kind of relationship between documents and it may make sense to examine multiple relationship types for some collections of documents. Consider the example of a collection of interest group policy recommendations. We may be interested in classifying them into types (a single-membership model). Depending on how they vary, these types may represent their issue area, or if they are all on the same policy issue, a common issue frame or lobbying coalition. Even within a coalition, different groups' texts may emphasize certain ideas (collections of words) more than others and we may we may want to estimate the content of these latent ideas and how emphasis differs between documents (a mixed-membership model). We may also want to identify the overall most similar texts (global alignment). This overall similarity (like the more nuanced similarity in topic distributions) may result from writers using similar ideas and issue frames, resulting in similar word frequencies. It may also result from using the exact same sentences (local alignment). Like topic similarity, global alignment scores are often based on the frequency of words used in each text, but result from very different methods. Local alignment a distinct concept. 

I focus on mixed-member topic models and local alignment scores. These approaches have two major differences. 

One major difference is that topic models are are based on probabilistic intuitions about how similarities and differences in text arise and text-reuse algorithms are not. Local alignment algorithms determine matching and non-matching tokens given certain parameters for the flexibility of the match; for example, how many words in a string can be a non-matching for the longer string to be counted as a match. To identify specific ``local'' alignments, the simplest method is to identify exact matches of a certain length, but this is likely to miss much.   In contrast, topic models estimate parameter values in a latent space that maximizes the likelihood of correct classification. 

The other major difference is the that topic models treat texts as a ``bag of words'' (or, occasionally, a bag of n-gram sequences), whereas word sequence is essential to local alignment methods. While some argue that sequence matters \citep{Wilkerson2015} and other find that it does not \citep{Grimmer2013}, I focus on the conditions under which we expect sequence to be useful in analyzing texts, specifically the nature of the data and what the design aims to measure.

Because of these differences, clustering and text reuse methods have been used to measure nominally similar but conceptually distinct phenomena. For example, identifying the origin of an idea (as measured by either a string or a distribution of words) relies of textual evidence. As Wilkerson (page 945) note, a ``policy idea'' is ``an admittedly ambiguous concept. For some, policy idea refers to a general policy objective (e.g., universal health care), whereas for others it refers to specific policy provisions in laws.'' The strategy one uses depends on the nature of the texts and the nature of the idea under study, specifically on how helpful or unhelpful it is to rely on the ordering of words in a text rather than just their likelihood of appearing. In policymaking (i.e. when a policy text is the dependent variable), the conceptual difference between clustering method and text reuse methods maps onto the distinction between political issues or issue frames and specific policy provisions or proposals. In broader political contexts, text reuse can also represent the repetition of political rhetoric, which is more helpful in identifying the influence of salience or agreement than specific causal relations like the origin of ideas. 

Both topic modeling and text reuse methods have been used to study the ``origins'' of politically-relevant ideas. \citet{Brookhart2015} trace the origins of ideas using a dynamic topic model to assess whether topics were first raised by elites, the media, or the pubic. For them, an idea is a policy issue, usually a policy problem, represented by the frequency the certain words appear. The same idea can be expressed a number of ways, but generally involves the a similar distribution of words that is distinct from the words one may use to speak about a different policy issue. They give the example of ``human trafficking'' which could also be described as ``trafficking of sex workers,'' both of which capture the same underlying policy problem. What they aim to discover are latent issues or issue frames in the frequency of word use. Focusing on matching sequences of words would fail to associate many texts that are about the same issue without copying text verbatim. Copied text may be interesting in this context. Indeed, when politicians or Twitter users quote new stories, this is evidence of the influence of the media, but quoted text would only identify the origins of the specific phrase or fact, not the origin of the issue or issue frame.

In contrast, Wilkerson uses text reuse to trace the origin of policy ideas in legislation. In this context an ``idea'' is a specific policy solution, an exact string of text crafted to address some aspect of a policy problem. Unlike the news medias and Twitter content used by Brookhart and Tahk, there are not norms against copying text from others. Indeed, it is common. Rewriting legal language from scratch is more costly than rephrasing a tweet and so policy ideas are more easily traced as an exact string of words rather than a distribution. 
Wilkerson's unit of analysis is a specific statutory provision. Because it is plausible that statutory provisions may have very similar distributions of words, but very different legal meanings and because statutory language is, in fact, often copied, using text reuse allows for a more nuanced measure. There is some risk of failing to identify earlier variations of the policy idea that used different wording, but given the norm of copying existing text, if a text is not copied, it may be reasonable to infer that it represents something new. 

% contrast 
%When speaking about influence 
%first to use this 

%Strings are ordered 



\section{Topic Models}


Topic models generally require one to set the number of topics. There are a number of approaches to topic selection, including algorithms\footnote{Models can also be estimated using the methods described in \citet{Lee2014}, which selects the number of topics based on t-distributed stochastic neighbor embedding.}, but none are objective \citep{Roberts2014}. \citet{Chuang2014} argue that topic modeling is done best with a "human-in-the-loop" approach where researchers review multiple candidate models. 

%The general task is to identify similarities and differences between groups of texts. 

The two core types of topic models, single-member models and mixed-member models, emerge from distinct assumptions about the texts. Single-member models are a classical mixture model where each document is assumed to belong a single topic. Single member models are best suited to classifying texts as belonging to one camp or another. Mixed-member models assume that documents are drawn from a latent distribution of topics\citep{Blei2009}. I focus on the most prominent approach to estimating topic distributions, the \textit{Latent Dirichlet Allocation} (LDA) model \citep{Blei2003}.



\subsection{Mechanics of LDA}
In LDA,  texts are assumed to be random mixtures of latent topics and topics are distributions of words  \citep{Blei2003}. Both the distribution of topics over documents and distribution of words over topics are estimated. Each word in the vocabulary may be associated with multiple topics, but each token in a document is assigned to exactly one topic. Thus, the output includes both the estimated distributions of words in each topic (the content of that topic) and a representation of each document or collection of documents as a vector of topic proportions (the topics content of the documents), which is simply the fraction of the words in each document belong to each topic. %The percent of each topic z within each document $d$, is estimated as $\pi_{d}$.% where:
%\begin{align}
%z_i &\sim Multinomial(\theta_{D_{i, j} })\\
%\theta_k &\sim Dirichlet(\alpha)\\
%w_i &\sim Multinomial(\phi_i)\\
%\phi &\sim Dirichlet(\beta) \\
%=\\
%\tau_{n, d} | W_{n, d} &\sim Multinomial(\pi_{w_{n, d} })\\
%\pi_{d} &\sim Dirichlet(\alpha) \\ % should tau be there
%W_{n, d} &\sim Multinomial(\rho_{\tau,w})\\ % should be W_{tao}, probability of drawing a word in W at token i is drawn from a multi distribution of the probabilities that each word is in each topic
% should be rho_{w, tau}
%\rho_{\tau,w} &\sim Dirichlet(\beta) 
%\end{align}

The main difference between LDA and a classic two-level model common in hierarchical Bayes is that LDA has a latent variable of topics (distributions of words) $z$ between the beliefs about topic proportions $\pi$ and observed words. 

The document generating process is assumed to have two steps.\footnote{\citet{Blei2003} describe the generating process as having three steps, first selecting the number of words in a document. However, because this process is independent of other data generating variables and because I discuss LDA as a tool for studying documents with observed length, not generating new documents, we can safely ignore this step.} First the topic proportions for each document are drawn from a Dirichlet distribution. Second, the topic for each word is drawn from from a multinomial distribution reflecting its document's topic proportions. The specific word is drawn from the distribution of words in that topic:
\begin{enumerate}
\item Draw topic proportions $\pi_d | \alpha \sim Dir(\alpha).$
\item For each token $w_{d,n}$:\begin{enumerate}
\item Draw topic assignment $z_{d, n} | \pi_d \sim Mult(\pi_d)$
\item Draw word $w_{d,n} | z_{d,n,\beta_{1:T}} \sim Mult(\beta_{z_{d,n}}).$\end{enumerate}
\end{enumerate}


We observe the total number of unique words ($w_1,...,w_W$) in the vocabulary of all documents and $w_{d,n}$ is the word observed at the $n$th token in document $d$. All texts are ``tokenized'' by giving each word\footnote{For topic estimation, tokenizing is usually done by word, but may also be done by sentence or by any n-gram string of characters or words.} a unique index $n$. If token $n$ belongs to topic $z$, then the probability that the token is word $w$ is the topic-specific probability $\pi_{\tau, w}$. At the document level, $\pi_{z, d}$ %/$\theta_{i, k}$ 
is our beliefs about the proportion of topic $z$ for document $d$. %, a $i$ x $\tau$ matrix of the proportion of words from each topic in each document. 

$T$, $\alpha$, and $\beta$ are defined. 
$T$ is the number of topics $(z_1,...z_T)$ where $z_{n, d}$ is the topic assignment of the $n$th token in document $d$. Each token comes from exactly one topic.
$\alpha$ is the parameter of the prior on the per-document topic distributions, and
$\beta$ is the parameter of the prior on the per-topic word distributions. 
%$z$ % / $\phi_{w, k}] / $ 
%is the latent topic distribution, the distribution over $w$ words in each topic $z$, i.e. the probability of drawing the $w$th word of the vocabulary for topic $\tau$.

One major problem is that topic estimation in LDA and its extensions can be arbitrary and unstable \citep{Jagarlamudi2012}. Computer scientists have extended the core LDA model, incorporating available information to improve the estimation of meaningful topics. This includes document attributes such as the author, year, or treatment condition \citep{Roberts2014}, document network structure such as citations \citep{Chang2009}, word correlations \citep{Blei2005}, sentiment of words% \citet{Lin2009JointAnalysis}
, constraints based on intuitions (``lexical priors'') on the distribution of words over topic \citep{Jagarlamudi2012}, or combinations of these kinds of information \citep{Kang2014}. 

In one of the most promising steps to help topic models measure influence, \citet{Chang2009} develop a version of LDA they call a Relational Topic Model. This model includes a binary random variable for each document pair that is conditioned by the latent space that also determines topic proportions. Thus, the document generating process has an extra step:
\begin{enumerate}
\item Draw topic proportions $\pi_d | \alpha \sim Dir(\alpha).$
\item For each word $w_{d,n}$:\begin{enumerate}
\item Draw assignment $z_{d, n} | \pi_d \sim Mult(\pi_d)$
\item Draw word $w_{d,n} | z_{d,n,\beta_{1:K}} \sim Mult(\beta_{z_{d,n}}).$\end{enumerate}
\item  For each pair of documents $d,d'$:\\ $y|z_d, z_{d'} \sim \psi (\cdot |x_d, z_{d'})$.
\end{enumerate}
	
\citet{Chang2009} demonstrate this model with respect to academic documents and citations, but political influence may be modeled similarly. For example, documents may be linked by one citing the other or plagiarizing the other. While \citet{Bode2014CandidateMidterms} use multidimensional scaling to explore topics of tweets that use common hashtags, a Relational Topic Model may be an ideal way to model tweet content linked by common hashtags.

Another variation on LDA that may help measure influence is Structural Topic Modeling (STM). Instead of covariates only being used post-hoc (estimating effects \textit{after} naively estimating topics), STM brings information contained in covariates into the topic model by (1) assigning unique priors by covariate value, (2) allowing topics to be correlated, (3) allowing word use within a topic to vary by covariate values. Instead of $\pi \sim Dirichlet(\alpha)$, topic proportions can be influenced by covariates $X$ through a regression model, $\pi \sim LogisticNormal(X\gamma, \Sigma)$. This helps the model avoid having to develop a categorization scheme from scratch \citep{Grimmer2011} and improves the consistency of estimated covariate effects \citep{Roberts2014}.

Whereas lexical priors include word-level information and STM includes document-level information, \citet{Kang2014} propose what they call a Hereto-Labeled LDA, which is able to incorporate both document and feature labels, even when only some document and feature types are known. 


Finally, a variety of approaches have built of the Dynamic Topic Model proposed by \citet{Blei2006}. This version allows topic content to change over time. \citet{Brookhart2015}  use a version of this model in their analysis of the emergence of political issues. In their model, an autoregressive latent variable allows the words used to discuss the same issue to change over time. 


\subsubsection{Computation of LDA}
The basic inferential task for for LDA is computing the posterior distribution of latent variables for topics over documents, $\pi$, and words over topic, $z$. 


Because the likelihood function contains latent variables, $p(w|\alpha, \beta)$ cannot be estimated exactly for LDA (unlike many hierarchical models where there are direct links between estimated and observed parameters). A lower bound on the log likelihood function can be estimated deterministically with variational inference \citep{Blei2003}, but many prefer to estimate LDA stochastically with Gibbs sampling and many extensions require Gibbs sampling. Additionally, Gibbs sampling allows the algorithm to jump out of local optima and does not require the assumption that all documents are independent as variational Bayes does (because the likelihood of seeing the corpus is the product of the likelihood for each document). 


\subsection{Applications of Topic Models in Political Science}
Most studies in the social sciences and humanities using topic models, especially LDA, rely on observational data and focus on descriptive findings, often called text mining \citep{Srivastava2009}.

Single-member models are often used to classify texts in order to create a variable. In one of the most well-know examples,\citet{Grimmer2010} analyzed over 100,000 press releases to identify policy agendas, characterize representational style, and predict voting behavior. Similarly, \citet{Quinn2010} use a single-member model to identify the topics of floor speeches and \citet{Wilkerson2016} use a range of LDA models to identify metatopics in one-minute floor speeches, finding support for the theory of issue ownership. 
\citet{Boussalis2016} describe discourse around climate change denial, focusing on the relationship between topics that focus on politics and science and how discourse has changed over time. They find increased proportion of text focusing on scientific integrity. While not explicitly an independent-dependent variable setup, they conclude that the observed discourse is often a reaction to scientific claims. 

While Structural Topic Models (STMs) have great potential to estimate differences in policy content across types of texts, they have not been widely applied in this area. Notable recent contributions on this front include \citet{Bagozzi2016} who use an STM to examine attention to different issues vary over time in State Department Reports and \citet{Genovese2017SectorsFrom} who uses an STM to study country positions on international climate change negotiations. %While their data allow them use the whole text, the versioning nature of budget justifications require the additional preprocessing discussed above. 

The concept of influence implies a change from a baseline condition or counter factual. In the case of topic models this means a significantly different distribution of words than was observed previously or than would be expected given some external information. If the hypothesized cause of change is also observed as a text (e.g. a survey experiment vignette or an interest group letter), influence will often mean that the outcome text changed significantly toward using a distribution of words more similar to the origin text. 

Model setup and interpretation depends on whether the texts are theorized to each contain a single topic or a mixture of several topics. If texts represent actor positions, a single-member models may be used to capture the coalition to which a text belongs and the distribution of topics from a mixed-member model may represent the distribution of priorities. The difference between influencing which cluster an actor is in and influencing the distribution of topic an actor emphasizes could be seen as a form of the distinction \citet{Carsey2006} make between changing sides and changing minds.

\subsubsection{Text as the DV: Survey Experiments}
A few studies using structural topic models have aimed at measuring influence by measuring treatment effects on survey experiment responses \citep{Roberts2014,Mildenberger2015,Fong2016}. The text of an open response question is the dependent variable. \citet{Mildenberger2015} find that framing climate change in different ways affects the topics discussed by survey respondents.

In an experimental context, inference is based on finding a credible difference in word distributions between treatment conditions. A naive mixed-member model can estimate the content of a given number of topics and the differences between treatment conditions on each topic. Treatments are almost often text, but these texts are not generally used in modeling. 

In survey experiments, treatment texts (e.g. the text of vignettes) could be used to improve topic estimation and inference in several ways.  First, the topic model could include priors over the words in the treatments such that each word is expected to be assigned to the same topic as other words unique to the same condition and not the same topic as words from different conditions. This would improve model estimation and make it more likely that topics reflect treatments. This may be helpful, for instance, when when studying framing effects and the same words used in the treatment appear in responses it may indicate successful manipulation or cuing. In other contexts, words repeated may represent uninformative parroting of the vignette.

\subsubsection{Text as the DV: Observational Data}

Recent scholarship has begun to use mixed-member models to describe political relationships using the relative emphasis of different topics in text as the dependent variable. 
\citet{Genovese2017SectorsFrom} uses a Structural Topic Model to investigate the relationship between statements made by businesses and governments regarding climate change and sectoral levels of pollution and trade. She finds that high-polluting sectors with more exposure to trade are less likely to support international cooperation on climate change, and low-polluting sectors with high exposure to trade are more supportive. Importantly, government statements mirror domestic industry positions suggesting that industry preferences influence government's approach to climate change.

Instead of a topic model,
\citet{Kluver2015} use a two-step process that first classifies interest groups with k-means clustering and then uses multidimensional scaling to identify the latent dimensions that best distinguish them, thus identifying multiple dimensions of disagreement. An alternative approach would be to use a single-member model to identify interest group coalitions and then use a version of LDA incorporating elements of the Structural Topic Model and Relational Topic Model to identify the dimensions of policy disagreement given this coalition structure. Though they do not use topic models, \citet{Kluver2015} make an important contribution by including the text of the outcome policy in the analysis. As discussed in the final section, placing policy outputs in the space of policy debate (whether measured through multidimensional scaling or latent topics) should be a priority for researchers aiming to estimate influence over policy. 

Political scientists may especially benefit from computer science work developing models that use both the content of texts and networked structure between them. Much of this work thus far has been demonstrated with respect to webpage structure or academic citation networks. \citet{Neiswanger2014} model \textit{Wikipedia} content in a latent variable setup (a topic model) with corrections, or weights, based on the content of linked articles to get a better picture of what each article is really about.  As noted, \citet{Chang2009} accomplish a similar task, explicitly building academic citation information into an LDA model, allowing them to predict the distribution of words for a new article based on its citations or its citations based only on its content. This could be especially useful in modeling influence in political networks when we observe some causal processes linking some pairs of documents and aim to estimate other linkages based on observed content. For example, politicians may not always cite news sources by name when policy action is motivated by media attention, but the occasions when a particular source is cited can be seen as a partially observed network of influence. By modeling the content of news stories that are cited and corresponding policy statements, we may be able to infer additional media-politician relationships from the words they use.

%While citations are common





\subsubsection{Validation}
Given the arbitrariness of the number of topics selected and the potential instability in estimation, one key validation step is to demonstrate that topics make sense \citep{Grimmer2013}. There is no guarantee that an unsupervised model will identify meaningful topics. As noted modeling strategies can help but are still no guarantee that meaningful latent topics will be recovered. 

When topic models are used to generate independent variables, interpretation of topic content naturally receives attention. However, if scholarship moves more in the direction of hypothesis testing where theories focus on the similarity, difference, or change in topic distribution between documents, there is a risk that topic content could become hidden behind correlations of topic proportion means. To avoid this, two validations strategies have been suggested: (1) demonstrate that different algorithms produce similar topics and (2) establish that variations in topic emphasis across time or venues correlate with real-world events \citep{Grimmer2011,Roberts2014,Blei2009,Quinn2010,Wilkerson2016}.  \citet{Beauchamp2017PredictingData} validates his topic model of political opinions on Twitter against opinion polls. 
\citet{Quinn2010} validate their classification of Senate speeches against previous findings from hand-coding approaches. \citet{Wilkerson2016} call attention that, because topic selection is arbitrary, reporting and validating a single model is insufficient. They argue that topic selection and validation should explicitly address the robustness of topics across specifications. Beyond validation, they illustrate that focusing on topic robustness can help interpret results by grouping potential topics into robust metatopics. Thus, in addition to interpretability and face validity, reliability across model specifications is an important standard for unsupervised approaches.

\section{Text Reuse}
In contrast to classification methods like topic models, recent advances in text reuse and semantic analysis has received less attention in political science. 
For text reuse, the unit of analysis is a pair of texts. Text reuse methods produce at least one of two statistics, a global alignment score and local alignment scores. Global alignment is a measure of how well documents align overall. Some methods are based on word frequencies and others are based on sequence. Local alignment methods identify and often score matching portions of two documents and are necessarily based on sequence.% Global alignment is measured in a wide variety of ways but generally aims to reflect the amount of content they share. Local alignment identifies specific matching sequences of words. 


If affecting which words people use is a measure of influence, then affecting the distribution \textit{and} sequence of words may be even stronger evidence. Thus I focus on local alignment and only briefly discuss global alignment and semantic methods. %\citep{Wilkerson2016}.
Additionally, for measuring influence, global alignment has a major disadvantage compared to both topic modeling and local alignment: results are much less informative about the content that is similar. Whereas topic models share the bag-of-words assumption with methods like cosine similarity, they can identify specific distributions of words on which documents are similar or dissimilar. Local alignment methods identify specific sequences of words on which documents are similar or dissimilar. Whereas \citet{Acree2016} suggest that if cosine similarity is to be used to estimate document similarity, it should be weighted with local alignment, incorporating local alignment into topic modeling strategies could accomplish the same aim with more interpretable results. 

Text reuse and semantic attributes are usually discovered using deterministic matching. Text reuse methods identify matching strings and often score these matches based on defined criteria, for example, the relative priority a researcher places on the exactness and the length of the matching sequence. Semantic attributes, such as tone or verb tense, are usually captured using dictionary-based methods. Words (or occasionally phrases) are compared against a dictionary of terms determined to have that attribute. One may use an existing dictionary or create a custom one for the research purpose, either by hand or with a learning algorithm. For example, \citet{Acree2014} hand-codes texts into ideological categories and then uses an algorithm to create a dictionary of phrases reflective of each group of texts, allowing him to measure the proportion different ideological phrases used by candidates. 

Matching algorithms loop over strings of words to identify words or sequences of words (or characters) that match align with a reference pattern. For dictionary-based methods, this is a simple set operation; a word or phrase is in the dictionary or it is not. For text reuse methods this is often much more difficult. There are often multiple ways that two texts with shared sequences can be aligned. Algorithms generate alignment scores based on user-defined points and penalties for the length and exactness of a match. 


\subsection{Mechanics of Smith-Waterman Local Alignment Algorithm}

Early work on alignment algorithms was advanced in the field of genetic sequencing. One common algorithm was developed by \citet{Smith1981} for matching gene sequence. 

The Smith-Waterman (SW) algorithm goes up when the next character is a match and down when it is not. One must set the size of the increase when the next character in a string is a match, the decrease when it is not, the size of the penalty for inserting a gap, and a condition for when the algorithm will terminate. Penalties for mismatches or gaps allow one to be more or less tolerant of differences in local alignments. Finally, if the goal is to identify a subset of the text pairs that match, one must set a minimum threshold score for inclusion of each local alignment in the output dataset. 

Given these values, the SW algorithm identifies optimal local alignment by selecting the highest score from the various ``paths'' through a matrix of possible word pairs in the two documents, generally maximizing the inclusion of matching tokens and minimizing the inclusion of mismatches. It does this in a three-step process. First, given two documents of length $a$ and $b$, a $a+1$ by $b+1$ matrix is initialized with its first row and column equal to 0. Second, the matrix is scored based on matches, nonmatches, and gaps. If matches and mismatches are assigned scores of 1 and -1, respectively, the substitution matrix can be described as

$s(a_i, b_j) = \begin{Bmatrix} +1, a_i = b_j\\ -1, a_i \neq b_j \end{Bmatrix}$ 


If the penalty for a gap is $W$, the matrix $H$ is filled in such that

$H_{ij} = max \begin{Bmatrix}H_{i-1,j-1} + s(a_i, b_j),\\ max_{k\geq 1} \{H_{i-k,j} - W_k\},\\ max_{l\geq 1} \{H_{i-l,j} - W_l\},\\0 \end{Bmatrix}$. 

Finally,  a traceback algorithm starts at the highest value in the matrix and follows the path that maximizes the cumulative score until reaching the termination threshold, usually 0. This is illustrated in Figure 1 where orange arrows indicate the path taken by the traceback algorithm from each cell with a positive score to the next cell that maximizes the cumulative score. The first match has a cumulative score of 4 and the second match has a cumulative score of 6.

\begin{figure}[!ht]
\centering
\caption{Illustration of Smith Waterman Scored $H$ Matrix and Traceback} %\citep{LinderVisualizingAlgorithm}}
\includegraphics[width=10cm]{s.png}
\end{figure}

\subsubsection{Computation}
Compared to topic modeling approaches, text reuse algorithms tend to involve more straightforward computations but in greater numbers. Because text alignment algorithms involve multiple operations on all possible word pairs in all possible document pairs these methods can quickly become computationally expensive larger numbers of longer documents. Thus, one of the key ways to reduce computational cost is to cut down the number of document pairs considered. 

If many documents in a corpus do not share any content, these pairs can be excluded. One way to do this is to filter out pairs that do not meet a matching criteria that is easier to compute. For example, \citet{Wilkerson2015} exclude pairs that do not share at least five 10-grams, greatly reducing the number of pairs considered. As noted, comparing n-grams is a simple set operation and is cheap to compute. Similarly, pairs that do not share a baseline level of similarity can be eliminated by converting text strings to hash codes, whereby duplicate strings are given a common index and are thus easy to detect \citep{Huston2011}. \citet{Collins2015} and \citet{Eshbaugh-Soha2013} use popular plagiarism detection program \textit{WCopyfind}, which uses hashing to detect exact matches between texts, but, unlike\textit{ Smith-Waterman}, the algorithm it uses is only able to skip over non-matching segments of near-identical length and does not attempt to optimize alignment \citep{Bloomfield2011}. 

Local alignment calculations can also be sped up by requiring that alignments include certain exact n-gram matches. If these anchoring n-gram matches are long enough, it is unlikely to lead to suboptimal matches. \citet{Wilkerson2015} find that 10 word matches worked well for identifying matching strings in sections of bills


\subsection{Applications of Text Reuse in Political Science}
Few studies focus directly on the concept of influence. 
\citet{Collins2015} uses \textit{WCopyfind} to assess the influence of \textit{amicus curiae} briefs on Supreme Court opinions and \citet{Eshbaugh-Soha2013} uses it to measure the influence of White House press conferences on news coverage. These inferences depend on the assumption that text matching that in \textit{amicus} briefs or press secretaries means it originated there, which requires information about how those documents are created. 

\citet{Hertel-Fernandez} use text reuse to detect the proposal and adoption of legislative language proposed by the American Legislative Exchange Council (ALEC), a group that drafts model state legislation and advocates for it. They find that states with less legislative professionalism are more likely to introduce and pass bills that contain ALEC language. Because they have external information about the document generating process---they know that ALEC plays a major role in writing and disseminating model legislation to state legislators---\citet{Hertel-Fernandez} can infer that the similarities they find are evidence of ALEC influence. Even if some state legislators are copying other states, ALEC is influential if it is the originator \textit{or} dissemination of the policy text, a plausible assumption for many of these texts.

Recent research has begun to incorporate information about the network of relationships among actors, greatly improving the plausibility that text similarity reflects influence .
\citet{Linder2017} find that state legislators with similar voting behavior introduce bills with matching text and that text reuse reflects policy diffusion networks found by other scholars. 
\citet{Garrett2015} assess the relative influence of interest groups and early adopting states on policy diffusion using text similarity as an attribute of a network model. They note that ``Knowing which states adopted similar policies does not inform our understanding of interest groups' impact on policy adoption and emulation...studies cannot distinguish the influence of interest group model legislation from the impact of other state and national actors'' (pg. 7). Using a network model allows them to measure the centrality of different texts.  However, because they use global alignment scores rather than local alignment or topics, their model cannot describe the nature of copied content. 

The next section suggests situations where research designs may benefit from drawing on the strengths both of topic modeling, especially variants of LDA, and of text reuse methods, especially local alignment matching.



%Future work using text reuse may explore influence across venues. For example, some interest groups draft language to be used in both legislation and legal briefs.

\section{Combining Methods}
My contribution is to suggest ways that scholars may combine topic modeling approaches with text reuse methods. I focus on two ways that text reuse is relevant to research designs. First, it can identify tokens that should be considered to have a distinct generative process. Identifying such portions of text allows researches to exclude these portions of text or adjust models to capture this information. Second, it can identify especially meaningful relationship between pairs of documents and text fragments within them. This information may be included in topic models that account for relationships among documents, in how the generative process for these specific tokens is modeled, or the weight of these tokens in determining topics. If, instead of improving topic estimation, the goal is to identify whose ideas are ending up in policy and under what circumstances certain ideas and not others are adopted, copied text may be the core focus of the model. 

\subsection{Boiler Plate and Versioning}

This subsection focuses on the cases where texts appear similar because of formalistic drafting practices and where a series of documents retain content from one version to the next, as is common in policy texts. Similarities and differences due to the former are usually uninteresting.\footnote{But see work that suggests that elites such as Supreme Court justices \citep{Owens2013} and central bankers \citep{Baerg2014} strategically adjust the clarity, precision, and style of policy statements.} Correctly identifying and removing them can add clarity. Similarities and differences due to the latter can add nuance to our understandings of policy stability and change and the allocation of attention. 

The first step is to identify aligned and unaligned portions of texts. As discussed above this can be done cheaply by selecting matching n-grams or sentences or more robustly with local alignment algorithms like the Smith-Waterman scoring.

Among formal policy documents, the overwhelming volume of content identified by text reuse methods may be boiler plate language that lacks specific meaning but is difficult to thin out by hand. As the name ``boiler plate'' suggests, this content had a different generative process that does not align with topic modeling assumptions. \citet{Wilkerson2015} find this to be the bulk of local alignments among bills in Congress. Removing this content results in cleaner more meaningful texts for subsequent analysis. 

Another distinct generative process often encountered in policy documents is version. In annual reports, authorizations, or budget requests a significant amount of content may be retained from the previous year's version. Retained content and new content may arise from different processes and thus violate topic modeling assumptions. For example, even if an appropriations committee chair first considers the topic and then, given this topic, decides whether to use the previous year's language or draft new language (i.e. both are drawn from the same word distributions), copied tokens should not be able to be assigned in different topics from year to year. 

When applied to evolving versions of policy texts like draft legislation or regulations the combination of topic modeling with text reuse may help us better understand not just what is discussed but the topic distributions of what is being added, cut, copied, or otherwise receiving special attention. All topic models focus on the relationship between text, but identifying aligned and unaligned versions allows for ``difference in difference'' (e.g. what was added or deleted) or ``difference in similarity'' (e.g. what was copied) comparisons. 


%attributes 
%dictionary approaches: 
%sentement 
%verb tense

\subsection{Text With Special Meaning}



When applied to evolving versions of policy texts like draft legislation or regulations the combination of topic modeling with text reuse may help us better understand not just what is discussed but the topic distributions of what is being added, cut, copied, or otherwise receiving special attention. All topic models focus on the relationship between text, but by making the text units being modeled itself a relationship between texts, STM takes a ``difference in difference'' (e.g. what was added or deleted) or ``difference in similarity'' (e.g. what was copied) form. 

The influence of legal precedent is a major topic of debate among scholars of courts \citep{Lupu2013}. Future research could look at both the content of court opinions and the network of citations to precedent. In these cases, textual content, information on network linkages (i.e. citations), and textual attributes of that link (i.e. the quoted text) are available in the\textit{ Lexis Nexis} database and ripe for analysis that combines topic modeling, text reuse, and network structure analysis. 


%What we want to know 
%- is a text on the winning side or losing side (more or less budget, court decision, rule)--on each issue, but we don't know how many issues 
%- within texts on the wining and losing side, which are most influential

\subsection{Measuring Ideal Points With Text}
Policy disagreements are disagreements about words that give governmental force to ideas. Information is an important currency for those trying to influence policy and that rhetoric and framing can affect perceptions of facts and policies. Policy learning can be seen as an updating of where one stands, an updating of beliefs about what is true about the world and, most specifically, an updating about which words (and thus ideas) ought to be in law. Importantly, policymakers are constantly learning about new problems, facts, and policy ideas about which they had no prior position. Thus new dimensions of disagreement are created every time claims are advanced about a new problem definitions, new facts, and new ideas for what the law ought to say.

One common way to infer influence in politics is to estimate the relative position of policy with respect to different actors' ideal points. If available and reliably reflecting what actors want, policy positions expressed as texts have great advantages over those expressed in votes (perhaps the most data used to estimate ideal points).

The most common method of estimating ideal points are derived from a series of votes. The quantity of interest is what kind of policy actors ideally want, and because voting only tells us whether one is for or against a policy text, we need multiple observations in which an actor (or others who plausibly share their position) falls on each side to begin to narrow down their ideological distance from any given policy. To get multiple observations, we must assume that a number of proposed policies can be placed as different points on the same underlying dimension of disagreement. When it is persuasively argued that a number of these underlying issue dimensions more or less collapse to an even more general dimension, we further increase our observations and thus the information we have about each actor regarding that more general dimension. 

Scholars have developed a diversity of approaches in the past 30 years, with pivotal work in the mid 2000s \citep{Monroe2008a}. Early work on estimating ideology with text relied on word scaling methods. Citing the limited nature of information contained in roll call votes, \citet{Monroe2004} use floor speeches and \citet{Laver2003} and \citet{Slapin2008} use party platforms to create alternative measures of ideology using scaling algorithms that estimate a latent dimension and word weights on that dimension based on word frequency (the Rhetorical Ideal-point model, WORDSCORES, and WORDFISH, respectively). These approaches assume that that policy positions reflect preferences over words. In contrast, a topic modeling approach would assume that policy positions are preferences over topics, which are in turn made up of distributions of words, which is more plausible. For this reason, more recent scholarship has tended to use topic models to estimate policy positions. For example \citet{Grimmer2013} uses a topic model of press releases to identify candidate priorities. 

% However, \citet{LoweScalingCount} argues that 

\subsubsection{Policy-specific Positions}
While ideal points on general dimensions may be important for many tasks, there are other ways to measure political influence, especially using policy specific position statements such as speeches, press releases, court briefs, budget requests, or interest group demands. 

When we have  text representing what an actor wanted, we have specific information about where they stand in relation to a policy text. Instead of needing to uncover more general latent dimensions and estimate actors' positions on multiple issues, we directly observe the quantity of interest---the substance, direction, and magnitude of the disagreement in each case. We can cluster these disagreements to make more general statements about the broader nature of disagreements and relative policy positions, but, unlike with voting data, this reduction is not necessary to know the extent to which actors are getting what they want as we can directly observe the extent to which outcome texts incorporate various actors' expressed ideal language. Flattening proposed changes in texts to fewer dimensions than the number of unique demands made by all actors may help descriptively, but is not required analytically. For example, we may reduce textual policy demands to left and right ideologies or preferences for more or less government, and doing so can be descriptively useful, but it is not necessary and may not always be helpful for answering the question of whose ideas end up in law.

If the primary aim is to identify policy actors' ideological proximity to each other, the analysis can be reduced to the similarity and difference in their ideal policy texts, summed across all areas or within broad policy areas. If, alternatively, we ask whose ideas end up in a specific policy, we want to know how similar the policy outcome was to the specific suggestions of each actors and the frequency of changes in their proposed direction regardless of whether they are advocating for ideologically consistent, orthogonal, or even opposite positions in different cases. In practice, the unique dimensions of disagreement in each policy process are rarely perfectly parallel or orthogonal. Mapping ideological distance requires reducing to some tractable number of dimensions. Measuring rates of getting one's way does not.

This is one great advantage of textual data compared to votes. Each observation is far richer in content and analysis requires fewer assumptions to determine an actor's position with respect to a proposed policy. A finding that the words one actor suggested be added to a policy were twice as likely to appear in the final policy as average is an intuitive description of where power to shape policy resides.\footnote{This is, perhaps, even more powerful than saying that the policy tended to shift toward their ideal point on some latent dimension, where the exact content anchoring the ideal point and the dimension are at least slightly ambiguous.} In the next section, I apply this intuition to the case of agency rulemaking. 

\subsection{Potential Applications of Combined Approaches}
To illustrate how multiple topic modeling and text reuse methods may be applied in tandem, I offer several three examples: interest group comments in agency rulemaking, \textit{amicus curiae} briefs in Supreme Court opinions, and agency budget requests. Each example offers distinct data types that require or allow distinct approaches, but all three aim to assess the extent to which certain actors get their way regarding a policy outcome. 

\subsubsection{Interest Group Comments in Agency Rulemaking}
The fact that the majority of new legally binding text is produced by agencies rather by Congress calls attention to the importance of identifying who influences the drafting of agency regulations. The Administrative Procedures Act often requires administrative agencies to solicit public comments on proposed regulations. Rich data on several decades of rulemaking are available but have yet to be fully utilized by scholars. Agencies publish draft rules, and thousands of comments received by interest groups, experts, and citizens. This offers leverage to identify the players, winners, and losers and to track those participating in the policy process over time.

In a classic study on the topic, \citet{Yackee2006} hand0codes interest group comments as requesting more or less government regulation and estimate which groups saw the final draft of the regulation move in their preferred direction. A combination of classification and text reuse methods may be able to examine these findings with more data and more nuanced measures.

I suggest that identifying the winners and losers in rulemaking is best tackled in three steps. First, a single-member model can identify coalitions among a large number of commenters, local alignment algorithm can identify content that interest group comments repeat from the draft rule and that the final rule copies from interest group comments. Finally, a mixed member model that accounts for these didactic relationships, such as the Relational Topic Model, can be used to estimate how interest group comments relate to the general distribution of the change in emphasis from the draft to the final rule.

The first step is to identify who is lobbying together. 

When actors sign onto the same comment, it is clear that they are lobbying together. This generally takes two forms. Businesses and groups representing allied industries often co-sign carefully crafted suggestions that reflect their common interest. We expect this to occur when the benefits of coordination outweigh the costs \citep{Yackee2009}. The other form this takes is public campaigns that ask citizens to submit a form letter, often alongside other actions such as protests. These occasional bursts of civic participation may affect rulemaking, but this is yet to be tested  \citep{Coglianese2001SocialMovement}.% In the first form, many of the businesses are repeat players and I record them individually. In the second form, the advocacy groups are repeat players, and I recorded their participation, but it would be citizens who participate are likely not and I record the number of these comments as an amplitude parameter for the text they signed and I attribute form-letter texts to the advocacy groups promoting them.

Various businesses, advocacy groups, and citizens often comment separately even when they aligned. The comment process is open to anyone and it is often not worthwhile for all actors to coordinate their messages. There may be many dimensions of demands and it is unclear to which coalition many comments belong.

Classifying comments into common groups is a task well suited for a single membership topic model.\footnote{This is in contrast to the mixture model I use to estimate the distribution of multiple topics in each document and each coalition} This model clusters documents by the frequency they use different words. Being classified together does not mean that the documents all address exactly the same distribution of substantive issues, just that how issues are discussed is similar relative to the full set of documents.

Identifying when commenters change coalitions could be done by indexing rules over time and adding a parameter for the probability that an actor switches from one coalition to another at each point in time. This would allow the model to achieve a better fit by reclassifying an actor after some point in time.% These actor and coalition specific points in time are a key output of this approach required to test theories of how policies reorganize political coalitions. 

Bounding the scope of this model (i.e. the policy system) is a challenge. On the one hand each agency deals with many issues of interest to different coalitions. On the other hand, many groups lobby across multiple agencies. %Coalition advocacy at a fine scale based on OMB agency sub-function codes, but I may try to link across related issues and agencies. 

The second step is to assess the extent to which commenters get the change they want.
Participants may ask for two general kinds of things: they ask for specific changes to identified parts of the text or they may ask for a broad shift in emphasis, what \citet{Jones2005} call a policy image. For example, on the same Clean Power Plan rule some may ask the Environmental Protection Agency to make specific changes to two sentences having to do with the classification of power plants and the division of federal and state enforcement authority. Others may ask for broad re-framing to focus less on economic costs and more on environmental equity and the effects of pollution on children. Many commenters do both. 

The two key pieces of information from the rulemaking record: the text comments and the change from the draft rule to the final. This allows a two-pronged approach, one targeting specific demands and one targeting broad demands. Both approaches require the same initial steps. To identify how exactly the rule changed from draft to final, local alignment text reuse methods may identify what is the same and has been added or subtracted. The same method could also identify text in comments that is not copied from the draft rule.\footnote{One may also need to exclude other kinds of text such as narratives introducing the commenter which commonly precede policy demands.} The result of these two steps is the changes requested by commenters and textual change in the rule. 

To identify the adoption of specific demands,  text reuse methods could identify any matches between textual changes in the rule and the changes requested by commenters. If final rules include the specific phrases suggested in comments, this is evidence that these commenters got some of what they asked for. The significance of this kind of relationship between texts could be measured by how many words were copied, weighted by the forcefulness of these words. For example, one could create a dictionary of legally-significant words such as ``shall,'' ``must,'' ``enforcement,'' and ``standard.'' and weight textual alignment scores accordingly. Text reuse can be measured for individual commenters and averaged over coalitions. 

A mixed-member topic model could then be used to identify the adoption of demands for broader shifts in policy image and emphasis. In contrast to the single-member model used to classify commenters into coalitions, we can now treat each text is a mixture over a number of topics. Each word token in a document is assigned to exactly one topic. Words and thus documents have distributions over topics. The extent to which distribution of topics that changed from the draft to the final is similar to the distribution in comments may be seen as a measure of whether the commenter got the kind of change in policy emphasis they asked for. 

Some rulemaking processes also have a commenting period before the draft policy is published. In these cases, commenters respond to an Advanced Notice of Proposed Rulemaking (ANPRM). A similar approach can be used in these processes with the key difference that similarities between comments and the draft rule (now the outcome text), either in specific text fragments or general topic distribution, take on a different meaning. Instead of representing changes to a policy text, it may represent common understandings of what policy already was or had to be on this topic. Changes in the final rule more plausibly represent differences in what policy could be. With respect to the ANPRM and proposed rule it is more difficult to infer that the same result would not have occurred without their comments. While such counter-factual inference may be dubious, both settings measure the same core phenomena of the words actors want becoming policy, interpretation of what this means must attend to this difference. 




\subsubsection{Measuring the Influence of Briefs on Supreme Court Opinions}
High-profile court cases, especially those at the Supreme Court are receiving a growing number of \textit{amicus curiae} briefs from third parties. These briefs often include carefully crafted suggestions for judges to consider and political scientists are interested the role briefs may play in court decisions and opinions \citep{Collins2008,Kearney2000,Corley2011}. The content of Supreme Court opinions matters. Justices devote significant time negotiating over the content of majority opinions \citep{Wahlbeck1998}. Political and private actors in society look to opinions to determine what actions are legal \citep{Spriggs2001}. ``[S]cholars, practitioners, lower court judges, bureaucrats, and the public closely analyze judicial opinions, dissecting their content in an endeavor to understand the doctrinal development of the law'' \citep[pg. 31]{Corley2011}.

A  topic modeling approach could be used to model the distribution of issues over court opinions and briefs. Unlike rulemaking, draft opinions are not written before briefs are received. Nevertheless, a single-member model could be used to identify coalitions and text fragments reused in opinions can identify the most influential members of winning coalitions. In the case of court opinions, citations to a brief represent a special kind of text reuse that indicates a special level of attention \citep{Cross2010}. All of this information could be leveraged to estimate topic model that accounts for the networked structure of both \textit{amicus} participants and of case law.  Viewing opinions and briefs as nodes and citations and coalition membership as linkages, a Relational Topic Model like that proposed by \citet{Chang2009} could provide a novel look at the relationship between justices and lawyers through the relationship of the words they write as they attempt to influence the law. For example, from such a model, topic distributions could be estimated for text copied from petitioner, respondent, and \textit{amicus curiae} briefs illustrating the kinds of words with which briefs successfully attract justices attention.

\subsubsection{Federal Appropriations Cycles}
To illustrate the importance of text reuse methods for topic modeling, I briefly assess textual data on the relationship between agency budget requests and House Appropriations Committee reports that specify how appropriated funds are to be spent. This exercise illustrates the importance of distinguishing between new and old text. Specifically, I suggest that it is important to pay attention to text reuse in this context because it provides information about the attention that members of Congress devote to issues that overall topic proportions do not.  For example, it is plausible that overall, the House committee on Agriculture used a similar distribution of words from in its reports from 2010-2017, but looking at when new text appear (Figure 2) show that a significant portion of it was edited after the change in party power, first in the House, then in the Presidency, with a period in between where appropriations reports remained 80\% the same year after year. Figure 2 also suggests that the content of Senate appropriations reports are less affected by political transitions. The next step is to ask \textit{which issues had text added and subtracted?} This is illustrated by Figure 3.
\begin{figure}[!ht]\centering
\label{USDA6}
\caption{Percent New Text in Agriculture, Rural Development, Food and Drug Administration, and Related Agencies Appropriations Subcommittee Reports 2010-2017.}
\includegraphics[width=10cm]{percent}
\end{figure}

\begin{figure}[!h]
\label{USFS6}
\caption{\textbf{Forest Service} budget justification sentences added (Forest Service+) and cut (Forest Service-) and corresponding house appropriations subcommittee report sentences added (House+) and cut (House-) over six topics 2008-2017. Dotted vertical lines indicate a new House Appropriations Chair. %Solid lines are a new Senate Appropriations Chair. 
Topics are labeled by the six most frequent and exclusive words as identified by the FREX algorithm.
%Shaded region is the 95\% credible interval.
%Lines are portion means.
Congressional texts are Appropriations Subcommittee Budget Justification Report pages that contain the agency's name or abbreviation.}
\includegraphics[width=\textwidth]{USFS6}
\end{figure}
Figure 3 shows the results from a basic LDA model with 6 topics. Having estimated topics, I examine how the topics are distributed across agencies and Congress over time. Each topic has a distribution over words and each document has a distribution over topics. Aggregating, I estimate the expected proportion of topic in a given document type, for example house appropriation committee reports. Specifically, Figure 3 shows the interaction effects (i.e. the expected proportion of each topic) for a given author in a given year. 

One can see that immediately after party transition in the house, the committee cut language related to conservation, restoration, management plans, and added language related to roads, recreation, and development, reflecting the different party priorities. This connection with historical events is one way of validating a topic model, but \citet{Wilkerson2016} argue persuasively that reporting a single model is insufficient. 


%To measure the relationship between how agency budget justifications and committee reports change from year to year I employ several extra preprocessing steps. As each committee oversees more than one agency, the first step was 
This example also illustrates another use of citations information. I use references to the agency's name to select the relevant portions of committee reports. I select all pages that cite the agency's name or abbreviation.\footnote{This is a simplistic use of citation information, which is a potentially powerful class of attributes for identifying the relationship between texts. For example, citations may identify more meaningful relationships between texts and text fragments in court cases \citep{Cross2010}.}

I focus on year-to-year change by selecting only the text that was added or subtracted using n-gram matching. This can be thought of as a versioning problem where the agency updates their budget justification each year and then the appropriations committee updates theirs. Budget justifications repeat about 20-60\% of their text verbatim from year to year. In one respect, this fact is interesting, as is the content that remains stable in budget justifications. This remarkable stability likely reflects inertia, limits on political attention, and areas of broad agreement. With respect to identifying agenda setting, however, we are also interested in what changes and who is driving these changes. If a topic model is estimated on the full budget justification texts, the stable core of these texts leads models to attribute almost all of the variation to the type of text and none to the year. 

I assume topics have the same word distributions from 2008 to 2017.  Rather than trying to measure change the content of topics or the number of documents assigned to a single topic, I assume that the content of topics is the same over my ten-year time period and measure the distribution of these topics in agency and congressional texts over time. This is reasonable because this time period is too short for major linguistic change. If linguistic change is occurring, this would likely show up as a slow shift in topic proportion across the time period and not significantly affect year-to-year differences. Thus, there is no need to estimate topics dynamically as developed by \cite{Blei2006} and \cite{Brookhart2015}.

With ``versioning-style'' documents parsed into added, deleted, and copied portions using text reuse methods and topic proportions estimated for each of these text types, a  time series model could be used to estimate if changes in these proportions are correlated between two document types, for example the change in a proportion of a topic for an agency justification and appropriation committee report.


\section{Conclusion}

Words give meaning to political ideas. Combining classification algorithms like topic models with matching algorithms that identify text reuse has broad potential in political science. The methods reviewed here offer scholars tools to model relationships across policy texts to identify what is flying under the radar, what is copied from elsewhere or otherwise receiving special attention, and ultimately who is driving the substance of policy change.






















\pagebreak
\singlespace
\printbibliography
\end{document}	